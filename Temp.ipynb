{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d39a06cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üéÑ FLUX.1-Kontext Diffusion Explainer (Memory-Optimized)\n",
      "================================================================================\n",
      "‚úÖ Using CUDA: NVIDIA A10G\n",
      "   Memory: 23.7 GB\n",
      "üîß Loading FLUX.1-Kontext model...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You are trying to load model files of the `variant=fp16`, but no such modeling files are available. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30827/3479515580.py\u001b[0m in \u001b[0;36m<cell line: 668>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;31m# Initialize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 683\u001b[0;31m     \u001b[0mexplainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDiffusionExplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m     \u001b[0;31m# STEP 1: Inspect architecture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_30827/3479515580.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_id, device)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Load with aggressive memory optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         self.pipe = FluxKontextPipeline.from_pretrained(\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/diffusers/pipelines/pipeline_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    831\u001b[0m                     \u001b[0;34m\" is neither a valid local path nor a valid repo id. Please check the parameter.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 )\n\u001b[0;32m--> 833\u001b[0;31m             cached_folder = cls.download(\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                 \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/diffusers/pipelines/pipeline_utils.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(cls, pretrained_model_name, **kwargs)\u001b[0m\n\u001b[1;32m   1577\u001b[0m             )\n\u001b[1;32m   1578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1579\u001b[0;31m             model_filenames, variant_filenames = variant_compatible_siblings(\n\u001b[0m\u001b[1;32m   1580\u001b[0m                 \u001b[0mfilenames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariant\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvariant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_patterns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_patterns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m             )\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/diffusers/pipelines/pipeline_loading_utils.py\u001b[0m in \u001b[0;36mvariant_compatible_siblings\u001b[0;34m(filenames, variant, ignore_patterns)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariant_filenames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mvariant\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0merror_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"You are trying to load model files of the `variant={variant}`, but no such modeling files are available. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariant_filenames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0musable_filenames\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mvariant_filenames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You are trying to load model files of the `variant=fp16`, but no such modeling files are available. "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Memory-optimized FLUX.1-Kontext Diffusion Explainer for AWS SageMaker\n",
    "\"\"\"\n",
    "import torch\n",
    "from diffusers import FluxKontextPipeline\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import requests\n",
    "import json\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "# Bedrock API Configuration\n",
    "API_ENDPOINT = \"https://ctwa92wg1b.execute-api.us-east-1.amazonaws.com/prod/invoke\"\n",
    "TEAM_ID = \"team_the_great_hack_2025_022\"\n",
    "API_TOKEN = \"znqXT5zCmCynAx-kyx_hldrxvSeyaWvxzx55vB5mfNg\"\n",
    "\n",
    "class DiffusionExplainer:\n",
    "    def __init__(self, model_id=\"black-forest-labs/FLUX.1-Kontext-dev\", device=\"cuda\"):\n",
    "        print(\"üîß Loading FLUX.1-Kontext model...\")\n",
    "        self.device = device\n",
    "        \n",
    "        # Load with aggressive memory optimization\n",
    "        self.pipe = FluxKontextPipeline.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=torch.float16,\n",
    "            variant=\"fp16\",  # Ensure FP16 weights\n",
    "            use_safetensors=True,\n",
    "        )\n",
    "        self.pipe = self.pipe.to(device)\n",
    "        \n",
    "        # Enable all memory optimizations\n",
    "        self.pipe.enable_attention_slicing(1)  # Max slicing\n",
    "        self.pipe.enable_vae_slicing()  # VAE tiling\n",
    "        \n",
    "        # Try to enable sequential CPU offload if available\n",
    "        try:\n",
    "            self.pipe.enable_model_cpu_offload()\n",
    "            print(\"‚úÖ Enabled CPU offloading\")\n",
    "        except:\n",
    "            print(\"‚ö†Ô∏è  CPU offloading not available\")\n",
    "        \n",
    "        self.original_image = None\n",
    "        self.attention_store = defaultdict(list)\n",
    "        self.hooks = []\n",
    "        self.attention_module_names = {'cross': [], 'self': []}\n",
    "        \n",
    "        # Create output directory\n",
    "        self.output_dir = Path(\"diffusion_analysis\")\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "    def clear_memory(self):\n",
    "        \"\"\"Aggressive memory cleanup\"\"\"\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "        elif torch.backends.mps.is_available():\n",
    "            torch.mps.empty_cache()\n",
    "    \n",
    "    def inspect_architecture(self):\n",
    "        \"\"\"Debug: Inspect FLUX transformer architecture to find attention modules\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üîç INSPECTING FLUX.1-KONTEXT ARCHITECTURE\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        architecture_info = {\n",
    "            'transformer_modules': [],\n",
    "            'attention_modules': [],\n",
    "            'cross_attention_modules': [],\n",
    "            'self_attention_modules': []\n",
    "        }\n",
    "        \n",
    "        # Traverse the transformer\n",
    "        for name, module in self.pipe.transformer.named_modules():\n",
    "            module_type = type(module).__name__\n",
    "            \n",
    "            # Store all modules\n",
    "            architecture_info['transformer_modules'].append({\n",
    "                'name': name,\n",
    "                'type': module_type,\n",
    "                'has_children': len(list(module.children())) > 0\n",
    "            })\n",
    "            \n",
    "            # Identify attention modules by common patterns\n",
    "            if any(keyword in name.lower() for keyword in ['attn', 'attention']):\n",
    "                architecture_info['attention_modules'].append({\n",
    "                    'name': name,\n",
    "                    'type': module_type\n",
    "                })\n",
    "                \n",
    "                if any(keyword in name.lower() for keyword in ['cross', 'context', 'encoder']):\n",
    "                    architecture_info['cross_attention_modules'].append(name)\n",
    "                else:\n",
    "                    architecture_info['self_attention_modules'].append(name)\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\nüìä Total modules: {len(architecture_info['transformer_modules'])}\")\n",
    "        print(f\"üéØ Attention modules found: {len(architecture_info['attention_modules'])}\")\n",
    "        print(f\"   ‚îú‚îÄ Cross-attention: {len(architecture_info['cross_attention_modules'])}\")\n",
    "        print(f\"   ‚îî‚îÄ Self-attention: {len(architecture_info['self_attention_modules'])}\")\n",
    "        \n",
    "        print(\"\\nüìã ATTENTION MODULES (first 20):\")\n",
    "        for i, attn in enumerate(architecture_info['attention_modules'][:20]):\n",
    "            print(f\"  {i+1}. {attn['name']} ({attn['type']})\")\n",
    "        \n",
    "        if len(architecture_info['attention_modules']) > 20:\n",
    "            print(f\"  ... and {len(architecture_info['attention_modules']) - 20} more\")\n",
    "        \n",
    "        # Save detailed report\n",
    "        with open(self.output_dir / \"architecture_inspection.json\", \"w\") as f:\n",
    "            json.dump(architecture_info, f, indent=2)\n",
    "        \n",
    "        print(f\"\\nüíæ Full architecture saved to: {self.output_dir / 'architecture_inspection.json'}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        return architecture_info\n",
    "    \n",
    "    def register_attention_hooks(self):\n",
    "        \"\"\"Hook into cross-attention and self-attention layers\"\"\"\n",
    "        if not self.attention_module_names['cross'] and not self.attention_module_names['self']:\n",
    "            print(\"‚ö†Ô∏è  No attention modules specified.\")\n",
    "            return\n",
    "        \n",
    "        def get_attention_hook(module_name, attn_type):\n",
    "            def hook(module, input, output):\n",
    "                try:\n",
    "                    # Only store MINIMAL data - don't keep full tensors!\n",
    "                    if isinstance(output, tuple):\n",
    "                        hidden = output[0]\n",
    "                        weights = output[1] if len(output) > 1 else None\n",
    "                    else:\n",
    "                        hidden = output\n",
    "                        weights = None\n",
    "                    \n",
    "                    # Compute statistics immediately, don't store tensors\n",
    "                    stats = {\n",
    "                        'hidden_mean': float(hidden.mean().cpu().item()),\n",
    "                        'hidden_std': float(hidden.std().cpu().item()),\n",
    "                        'hidden_shape': list(hidden.shape),\n",
    "                    }\n",
    "                    \n",
    "                    if weights is not None:\n",
    "                        # Downsample attention weights heavily\n",
    "                        with torch.no_grad():\n",
    "                            # Take only a small spatial sample\n",
    "                            if len(weights.shape) >= 3:\n",
    "                                sampled = weights[0, :, :64, :64].cpu().numpy()  # Heavily downsample\n",
    "                            else:\n",
    "                                sampled = weights[:64, :64].cpu().numpy()\n",
    "                        \n",
    "                        stats['weights'] = sampled\n",
    "                        stats['weights_mean'] = float(weights.mean().cpu().item())\n",
    "                        stats['weights_max'] = float(weights.max().cpu().item())\n",
    "                    else:\n",
    "                        stats['weights'] = None\n",
    "                    \n",
    "                    self.attention_store[f\"{module_name}_{attn_type}\"].append(stats)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è  Hook error on {module_name}: {e}\")\n",
    "            \n",
    "            return hook\n",
    "        \n",
    "        # Register hooks\n",
    "        for name, module in self.pipe.transformer.named_modules():\n",
    "            if name in self.attention_module_names['cross']:\n",
    "                hook = module.register_forward_hook(get_attention_hook(name, 'cross'))\n",
    "                self.hooks.append(hook)\n",
    "            elif name in self.attention_module_names['self']:\n",
    "                hook = module.register_forward_hook(get_attention_hook(name, 'self'))\n",
    "                self.hooks.append(hook)\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "        self.attention_store.clear()\n",
    "    \n",
    "    def create_attention_heatmap(self, attention_weights, image_size=(512, 512)):\n",
    "        \"\"\"Generate visual attention heatmap overlay\"\"\"\n",
    "        if attention_weights is None or len(attention_weights.shape) < 2:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Average if needed\n",
    "            if len(attention_weights.shape) > 2:\n",
    "                spatial_attn = attention_weights.mean(0).mean(0) if len(attention_weights.shape) == 4 else attention_weights.mean(0)\n",
    "            else:\n",
    "                spatial_attn = attention_weights.mean(0) if attention_weights.shape[0] > 1 else attention_weights[0]\n",
    "            \n",
    "            # Ensure 1D\n",
    "            if len(spatial_attn.shape) > 1:\n",
    "                spatial_attn = spatial_attn.flatten()\n",
    "            \n",
    "            # Reshape to square\n",
    "            size = int(np.sqrt(len(spatial_attn)))\n",
    "            if size < 2:\n",
    "                return None\n",
    "                \n",
    "            target_len = size * size\n",
    "            if len(spatial_attn) < target_len:\n",
    "                spatial_attn = np.pad(spatial_attn, (0, target_len - len(spatial_attn)))\n",
    "            else:\n",
    "                spatial_attn = spatial_attn[:target_len]\n",
    "            \n",
    "            heatmap = spatial_attn.reshape(size, size)\n",
    "            \n",
    "            # Normalize\n",
    "            heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min() + 1e-8)\n",
    "            \n",
    "            # Resize\n",
    "            heatmap_img = Image.fromarray((heatmap * 255).astype(np.uint8))\n",
    "            heatmap_img = heatmap_img.resize(image_size, Image.BILINEAR)\n",
    "            \n",
    "            # Apply colormap\n",
    "            heatmap_colored = cm.jet(np.array(heatmap_img) / 255.0)[:, :, :3]\n",
    "            heatmap_colored = (heatmap_colored * 255).astype(np.uint8)\n",
    "            \n",
    "            return Image.fromarray(heatmap_colored)\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è  Heatmap generation failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def overlay_attention(self, base_image, attention_heatmap, alpha=0.4):\n",
    "        \"\"\"Overlay attention heatmap on base image\"\"\"\n",
    "        if attention_heatmap is None:\n",
    "            return base_image\n",
    "        \n",
    "        try:\n",
    "            attention_heatmap = attention_heatmap.resize(base_image.size, Image.BILINEAR)\n",
    "            blended = Image.blend(base_image, attention_heatmap, alpha=alpha)\n",
    "            return blended\n",
    "        except:\n",
    "            return base_image\n",
    "    \n",
    "    def image_to_base64(self, image):\n",
    "        \"\"\"Convert PIL Image or tensor to base64\"\"\"\n",
    "        if torch.is_tensor(image):\n",
    "            # Move to CPU and convert\n",
    "            with torch.no_grad():\n",
    "                image = image.squeeze(0).permute(1, 2, 0).cpu().float().numpy()\n",
    "            image = np.clip((image + 1) / 2 * 255, 0, 255).astype(np.uint8)\n",
    "            image = Image.fromarray(image)\n",
    "        \n",
    "        buffered = BytesIO()\n",
    "        image.save(buffered, format=\"PNG\", optimize=True, quality=85)\n",
    "        return base64.b64encode(buffered.getvalue()).decode()\n",
    "    \n",
    "    def send_to_claude(self, step_data, attention_visualizations):\n",
    "        \"\"\"Send step data to Claude for analysis\"\"\"\n",
    "        original_b64 = self.image_to_base64(self.original_image)\n",
    "        current_b64 = self.image_to_base64(step_data['current_image'])\n",
    "        \n",
    "        attention_summary = self.summarize_attention(step_data['attention_maps'])\n",
    "        \n",
    "        content = [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": f\"\"\"You are analyzing step {step_data['step_index']} of {step_data['total_steps']} in a diffusion model image editing process.\n",
    "\n",
    "**Task:** The model is editing an image with the prompt: \"{step_data['prompt']}\"\n",
    "\n",
    "**Timestep:** {step_data['timestep']:.4f} (noise level - higher = earlier in denoising process)\n",
    "\n",
    "**Token breakdown:** {step_data['tokenized_prompt']}\n",
    "\n",
    "**Attention Statistics:** {json.dumps(attention_summary, indent=2)}\n",
    "\n",
    "\n",
    "**Images provided:**\n",
    "1. Original image (reference)\n",
    "2. Current denoised output at this step\n",
    "3-N. Attention heatmap overlays (red = high attention, blue = low attention)\n",
    "\n",
    "**Your analysis should cover:**\n",
    "\n",
    "1. **Visual Changes**: What specific changes are visible compared to the original?\n",
    "\n",
    "2. **Prompt Token Influence**: Map each token (\"Add\", \"christmas\", \"vibe\") to specific spatial regions\n",
    "\n",
    "3. **Attention Pattern Analysis**:\n",
    "   - **Cross-attention**: Which text tokens attend to which image regions?\n",
    "   - **Self-attention**: Which image regions influence each other?\n",
    "\n",
    "4. **Semantic Evolution**: What high-level semantic changes are happening?\n",
    "\n",
    "5. **Denoising Stage**: Early (structure), Mid (objects), or Late (details)?\n",
    "\n",
    "Be specific about spatial locations and quantify when possible.\"\"\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"source\": {\n",
    "                    \"type\": \"base64\",\n",
    "                    \"media_type\": \"image/png\",\n",
    "                    \"data\": original_b64\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"source\": {\n",
    "                    \"type\": \"base64\",\n",
    "                    \"media_type\": \"image/png\",\n",
    "                    \"data\": current_b64\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Add attention visualizations (limit to 3 max to avoid payload bloat)\n",
    "        for viz_name, viz_image in list(attention_visualizations.items())[:3]:\n",
    "            content.append({\n",
    "                \"type\": \"image\",\n",
    "                \"source\": {\n",
    "                    \"type\": \"base64\",\n",
    "                    \"media_type\": \"image/png\",\n",
    "                    \"data\": self.image_to_base64(viz_image)\n",
    "                }\n",
    "            })\n",
    "        \n",
    "        payload = {\n",
    "            \"teamId\": TEAM_ID,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": content}]\n",
    "        }\n",
    "        \n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {API_TOKEN}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(API_ENDPOINT, json=payload, headers=headers, timeout=90)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                return result.get('content', [{}])[0].get('text', 'No response')\n",
    "            else:\n",
    "                return f\"Error: {response.status_code} - {response.text}\"\n",
    "        except Exception as e:\n",
    "            return f\"Error calling Claude API: {str(e)}\"\n",
    "    \n",
    "    def summarize_attention(self, attention_maps):\n",
    "        \"\"\"Create numerical summary of attention patterns\"\"\"\n",
    "        summary = {'cross_attention': {}, 'self_attention': {}}\n",
    "        \n",
    "        for key, values in attention_maps.items():\n",
    "            if not values:\n",
    "                continue\n",
    "            \n",
    "            attn_type = 'cross_attention' if 'cross' in key else 'self_attention'\n",
    "            layer_name = key.split('_')[0] if '_' in key else key\n",
    "            \n",
    "            for i, stats in enumerate(values[:1]):  # Only first instance\n",
    "                summary[attn_type][f'{layer_name}_{i}'] = {\n",
    "                    'hidden_mean': stats.get('hidden_mean', 0),\n",
    "                    'hidden_std': stats.get('hidden_std', 0),\n",
    "                    'hidden_shape': stats.get('hidden_shape', []),\n",
    "                    'weights_mean': stats.get('weights_mean', 0),\n",
    "                    'weights_max': stats.get('weights_max', 0),\n",
    "                }\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def visualize_step(self, step_index, current_image, attention_maps):\n",
    "        \"\"\"Create and save attention visualizations for current step\"\"\"\n",
    "        print(f\"  üé® Generating attention visualizations...\")\n",
    "        \n",
    "        # Convert tensor to PIL\n",
    "        if torch.is_tensor(current_image):\n",
    "            with torch.no_grad():\n",
    "                img_np = current_image.squeeze(0).permute(1, 2, 0).cpu().float().numpy()\n",
    "            img_np = np.clip((img_np + 1) / 2 * 255, 0, 255).astype(np.uint8)\n",
    "            current_pil = Image.fromarray(img_np)\n",
    "        else:\n",
    "            current_pil = current_image\n",
    "        \n",
    "        visualizations = {}\n",
    "        \n",
    "        # Generate heatmaps for key attention layers (LIMIT TO 3 MAX)\n",
    "        count = 0\n",
    "        for attn_name, attn_values in attention_maps.items():\n",
    "            if not attn_values or count >= 3:\n",
    "                break\n",
    "            \n",
    "            stats = attn_values[0]  # Only first\n",
    "            if stats['weights'] is not None:\n",
    "                heatmap = self.create_attention_heatmap(\n",
    "                    stats['weights'],\n",
    "                    image_size=current_pil.size\n",
    "                )\n",
    "                \n",
    "                if heatmap is not None:\n",
    "                    overlay = self.overlay_attention(current_pil, heatmap, alpha=0.4)\n",
    "                    \n",
    "                    viz_key = f\"{attn_name}_0\"\n",
    "                    visualizations[viz_key] = overlay\n",
    "                    \n",
    "                    # Save to disk\n",
    "                    overlay.save(\n",
    "                        self.output_dir / f\"step_{step_index:03d}_{viz_key}.png\",\n",
    "                        optimize=True,\n",
    "                        quality=85\n",
    "                    )\n",
    "                    count += 1\n",
    "        \n",
    "        # Save current image\n",
    "        current_pil.save(\n",
    "            self.output_dir / f\"step_{step_index:03d}_output.png\",\n",
    "            optimize=True,\n",
    "            quality=85\n",
    "        )\n",
    "        \n",
    "        print(f\"  ‚úÖ Saved {len(visualizations)} attention visualizations\")\n",
    "        \n",
    "        return visualizations\n",
    "    \n",
    "    def run_with_explanation(\n",
    "        self,\n",
    "        image_path,\n",
    "        prompt,\n",
    "        num_inference_steps=20,  # REDUCED from 30\n",
    "        sample_every_n_steps=5,   # INCREASED from 3\n",
    "        pause_between_steps=True,\n",
    "        skip_baseline=False  # NEW: option to skip baseline\n",
    "    ):\n",
    "        \"\"\"Main execution loop with step-by-step Claude analysis\"\"\"\n",
    "        \n",
    "        # Load original image\n",
    "        self.original_image = Image.open(image_path).convert(\"RGB\")\n",
    "        # Resize if too large\n",
    "        max_size = 768\n",
    "        if max(self.original_image.size) > max_size:\n",
    "            self.original_image.thumbnail((max_size, max_size), Image.Resampling.LANCZOS)\n",
    "            print(f\"  üìê Resized image to {self.original_image.size}\")\n",
    "        \n",
    "        self.original_image.save(self.output_dir / \"00_original.png\", optimize=True, quality=90)\n",
    "        \n",
    "        baseline_output = None\n",
    "        \n",
    "        # Run baseline (optional)\n",
    "        if not skip_baseline:\n",
    "            print(\"\\nüîÑ Running baseline (no-edit) pass...\")\n",
    "            try:\n",
    "                baseline_output = self.pipe(\n",
    "                    prompt=\"\",\n",
    "                    image=self.original_image,\n",
    "                    num_inference_steps=num_inference_steps,\n",
    "                    guidance_scale=0.0\n",
    "                ).images[0]\n",
    "                baseline_output.save(self.output_dir / \"01_baseline_output.png\", optimize=True, quality=90)\n",
    "                self.clear_memory()\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Baseline failed: {e}\")\n",
    "                skip_baseline = True\n",
    "        \n",
    "        explanations = []\n",
    "        \n",
    "        def step_callback(pipe, step_index, timestep, callback_kwargs):\n",
    "            # Only analyze every Nth step\n",
    "            if (step_index + 1) % sample_every_n_steps != 0:\n",
    "                return callback_kwargs\n",
    "            \n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"üìä ANALYZING STEP {step_index + 1}/{num_inference_steps}\")\n",
    "            print(f\"{'='*80}\")\n",
    "            \n",
    "            # Clear and register hooks\n",
    "            self.attention_store.clear()\n",
    "            self.register_attention_hooks()\n",
    "            \n",
    "            # Extract latent (keep on GPU)\n",
    "            latents = callback_kwargs[\"latents\"]\n",
    "            \n",
    "            # Decode ONLY the current step (memory critical!)\n",
    "            with torch.no_grad():\n",
    "                decoded = pipe.vae.decode(\n",
    "                    latents / pipe.vae.config.scaling_factor,\n",
    "                    return_dict=False\n",
    "                )[0]\n",
    "            \n",
    "            # Generate visualizations\n",
    "            attention_visualizations = self.visualize_step(\n",
    "                step_index + 1,\n",
    "                decoded,\n",
    "                dict(self.attention_store)\n",
    "            )\n",
    "            \n",
    "            # Prepare step data\n",
    "            step_data = {\n",
    "                'step_index': step_index + 1,\n",
    "                'total_steps': num_inference_steps,\n",
    "                'timestep': float(timestep.item() if torch.is_tensor(timestep) else timestep),\n",
    "                'prompt': prompt,\n",
    "                'tokenized_prompt': prompt.split(),\n",
    "                'current_image': decoded,\n",
    "                'attention_maps': dict(self.attention_store)\n",
    "            }\n",
    "            \n",
    "            # Send to Claude\n",
    "            print(\"  ü§ñ Sending to Claude for analysis...\")\n",
    "            explanation = self.send_to_claude(step_data, attention_visualizations)\n",
    "            \n",
    "            print(\"\\nüìù CLAUDE'S ANALYSIS:\")\n",
    "            print(\"-\" * 80)\n",
    "            print(explanation)\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            explanations.append({\n",
    "                'step': step_index + 1,\n",
    "                'timestep': step_data['timestep'],\n",
    "                'explanation': explanation\n",
    "            })\n",
    "            \n",
    "            # CRITICAL: Cleanup immediately\n",
    "            self.remove_hooks()\n",
    "            del decoded\n",
    "            del attention_visualizations\n",
    "            self.clear_memory()\n",
    "            \n",
    "            # Pause for manual review\n",
    "            if pause_between_steps and step_index + sample_every_n_steps < num_inference_steps:\n",
    "                input(\"\\n‚è∏Ô∏è  Press ENTER to continue to next step...\")\n",
    "            \n",
    "            return callback_kwargs\n",
    "        \n",
    "        # Run the actual edit\n",
    "        print(f\"\\nüéÑ Starting Christmas edit (analyzing every {sample_every_n_steps} steps)...\\n\")\n",
    "        \n",
    "        try:\n",
    "            output = self.pipe(\n",
    "                prompt=prompt,\n",
    "                image=self.original_image,\n",
    "                num_inference_steps=num_inference_steps,\n",
    "                callback_on_step_end=step_callback,\n",
    "                callback_on_step_end_tensor_inputs=['latents']\n",
    "            )\n",
    "            \n",
    "            # Save final output\n",
    "            output.images[0].save(self.output_dir / \"02_final_christmas_output.png\", optimize=True, quality=90)\n",
    "            \n",
    "            # Generate final summary\n",
    "            if not skip_baseline and baseline_output is not None:\n",
    "                self.generate_final_report(explanations, baseline_output, output.images[0], prompt)\n",
    "            else:\n",
    "                self.generate_final_report(explanations, self.original_image, output.images[0], prompt)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå ERROR during generation: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        \n",
    "        return explanations\n",
    "    \n",
    "    def generate_final_report(self, explanations, baseline, final_output, prompt):\n",
    "        \"\"\"Send complete analysis to Claude for final summary\"\"\"\n",
    "        if not explanations:\n",
    "            print(\"‚ö†Ô∏è  No explanations to summarize\")\n",
    "            return\n",
    "        \n",
    "        original_b64 = self.image_to_base64(self.original_image)\n",
    "        baseline_b64 = self.image_to_base64(baseline)\n",
    "        final_b64 = self.image_to_base64(final_output)\n",
    "        \n",
    "        all_steps = \"\\n\\n\".join([\n",
    "            f\"**Step {e['step']} (timestep={e['timestep']:.4f}):**\\n{e['explanation']}\"\n",
    "            for e in explanations\n",
    "        ])\n",
    "        \n",
    "        prompt_text = f\"\"\"You've analyzed {len(explanations)} sampled denoising steps of a diffusion model editing process.\n",
    "\n",
    "**Original Prompt:** \"{prompt}\"\n",
    "\n",
    "**STEP-BY-STEP ANALYSES:**\n",
    "{all_steps}\n",
    "\n",
    "**Now provide a COMPREHENSIVE FINAL SUMMARY:**\n",
    "\n",
    "## 1. Overall Transformation Journey\n",
    "Describe the complete evolution from original ‚Üí final output.\n",
    "\n",
    "## 2. Prompt Component Deep-Dive\n",
    "Break down how each part affected the image: \"Add\", \"christmas\", \"vibe\"\n",
    "\n",
    "## 3. Temporal Dynamics\n",
    "Early vs middle vs late stage changes\n",
    "\n",
    "## 4. Baseline Comparison\n",
    "What did the prompt specifically add vs baseline reconstruction?\n",
    "\n",
    "## 5. Attention Mechanism Insights\n",
    "Text-to-image grounding and intra-image dependencies\n",
    "\n",
    "## 6. Key Takeaways\n",
    "5 bullet points about FLUX.1-Kontext's editing strategy\n",
    "\n",
    "**Attached images:**\n",
    "1. Original image\n",
    "2. Baseline\n",
    "3. Final Christmas-themed output\"\"\"\n",
    "\n",
    "        payload = {\n",
    "            \"teamId\": TEAM_ID,\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": prompt_text},\n",
    "                        {\"type\": \"image\", \"source\": {\"type\": \"base64\", \"media_type\": \"image/png\", \"data\": original_b64}},\n",
    "                        {\"type\": \"image\", \"source\": {\"type\": \"base64\", \"media_type\": \"image/png\", \"data\": baseline_b64}},\n",
    "                        {\"type\": \"image\", \"source\": {\"type\": \"base64\", \"media_type\": \"image/png\", \"data\": final_b64}}\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {API_TOKEN}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(API_ENDPOINT, json=payload, headers=headers, timeout=120)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                final_summary = result.get('content', [{}])[0].get('text', 'No response')\n",
    "                \n",
    "                print(\"\\n\" + \"=\"*80)\n",
    "                print(\"üéÅ FINAL COMPREHENSIVE ANALYSIS\")\n",
    "                print(\"=\"*80)\n",
    "                print(final_summary)\n",
    "                print(\"=\"*80)\n",
    "                \n",
    "                # Save complete report\n",
    "                with open(self.output_dir / \"FULL_ANALYSIS_REPORT.md\", \"w\", encoding='utf-8') as f:\n",
    "                    f.write(f\"# FLUX.1-Kontext Diffusion Analysis Report\\n\\n\")\n",
    "                    f.write(f\"**Prompt:** {prompt}\\n\\n\")\n",
    "                    f.write(f\"**Steps Analyzed:** {len(explanations)}\\n\\n\")\n",
    "                    f.write(\"---\\n\\n\")\n",
    "                    f.write(\"# STEP-BY-STEP EXPLANATIONS\\n\\n\")\n",
    "                    f.write(all_steps)\n",
    "                    f.write(\"\\n\\n---\\n\\n\")\n",
    "                    f.write(\"# FINAL COMPREHENSIVE SUMMARY\\n\\n\")\n",
    "                    f.write(final_summary)\n",
    "                \n",
    "                print(f\"\\nüìÑ Full report saved to: {self.output_dir / 'FULL_ANALYSIS_REPORT.md'}\")\n",
    "                print(f\"üìÅ All outputs saved to: {self.output_dir}/\")\n",
    "                \n",
    "            else:\n",
    "                print(f\"‚ùå Error generating final summary: {response.status_code}\")\n",
    "                print(response.text)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error calling Claude API: {e}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# USAGE SCRIPT\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*80)\n",
    "    print(\"üéÑ FLUX.1-Kontext Diffusion Explainer (Memory-Optimized)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Detect device\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "        print(f\"‚úÖ Using CUDA: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        print(\"‚ö†Ô∏è  Using CPU (this will be slow)\")\n",
    "    \n",
    "    # Initialize\n",
    "    explainer = DiffusionExplainer(device=device)\n",
    "    \n",
    "    # STEP 1: Inspect architecture\n",
    "    print(\"\\nüëâ STEP 1: Architecture Inspection\")\n",
    "    print(\"This will identify all attention modules in FLUX.1-Kontext\")\n",
    "    input(\"Press ENTER to start inspection...\")\n",
    "    \n",
    "    arch_info = explainer.inspect_architecture()\n",
    "    \n",
    "    # STEP 2: Auto-select modules\n",
    "    print(\"\\nüëâ STEP 2: Select Attention Modules to Monitor\")\n",
    "    selection = input(\"\\nEnter 'auto' for automatic selection, or 'manual': \").strip().lower()\n",
    "    \n",
    "    if selection == 'auto':\n",
    "        explainer.attention_module_names = {\n",
    "            'cross': arch_info['cross_attention_modules'][:3],  # Reduced to 3\n",
    "            'self': arch_info['self_attention_modules'][:3]      # Reduced to 3\n",
    "        }\n",
    "    else:\n",
    "        print(\"\\nEnter comma-separated module names for cross-attention:\")\n",
    "        cross_modules = input(\"Cross-attention modules: \").strip().split(',')\n",
    "        print(\"\\nEnter comma-separated module names for self-attention:\")\n",
    "        self_modules = input(\"Self-attention modules: \").strip().split(',')\n",
    "        \n",
    "        explainer.attention_module_names = {\n",
    "            'cross': [m.strip() for m in cross_modules if m.strip()],\n",
    "            'self': [m.strip() for m in self_modules if m.strip()]\n",
    "        }\n",
    "    \n",
    "    print(f\"\\n‚úÖ Will monitor {len(explainer.attention_module_names['cross'])} cross-attention modules\")\n",
    "    print(f\"‚úÖ Will monitor {len(explainer.attention_module_names['self'])} self-attention modules\")\n",
    "    \n",
    "    # STEP 3: Run analysis\n",
    "    print(\"\\nüëâ STEP 3: Run Diffusion Analysis\")\n",
    "    \n",
    "    image_path = input(\"\\nEnter path to image (default: holistic.png): \").strip() or \"holistic.png\"\n",
    "    prompt = input(\"Enter prompt (default: 'Add a christmas vibe to it'): \").strip() or \"Add a christmas vibe to it\"\n",
    "    \n",
    "    try:\n",
    "        num_steps = int(input(\"Number of denoising steps (default: 20): \").strip() or \"20\")\n",
    "        sample_every = int(input(\"Analyze every N steps (default: 5): \").strip() or \"5\")\n",
    "    except:\n",
    "        num_steps = 20\n",
    "        sample_every = 5\n",
    "    \n",
    "    pause = input(\"Pause between steps for review? (y/n, default: n): \").strip().lower() == 'y'\n",
    "    skip_baseline = input(\"Skip baseline pass to save memory? (y/n, default: y): \").strip().lower() != 'n'\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üöÄ STARTING ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Image: {image_path}\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Steps: {num_steps} (analyzing every {sample_every})\")\n",
    "    print(f\"Pause mode: {'ON' if pause else 'OFF'}\")\n",
    "    print(f\"Skip baseline: {'YES' if skip_baseline else 'NO'}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    input(\"\\nPress ENTER to begin...\")\n",
    "    \n",
    "    # Run!\n",
    "    explanations = explainer.run_with_explanation(\n",
    "        image_path=image_path,\n",
    "        prompt=prompt,\n",
    "        num_inference_steps=num_steps,\n",
    "        sample_every_n_steps=sample_every,\n",
    "        pause_between_steps=pause,\n",
    "        skip_baseline=skip_baseline\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚ú® ANALYSIS COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üìä Analyzed {len(explanations)} steps\")\n",
    "    print(f\"üìÅ All outputs in: {explainer.output_dir}/\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6a8ff1-3433-49d0-acf6-051648decb04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
