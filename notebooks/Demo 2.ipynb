{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b0efffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üé® Stable Diffusion Cross-Attention Word Attribution (FIXED)\n",
      "================================================================================\n",
      "\n",
      "üì¶ Loading Stable Diffusion...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5de00b644994183b6b9ab34145fd9fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded!\n",
      "\n",
      "\n",
      "================================================================================\n",
      "PRODUCT: MUG\n",
      "================================================================================\n",
      "\n",
      "üé® Generating: transform holistic logo into festive holiday mug design with snowflakes and warm...\n",
      "   Image size: (704, 768)\n",
      "‚úÖ Registered attention control on 0 layers\n",
      "‚ùå WARNING: No attention layers registered!\n",
      "\n",
      "================================================================================\n",
      "PRODUCT: TSHIRT\n",
      "================================================================================\n",
      "\n",
      "üé® Generating: adapt holistic logo for modern t-shirt print with geometric patterns and cool to...\n",
      "   Image size: (704, 768)\n",
      "‚úÖ Registered attention control on 0 layers\n",
      "‚ùå WARNING: No attention layers registered!\n",
      "\n",
      "================================================================================\n",
      "PRODUCT: GIFTBAG\n",
      "================================================================================\n",
      "\n",
      "üé® Generating: convert holistic logo to elegant gift bag design with ribbons and gold accents...\n",
      "   Image size: (704, 768)\n",
      "‚úÖ Registered attention control on 0 layers\n",
      "‚ùå WARNING: No attention layers registered!\n",
      "\n",
      "================================================================================\n",
      "‚úÖ COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "üìÅ Results: sd_attention_outputs/\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Stable Diffusion Cross-Attention Extraction for Word-Level Attribution\n",
    "Fixed version compatible with modern diffusers library\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from diffusers import StableDiffusionImg2ImgPipeline, DDIMScheduler\n",
    "from diffusers.models.attention_processor import Attention\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import gc\n",
    "from typing import Optional, Union, List, Dict\n",
    "from collections import defaultdict\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "MODEL_ID = \"runwayml/stable-diffusion-v1-5\"\n",
    "INPUT_IMAGE = \"holistic.png\"\n",
    "OUTPUT_DIR = \"sd_attention_outputs\"\n",
    "\n",
    "PROMPTS = {\n",
    "    \"mug\": \"transform holistic logo into festive holiday mug design with snowflakes and warm colors\",\n",
    "    \"tshirt\": \"adapt holistic logo for modern t-shirt print with geometric patterns and cool tones\",\n",
    "    \"giftbag\": \"convert holistic logo to elegant gift bag design with ribbons and gold accents\"\n",
    "}\n",
    "\n",
    "# ===== CROSS-ATTENTION CAPTURE (FIXED) =====\n",
    "class CrossAttentionStore:\n",
    "    \"\"\"Stores cross-attention maps during diffusion - handles multiple resolutions\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.step_store = defaultdict(lambda: defaultdict(list))\n",
    "        self.attention_store = {}\n",
    "        self.current_step = 0\n",
    "        \n",
    "    def __call__(self, attn, is_cross: bool, place_in_unet: str, resolution: int):\n",
    "        \"\"\"Store attention with resolution information\"\"\"\n",
    "        if is_cross:\n",
    "            key = f\"{place_in_unet}_res{resolution}\"\n",
    "            # attn shape: [batch_size, seq_len, num_tokens]\n",
    "            self.step_store[self.current_step][key].append(attn.detach().cpu())\n",
    "        return attn\n",
    "    \n",
    "    def between_steps(self):\n",
    "        \"\"\"Move to next step\"\"\"\n",
    "        self.current_step += 1\n",
    "    \n",
    "    def get_attention_by_resolution(self, target_resolution=64):\n",
    "        \"\"\"Get attention maps at specific resolution (more reliable)\"\"\"\n",
    "        attention_maps = []\n",
    "        \n",
    "        for step_idx, layers_dict in self.step_store.items():\n",
    "            for layer_key, attn_list in layers_dict.items():\n",
    "                # Extract resolution from key\n",
    "                if f\"res{target_resolution}\" in layer_key:\n",
    "                    for attn in attn_list:\n",
    "                        if attn.dim() == 3:\n",
    "                            attention_maps.append(attn)\n",
    "        \n",
    "        if not attention_maps:\n",
    "            return None\n",
    "        \n",
    "        # Stack and average\n",
    "        avg_attn = torch.stack(attention_maps).mean(dim=0)\n",
    "        return avg_attn\n",
    "    \n",
    "    def get_all_resolutions(self):\n",
    "        \"\"\"Get list of all captured resolutions\"\"\"\n",
    "        resolutions = set()\n",
    "        for step_idx, layers_dict in self.step_store.items():\n",
    "            for layer_key in layers_dict.keys():\n",
    "                if \"res\" in layer_key:\n",
    "                    res = int(layer_key.split(\"res\")[1])\n",
    "                    resolutions.add(res)\n",
    "        return sorted(resolutions)\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Clear all stored attention\"\"\"\n",
    "        self.step_store.clear()\n",
    "        self.attention_store.clear()\n",
    "        self.current_step = 0\n",
    "\n",
    "\n",
    "class ModernAttentionProcessor:\n",
    "    \"\"\"\n",
    "    Modern attention processor compatible with diffusers 0.21+\n",
    "    Uses the new attention processor API\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, attn_store: CrossAttentionStore, place_in_unet: str):\n",
    "        self.attn_store = attn_store\n",
    "        self.place_in_unet = place_in_unet\n",
    "    \n",
    "    def __call__(\n",
    "        self,\n",
    "        attn: Attention,\n",
    "        hidden_states,\n",
    "        encoder_hidden_states=None,\n",
    "        attention_mask=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Process attention with new API\"\"\"\n",
    "        batch_size, sequence_length, _ = hidden_states.shape\n",
    "        \n",
    "        is_cross = encoder_hidden_states is not None\n",
    "        \n",
    "        # Compute queries\n",
    "        query = attn.to_q(hidden_states)\n",
    "        \n",
    "        # Keys and values\n",
    "        if encoder_hidden_states is None:\n",
    "            encoder_hidden_states = hidden_states\n",
    "        \n",
    "        key = attn.to_k(encoder_hidden_states)\n",
    "        value = attn.to_v(encoder_hidden_states)\n",
    "        \n",
    "        # Get resolution for storing\n",
    "        resolution = int(np.sqrt(sequence_length))\n",
    "        \n",
    "        # Reshape for multi-head\n",
    "        inner_dim = key.shape[-1]\n",
    "        head_dim = inner_dim // attn.heads\n",
    "        \n",
    "        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Compute attention\n",
    "        attention_probs = torch.nn.functional.scaled_dot_product_attention(\n",
    "            query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False\n",
    "        )\n",
    "        \n",
    "        # Store cross-attention before reshaping\n",
    "        if is_cross:\n",
    "            # Compute attention weights for storage\n",
    "            scale = 1 / np.sqrt(head_dim)\n",
    "            attn_weights = torch.matmul(query, key.transpose(-2, -1)) * scale\n",
    "            attn_weights = torch.softmax(attn_weights, dim=-1)\n",
    "            \n",
    "            # Average across heads: [batch, seq_len, text_tokens]\n",
    "            attn_weights_mean = attn_weights.mean(dim=1)\n",
    "            \n",
    "            # Store with resolution info\n",
    "            self.attn_store(attn_weights_mean, is_cross, self.place_in_unet, resolution)\n",
    "        \n",
    "        # Reshape output\n",
    "        attention_probs = attention_probs.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
    "        \n",
    "        # Linear projection\n",
    "        hidden_states = attn.to_out[0](attention_probs)\n",
    "        hidden_states = attn.to_out[1](hidden_states)\n",
    "        \n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "def register_attention_control(model, attention_store: CrossAttentionStore):\n",
    "    \"\"\"Register custom attention processors - modern method\"\"\"\n",
    "    \n",
    "    attn_procs = {}\n",
    "    \n",
    "    # Get all attention processor keys\n",
    "    for name in model.unet.attn_processors.keys():\n",
    "        # Determine location\n",
    "        if name.startswith(\"mid_block\"):\n",
    "            place_in_unet = \"mid\"\n",
    "        elif name.startswith(\"up_blocks\"):\n",
    "            place_in_unet = \"up\"\n",
    "        elif name.startswith(\"down_blocks\"):\n",
    "            place_in_unet = \"down\"\n",
    "        else:\n",
    "            place_in_unet = \"other\"\n",
    "        \n",
    "        # Create custom processor\n",
    "        attn_procs[name] = ModernAttentionProcessor(attention_store, place_in_unet)\n",
    "    \n",
    "    # Set processors\n",
    "    model.unet.set_attn_processor(attn_procs)\n",
    "    print(f\"‚úÖ Registered attention control on {len(attn_procs)} layers\")\n",
    "    return len(attn_procs)\n",
    "\n",
    "\n",
    "# ===== WORD-LEVEL HEATMAP EXTRACTION (FIXED) =====\n",
    "def extract_word_heatmaps(\n",
    "    attention_store: CrossAttentionStore,\n",
    "    tokenizer,\n",
    "    prompt: str,\n",
    "    image_shape: tuple,\n",
    "    target_resolution: int = 64\n",
    ") -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Extract per-word attention heatmaps at specific resolution\n",
    "    \"\"\"\n",
    "    \n",
    "    # Try to get attention at target resolution\n",
    "    avg_attention = attention_store.get_attention_by_resolution(target_resolution)\n",
    "    \n",
    "    # If not available, try other resolutions\n",
    "    if avg_attention is None:\n",
    "        available_res = attention_store.get_all_resolutions()\n",
    "        print(f\"‚ö†Ô∏è No attention at res={target_resolution}, trying: {available_res}\")\n",
    "        \n",
    "        if available_res:\n",
    "            target_resolution = available_res[-1]  # Use highest available\n",
    "            avg_attention = attention_store.get_attention_by_resolution(target_resolution)\n",
    "    \n",
    "    if avg_attention is None:\n",
    "        print(\"‚ùå No attention maps captured!\")\n",
    "        return {}\n",
    "    \n",
    "    # Tokenize prompt\n",
    "    tokens = tokenizer.tokenize(prompt)\n",
    "    \n",
    "    print(f\"üìù Prompt tokens: {tokens}\")\n",
    "    print(f\"   Attention shape: {avg_attention.shape}\")\n",
    "    print(f\"   Resolution: {target_resolution}x{target_resolution}\")\n",
    "    \n",
    "    # Get spatial and text dimensions\n",
    "    batch_size, spatial_len, text_len = avg_attention.shape\n",
    "    spatial_res = int(np.sqrt(spatial_len))\n",
    "    \n",
    "    # Map words to token indices\n",
    "    important_words = []\n",
    "    word_to_token_idx = {}\n",
    "    \n",
    "    skip_tokens = ['<|startoftext|>', '<|endoftext|>', '<s>', '</s>', ',', '.']\n",
    "    \n",
    "    for idx, token in enumerate(tokens):\n",
    "        cleaned = token.replace('</w>', '').replace('ƒ†', '').strip()\n",
    "        \n",
    "        if (cleaned and \n",
    "            token not in skip_tokens and \n",
    "            cleaned.lower() not in ['a', 'an', 'the', 'to', 'with', 'and', 'for', 'into', 'of']):\n",
    "            \n",
    "            important_words.append(cleaned)\n",
    "            word_to_token_idx[cleaned] = idx + 1  # +1 for start token\n",
    "    \n",
    "    # Limit to top 6 words\n",
    "    important_words = important_words[:6]\n",
    "    \n",
    "    print(f\"üéØ Extracting attention for words: {important_words}\")\n",
    "    \n",
    "    word_heatmaps = {}\n",
    "    \n",
    "    for word in important_words:\n",
    "        if word not in word_to_token_idx:\n",
    "            continue\n",
    "        \n",
    "        token_idx = word_to_token_idx[word]\n",
    "        \n",
    "        if token_idx < text_len:\n",
    "            # Extract attention for this token\n",
    "            token_attention = avg_attention[0, :, token_idx].numpy()\n",
    "            \n",
    "            # Reshape to 2D\n",
    "            attention_map = token_attention.reshape(spatial_res, spatial_res)\n",
    "            \n",
    "            # Normalize\n",
    "            attention_map = (attention_map - attention_map.min()) / \\\n",
    "                          (attention_map.max() - attention_map.min() + 1e-8)\n",
    "            \n",
    "            # Resize to image size\n",
    "            from scipy.ndimage import zoom\n",
    "            zoom_factor = (image_shape[0] / spatial_res, image_shape[1] / spatial_res)\n",
    "            attention_map_resized = zoom(attention_map, zoom_factor, order=1)\n",
    "            \n",
    "            word_heatmaps[word] = attention_map_resized\n",
    "    \n",
    "    print(f\"‚úÖ Extracted {len(word_heatmaps)} word heatmaps\")\n",
    "    return word_heatmaps\n",
    "\n",
    "\n",
    "# ===== VISUALIZATION =====\n",
    "def visualize_word_attribution(\n",
    "    image: Image.Image,\n",
    "    word_heatmaps: Dict[str, np.ndarray],\n",
    "    prompt: str,\n",
    "    save_path: Path\n",
    "):\n",
    "    \"\"\"Create word-level attribution visualization\"\"\"\n",
    "    \n",
    "    if not word_heatmaps:\n",
    "        print(\"‚ö†Ô∏è No heatmaps to visualize\")\n",
    "        return\n",
    "    \n",
    "    words = list(word_heatmaps.keys())\n",
    "    n_words = len(words)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, n_words, figsize=(4*n_words, 8))\n",
    "    if n_words == 1:\n",
    "        axes = axes.reshape(2, 1)\n",
    "    \n",
    "    image_array = np.array(image)\n",
    "    \n",
    "    # Colormaps\n",
    "    colormaps = ['hot', 'viridis', 'plasma', 'inferno', 'magma', 'cividis']\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        heatmap = word_heatmaps[word]\n",
    "        \n",
    "        # Top row: Original\n",
    "        axes[0, i].imshow(image)\n",
    "        axes[0, i].set_title(f'\"{word}\"', fontsize=14, fontweight='bold')\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # Bottom row: Attention overlay\n",
    "        cmap = plt.get_cmap(colormaps[i % len(colormaps)])\n",
    "        heatmap_colored = cmap(heatmap)[:, :, :3]\n",
    "        \n",
    "        alpha = 0.65\n",
    "        overlay = (image_array / 255.0) * (1 - alpha) + heatmap_colored * alpha\n",
    "        overlay = np.clip(overlay, 0, 1)\n",
    "        \n",
    "        axes[1, i].imshow(overlay)\n",
    "        axes[1, i].set_title('Cross-Attention', fontsize=11, color='red', fontweight='bold')\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Word-Level Cross-Attention Maps\\n\"{prompt}\"', \n",
    "                 fontsize=13, y=0.98, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight', facecolor='white')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"‚úÖ Saved visualization: {save_path}\")\n",
    "\n",
    "\n",
    "def create_heatmap_grid(\n",
    "    word_heatmaps: Dict[str, np.ndarray],\n",
    "    prompt: str,\n",
    "    save_path: Path\n",
    "):\n",
    "    \"\"\"Create grid of raw heatmaps\"\"\"\n",
    "    \n",
    "    words = list(word_heatmaps.keys())\n",
    "    n_words = len(words)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, n_words, figsize=(4*n_words, 4))\n",
    "    if n_words == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    colormaps = ['hot', 'viridis', 'plasma', 'inferno', 'magma', 'cividis']\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        heatmap = word_heatmaps[word]\n",
    "        \n",
    "        im = axes[i].imshow(heatmap, cmap=colormaps[i % len(colormaps)], \n",
    "                           interpolation='bilinear')\n",
    "        axes[i].set_title(f'\"{word}\"', fontsize=13, fontweight='bold')\n",
    "        axes[i].axis('off')\n",
    "        plt.colorbar(im, ax=axes[i], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    plt.suptitle(f'Raw Attention Heatmaps\\n\"{prompt}\"', \n",
    "                 fontsize=12, y=1.02, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight', facecolor='white')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ===== MAIN GENERATION =====\n",
    "def generate_with_cross_attention(\n",
    "    pipe,\n",
    "    image_path: str,\n",
    "    prompt: str,\n",
    "    output_dir: Path,\n",
    "    strength: float = 0.75,\n",
    "    num_steps: int = 50\n",
    "):\n",
    "    \"\"\"Generate with cross-attention capture\"\"\"\n",
    "    \n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Load image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    max_size = 768\n",
    "    if max(image.size) > max_size:\n",
    "        ratio = max_size / max(image.size)\n",
    "        new_size = tuple(int(dim * ratio // 8 * 8) for dim in image.size)\n",
    "        image = image.resize(new_size, Image.Resampling.LANCZOS)\n",
    "    \n",
    "    image.save(output_dir / \"input.png\")\n",
    "    \n",
    "    print(f\"\\nüé® Generating: {prompt[:80]}...\")\n",
    "    print(f\"   Image size: {image.size}\")\n",
    "    \n",
    "    # Create attention store\n",
    "    attention_store = CrossAttentionStore()\n",
    "    \n",
    "    # Register hooks\n",
    "    num_layers = register_attention_control(pipe, attention_store)\n",
    "    \n",
    "    if num_layers == 0:\n",
    "        print(\"‚ùå WARNING: No attention layers registered!\")\n",
    "        return None, {}\n",
    "    \n",
    "    # Step callback\n",
    "    def step_callback(pipe, step_idx, timestep, callback_kwargs):\n",
    "        attention_store.between_steps()\n",
    "        return callback_kwargs\n",
    "    \n",
    "    # Generate\n",
    "    torch.cuda.empty_cache()\n",
    "    generator = torch.Generator(device=pipe.device).manual_seed(42)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = pipe(\n",
    "            prompt=prompt,\n",
    "            image=image,\n",
    "            strength=strength,\n",
    "            num_inference_steps=num_steps,\n",
    "            guidance_scale=7.5,\n",
    "            generator=generator,\n",
    "            callback_on_step_end=step_callback,\n",
    "            callback_on_step_end_tensor_inputs=[\"latents\"]\n",
    "        )\n",
    "    \n",
    "    output_image = output.images[0]\n",
    "    output_image.save(output_dir / \"output.png\")\n",
    "    print(\"‚úÖ Generation complete\")\n",
    "    \n",
    "    # Check what resolutions we captured\n",
    "    available_res = attention_store.get_all_resolutions()\n",
    "    print(f\"üìä Captured attention at resolutions: {available_res}\")\n",
    "    \n",
    "    # Extract heatmaps\n",
    "    word_heatmaps = extract_word_heatmaps(\n",
    "        attention_store,\n",
    "        pipe.tokenizer,\n",
    "        prompt,\n",
    "        image_shape=output_image.size[::-1],\n",
    "        target_resolution=64 if 64 in available_res else (available_res[-1] if available_res else 64)\n",
    "    )\n",
    "    \n",
    "    # Visualize\n",
    "    if word_heatmaps:\n",
    "        visualize_word_attribution(\n",
    "            output_image,\n",
    "            word_heatmaps,\n",
    "            prompt,\n",
    "            output_dir / \"word_attribution.png\"\n",
    "        )\n",
    "        \n",
    "        create_heatmap_grid(\n",
    "            word_heatmaps,\n",
    "            prompt,\n",
    "            output_dir / \"heatmaps_only.png\"\n",
    "        )\n",
    "    \n",
    "    attention_store.reset()\n",
    "    \n",
    "    return output_image, word_heatmaps\n",
    "\n",
    "\n",
    "# ===== MAIN DEMO =====\n",
    "def run_stable_diffusion_demo():\n",
    "    print(\"=\" * 80)\n",
    "    print(\"üé® Stable Diffusion Cross-Attention Word Attribution (FIXED)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"\\nüì¶ Loading Stable Diffusion...\")\n",
    "    pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=torch.float16,\n",
    "        safety_checker=None,\n",
    "        requires_safety_checker=False\n",
    "    )\n",
    "    \n",
    "    pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "    pipe = pipe.to(\"cuda\")\n",
    "    pipe.enable_attention_slicing(1)\n",
    "    pipe.enable_vae_slicing()\n",
    "    \n",
    "    print(\"‚úÖ Model loaded!\\n\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for product_name, prompt in PROMPTS.items():\n",
    "        print(f\"\\n{'=' * 80}\")\n",
    "        print(f\"PRODUCT: {product_name.upper()}\")\n",
    "        print(f\"{'=' * 80}\")\n",
    "        \n",
    "        output_dir = Path(OUTPUT_DIR) / product_name\n",
    "        \n",
    "        try:\n",
    "            output_img, word_maps = generate_with_cross_attention(\n",
    "                pipe, INPUT_IMAGE, prompt, output_dir,\n",
    "                strength=0.75, num_steps=50\n",
    "            )\n",
    "            \n",
    "            if output_img is not None:\n",
    "                results[product_name] = {\n",
    "                    \"prompt\": prompt,\n",
    "                    \"output_path\": str(output_dir),\n",
    "                    \"num_words\": len(word_maps)\n",
    "                }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"‚úÖ COMPLETE!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nüìÅ Results: {OUTPUT_DIR}/\")\n",
    "    \n",
    "    for product, info in results.items():\n",
    "        print(f\"   ‚Ä¢ {product}: {info['num_words']} words\")\n",
    "    \n",
    "    del pipe\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_stable_diffusion_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35395d00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üé® STABLE DIFFUSION + CONTROLNET: LOGO ON PRODUCTS\n",
      "======================================================================\n",
      "üîÑ Loading ControlNet (Canny)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b324f06946c749e39519bf4afdd0b5b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have disabled the safety checker for <class 'diffusers.pipelines.controlnet.pipeline_controlnet.StableDiffusionControlNetPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ControlNet loaded on cuda\n",
      "‚úÖ Attention processor installed on 0 layers\n",
      "\n",
      "======================================================================\n",
      "\n",
      "üé® Generating MUG\n",
      "   Prompt: ceramic coffee mug on wooden table with snowflake decorations and cinnamon stick...\n",
      "   Key tokens: ['ceramic', 'coffee', 'mug', 'wooden', 'table', 'snowflake']\n",
      "üîÑ Generating with ControlNet...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "423e470b2ac94b9991d44edbff5db92f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated mug\n",
      "üìä Creating attention heatmaps...\n",
      "   Processing attention from 50 timesteps...\n",
      "   Using 250 maps at resolution 4096\n",
      "   Created 6 token heatmaps\n",
      "‚úÖ Saved attention visualization\n",
      "\n",
      "======================================================================\n",
      "\n",
      "üé® Generating TSHIRT\n",
      "   Prompt: black cotton tshirt on mannequin with spotlight from left, geometric shadows on ...\n",
      "   Key tokens: ['black', 'cotton', 'tshirt', 'mannequin', 'spotlight', 'from']\n",
      "üîÑ Generating with ControlNet...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09354f9691c44d4d919f1482bf44bc0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated tshirt\n",
      "üìä Creating attention heatmaps...\n",
      "   Processing attention from 50 timesteps...\n",
      "   Using 250 maps at resolution 4096\n",
      "   Created 6 token heatmaps\n",
      "‚úÖ Saved attention visualization\n",
      "\n",
      "======================================================================\n",
      "\n",
      "üé® Generating GIFTBAG\n",
      "   Prompt: luxury red gift bag with gold ribbon bow on marble surface, soft shadows, elegan...\n",
      "   Key tokens: ['luxury', 'red', 'gift', 'bag', 'gold', 'ribbon']\n",
      "üîÑ Generating with ControlNet...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d4459eb11db449994d71c6679ee4e30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated giftbag\n",
      "üìä Creating attention heatmaps...\n",
      "   Processing attention from 50 timesteps...\n",
      "   Using 250 maps at resolution 4096\n",
      "   Created 6 token heatmaps\n",
      "‚úÖ Saved attention visualization\n",
      "\n",
      "======================================================================\n",
      "‚úÖ ALL PRODUCTS GENERATED!\n",
      "üìÅ Results: sd_controlnet_outputs/\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Stable Diffusion with ControlNet - Place Logo on Products\n",
    "Uses ControlNet to preserve logo while generating products around it\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, DDIMScheduler\n",
    "from diffusers.utils import load_image\n",
    "from PIL import Image, ImageDraw, ImageFilter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import gc\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "INPUT_IMAGE = \"holistic.png\"\n",
    "OUTPUT_DIR = \"sd_controlnet_outputs\"\n",
    "\n",
    "# Highly specific prompts designed for clear attention differences\n",
    "PROMPTS = {\n",
    "    \"mug\": \"ceramic coffee mug on wooden table with snowflake decorations and cinnamon sticks, warm lighting, cozy winter atmosphere\",\n",
    "    \"tshirt\": \"black cotton tshirt on mannequin with spotlight from left, geometric shadows on right, modern studio background\",\n",
    "    \"giftbag\": \"luxury red gift bag with gold ribbon bow on marble surface, soft shadows, elegant presentation\"\n",
    "}\n",
    "\n",
    "# Negative prompt to avoid bad results\n",
    "NEGATIVE_PROMPT = \"blurry, distorted, low quality, watermark, text, cropped, deformed, multiple objects\"\n",
    "\n",
    "# Generation parameters for maximum attention clarity\n",
    "GENERATION_PARAMS = {\n",
    "    \"num_inference_steps\": 50,\n",
    "    \"guidance_scale\": 9.0,  # Higher = stronger text influence\n",
    "    \"controlnet_conditioning_scale\": 0.5,  # Lower = more text freedom\n",
    "}\n",
    "\n",
    "# ===== WORKING ATTENTION EXTRACTOR (SAME AS BEFORE) =====\n",
    "class WorkingAttentionExtractor:\n",
    "    def __init__(self):\n",
    "        self.attention_store = {}\n",
    "        self.step_count = 0\n",
    "        \n",
    "    def __call__(self, attn, hidden_states, encoder_hidden_states=None, attention_mask=None):\n",
    "        batch_size, sequence_length, _ = hidden_states.shape\n",
    "        query = attn.to_q(hidden_states)\n",
    "        is_cross = encoder_hidden_states is not None\n",
    "        \n",
    "        if is_cross:\n",
    "            key = attn.to_k(encoder_hidden_states)\n",
    "            value = attn.to_v(encoder_hidden_states)\n",
    "        else:\n",
    "            key = attn.to_k(hidden_states)\n",
    "            value = attn.to_v(hidden_states)\n",
    "        \n",
    "        query = attn.head_to_batch_dim(query)\n",
    "        key = attn.head_to_batch_dim(key)\n",
    "        value = attn.head_to_batch_dim(value)\n",
    "        \n",
    "        attention_probs = attn.get_attention_scores(query, key, attention_mask)\n",
    "        \n",
    "        if is_cross:\n",
    "            store_key = f\"step_{self.step_count}\"\n",
    "            if store_key not in self.attention_store:\n",
    "                self.attention_store[store_key] = []\n",
    "            attn_mean = attention_probs.mean(dim=0).cpu()\n",
    "            self.attention_store[store_key].append(attn_mean)\n",
    "        \n",
    "        hidden_states = torch.bmm(attention_probs, value)\n",
    "        hidden_states = attn.batch_to_head_dim(hidden_states)\n",
    "        hidden_states = attn.to_out[0](hidden_states)\n",
    "        hidden_states = attn.to_out[1](hidden_states)\n",
    "        \n",
    "        return hidden_states\n",
    "    \n",
    "    def reset(self):\n",
    "        self.attention_store = {}\n",
    "        self.step_count = 0\n",
    "    \n",
    "    def step(self):\n",
    "        self.step_count += 1\n",
    "\n",
    "# ===== CONTROLNET SETUP =====\n",
    "def setup_controlnet_pipeline():\n",
    "    \"\"\"\n",
    "    Setup ControlNet pipeline for structure-preserving generation\n",
    "    Using Canny edge detection to preserve logo structure\n",
    "    \"\"\"\n",
    "    print(\"üîÑ Loading ControlNet (Canny)...\")\n",
    "    \n",
    "    # Load ControlNet model\n",
    "    controlnet = ControlNetModel.from_pretrained(\n",
    "        \"lllyasviel/sd-controlnet-canny\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    # Load SD pipeline with ControlNet\n",
    "    pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "        \"runwayml/stable-diffusion-v1-5\",\n",
    "        controlnet=controlnet,\n",
    "        torch_dtype=torch.float16,\n",
    "        safety_checker=None\n",
    "    )\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    pipe = pipe.to(device)\n",
    "    pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "    \n",
    "    print(f\"‚úÖ ControlNet loaded on {device}\")\n",
    "    return pipe\n",
    "\n",
    "def setup_attention_capture(pipe):\n",
    "    \"\"\"Install attention processor\"\"\"\n",
    "    attention_processor = WorkingAttentionExtractor()\n",
    "    attn_procs = {}\n",
    "    for name in pipe.unet.attn_processors.keys():\n",
    "        attn_procs[name] = attention_processor\n",
    "    pipe.unet.set_attn_processor(attn_procs)\n",
    "    print(f\"‚úÖ Attention processor installed on {len(attn_procs)} layers\")\n",
    "    return attention_processor\n",
    "\n",
    "# ===== IMAGE PREPROCESSING =====\n",
    "def prepare_logo_for_product(logo_path, product_type, size=512):\n",
    "    \"\"\"\n",
    "    Prepare logo image with appropriate positioning for different products\n",
    "    \"\"\"\n",
    "    logo = Image.open(logo_path).convert(\"RGBA\")\n",
    "    \n",
    "    # Create canvas\n",
    "    canvas = Image.new(\"RGBA\", (size, size), (255, 255, 255, 0))\n",
    "    \n",
    "    # Position logo based on product type\n",
    "    if product_type == \"mug\":\n",
    "        # Center logo on mug (slightly smaller)\n",
    "        logo_size = int(size * 0.4)\n",
    "        logo_resized = logo.resize((logo_size, logo_size), Image.Resampling.LANCZOS)\n",
    "        position = ((size - logo_size) // 2, (size - logo_size) // 2)\n",
    "        \n",
    "    elif product_type == \"tshirt\":\n",
    "        # Center chest position\n",
    "        logo_size = int(size * 0.35)\n",
    "        logo_resized = logo.resize((logo_size, logo_size), Image.Resampling.LANCZOS)\n",
    "        position = ((size - logo_size) // 2, int(size * 0.3))\n",
    "        \n",
    "    elif product_type == \"giftbag\":\n",
    "        # Upper center\n",
    "        logo_size = int(size * 0.3)\n",
    "        logo_resized = logo.resize((logo_size, logo_size), Image.Resampling.LANCZOS)\n",
    "        position = ((size - logo_size) // 2, int(size * 0.25))\n",
    "    else:\n",
    "        logo_resized = logo.resize((size, size), Image.Resampling.LANCZOS)\n",
    "        position = (0, 0)\n",
    "    \n",
    "    canvas.paste(logo_resized, position, logo_resized)\n",
    "    \n",
    "    # Convert to RGB\n",
    "    white_bg = Image.new(\"RGB\", (size, size), (255, 255, 255))\n",
    "    white_bg.paste(canvas, (0, 0), canvas)\n",
    "    \n",
    "    return white_bg\n",
    "\n",
    "def create_canny_edge(image, low_threshold=100, high_threshold=200):\n",
    "    \"\"\"\n",
    "    Create Canny edge detection for ControlNet\n",
    "    This preserves the structure of the logo\n",
    "    \"\"\"\n",
    "    image_np = np.array(image)\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Apply Canny edge detection\n",
    "    edges = cv2.Canny(gray, low_threshold, high_threshold)\n",
    "    \n",
    "    # Convert back to RGB\n",
    "    edges_rgb = cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    return Image.fromarray(edges_rgb)\n",
    "\n",
    "# ===== GENERATION WITH CONTROLNET =====\n",
    "def generate_product_with_attention(pipe, attention_processor, logo_path, product_type, prompt, output_path):\n",
    "    \"\"\"\n",
    "    Generate product with logo using ControlNet + attention extraction\n",
    "    \"\"\"\n",
    "    Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\nüé® Generating {product_type.upper()}\")\n",
    "    print(f\"   Prompt: {prompt[:80]}...\")\n",
    "    \n",
    "    # Prepare logo positioning\n",
    "    logo_positioned = prepare_logo_for_product(logo_path, product_type)\n",
    "    logo_positioned.save(Path(output_path) / \"logo_positioned.png\")\n",
    "    \n",
    "    # Create Canny edge map (preserves structure)\n",
    "    canny_image = create_canny_edge(logo_positioned)\n",
    "    canny_image.save(Path(output_path) / \"canny_edges.png\")\n",
    "    \n",
    "    # Tokenize for attention tracking\n",
    "    tokens_raw = pipe.tokenizer.encode(prompt)\n",
    "    tokens_decoded = [pipe.tokenizer.decode([t]) for t in tokens_raw]\n",
    "    stopwords = ['<|startoftext|>', '<|endoftext|>', ',', '.', 'the', 'a', 'an', \n",
    "                 'to', 'with', 'and', 'for', 'into', 'of', 'in', 'on']\n",
    "    tokens = [t.strip() for t in tokens_decoded \n",
    "              if t.strip() and t.strip() not in stopwords][:6]\n",
    "    print(f\"   Key tokens: {tokens}\")\n",
    "    \n",
    "    # Reset attention capture\n",
    "    attention_processor.reset()\n",
    "    \n",
    "    def step_callback(pipe, step_idx, timestep, callback_kwargs):\n",
    "        attention_processor.step()\n",
    "        return callback_kwargs\n",
    "    \n",
    "    # Generate with ControlNet\n",
    "    print(\"üîÑ Generating with ControlNet...\")\n",
    "    with torch.no_grad():\n",
    "        output = pipe(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=NEGATIVE_PROMPT,\n",
    "            image=canny_image,\n",
    "            num_inference_steps=GENERATION_PARAMS[\"num_inference_steps\"],\n",
    "            guidance_scale=GENERATION_PARAMS[\"guidance_scale\"],\n",
    "            controlnet_conditioning_scale=GENERATION_PARAMS[\"controlnet_conditioning_scale\"],\n",
    "            generator=torch.Generator(device=pipe.device).manual_seed(42),\n",
    "            callback_on_step_end=step_callback\n",
    "        )\n",
    "    \n",
    "    output_img = output.images[0]\n",
    "    output_img.save(Path(output_path) / \"output.png\")\n",
    "    print(f\"‚úÖ Generated {product_type}\")\n",
    "    \n",
    "    # Process attention\n",
    "    print(\"üìä Creating attention heatmaps...\")\n",
    "    token_heatmaps = create_token_heatmaps(\n",
    "        attention_processor.attention_store,\n",
    "        tokens,\n",
    "        output_size=(512, 512)\n",
    "    )\n",
    "    \n",
    "    # Visualize\n",
    "    if token_heatmaps:\n",
    "        create_attention_visualization(output_img, token_heatmaps, tokens, prompt, output_path)\n",
    "    \n",
    "    return output_img, token_heatmaps\n",
    "\n",
    "# ===== ATTENTION PROCESSING (SAME AS BEFORE) =====\n",
    "def create_token_heatmaps(attention_store, tokens, output_size=(512, 512)):\n",
    "    if not attention_store:\n",
    "        print(\"‚ö†Ô∏è No attention captured!\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"   Processing attention from {len(attention_store)} timesteps...\")\n",
    "    \n",
    "    attention_by_size = {}\n",
    "    for step_key, attention_list in attention_store.items():\n",
    "        for attn in attention_list:\n",
    "            size_key = attn.shape[0]\n",
    "            if size_key not in attention_by_size:\n",
    "                attention_by_size[size_key] = []\n",
    "            attention_by_size[size_key].append(attn)\n",
    "    \n",
    "    if not attention_by_size:\n",
    "        return None\n",
    "    \n",
    "    max_size = max(attention_by_size.keys())\n",
    "    high_res_attention = attention_by_size[max_size]\n",
    "    \n",
    "    print(f\"   Using {len(high_res_attention)} maps at resolution {max_size}\")\n",
    "    \n",
    "    stacked = torch.stack(high_res_attention)\n",
    "    averaged = stacked.mean(dim=0)\n",
    "    \n",
    "    num_spatial, num_text = averaged.shape\n",
    "    spatial_size = int(np.sqrt(num_spatial))\n",
    "    spatial_h = spatial_w = spatial_size\n",
    "    \n",
    "    token_heatmaps = {}\n",
    "    \n",
    "    for token_idx, token in enumerate(tokens):\n",
    "        if token_idx >= num_text:\n",
    "            break\n",
    "        \n",
    "        token_attention = averaged[:, token_idx].float().numpy()\n",
    "        \n",
    "        try:\n",
    "            if len(token_attention) < spatial_h * spatial_w:\n",
    "                padded = np.zeros(spatial_h * spatial_w)\n",
    "                padded[:len(token_attention)] = token_attention\n",
    "                token_attention = padded\n",
    "            elif len(token_attention) > spatial_h * spatial_w:\n",
    "                token_attention = token_attention[:spatial_h * spatial_w]\n",
    "            \n",
    "            heatmap = token_attention.reshape(spatial_h, spatial_w)\n",
    "        except Exception as e:\n",
    "            print(f\"   Warning: Could not reshape token {token}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        from scipy.ndimage import zoom\n",
    "        scale_h = output_size[0] / heatmap.shape[0]\n",
    "        scale_w = output_size[1] / heatmap.shape[1]\n",
    "        heatmap_resized = zoom(heatmap, (scale_h, scale_w), order=3)\n",
    "        \n",
    "        heatmap_min = heatmap_resized.min()\n",
    "        heatmap_max = heatmap_resized.max()\n",
    "        if heatmap_max > heatmap_min:\n",
    "            heatmap_resized = (heatmap_resized - heatmap_min) / (heatmap_max - heatmap_min)\n",
    "        \n",
    "        token_heatmaps[token] = heatmap_resized\n",
    "    \n",
    "    print(f\"   Created {len(token_heatmaps)} token heatmaps\")\n",
    "    return token_heatmaps\n",
    "\n",
    "def create_attention_visualization(output_img, token_heatmaps, tokens, prompt, output_path):\n",
    "    if not token_heatmaps:\n",
    "        return\n",
    "    \n",
    "    selected_tokens = list(token_heatmaps.keys())[:6]  # Show up to 6 tokens\n",
    "    n = len(selected_tokens)\n",
    "    \n",
    "    if n == 0:\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, n, figsize=(4*n, 8))\n",
    "    if n == 1:\n",
    "        axes = axes.reshape(2, 1)\n",
    "    \n",
    "    output_array = np.array(output_img)\n",
    "    \n",
    "    # Use consistent blue-to-red colormap (RdBu_r: red=high attention, blue=low)\n",
    "    colormap = plt.get_cmap('RdBu_r')\n",
    "    \n",
    "    for i, token in enumerate(selected_tokens):\n",
    "        heatmap = token_heatmaps[token]\n",
    "        \n",
    "        # Top: Output image\n",
    "        axes[0, i].imshow(output_img)\n",
    "        axes[0, i].set_title(f'\"{token}\"', fontsize=14, fontweight='bold')\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # Bottom: Attention heatmap (consistent blue-to-red)\n",
    "        heatmap_colored = colormap(heatmap)[:, :, :3]\n",
    "        \n",
    "        # Blend with original image\n",
    "        overlay = (output_array / 255.0) * 0.3 + heatmap_colored * 0.7\n",
    "        overlay = np.clip(overlay * 255, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        axes[1, i].imshow(overlay)\n",
    "        axes[1, i].set_title('Cross-Attention', fontsize=11, fontweight='bold')\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Average cross-attention maps across all timesteps\\n{prompt}', \n",
    "                 fontsize=12, y=0.99)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Path(output_path) / \"attention_visualization.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"‚úÖ Saved attention visualization\")\n",
    "\n",
    "# ===== MAIN =====\n",
    "def run_demo():\n",
    "    print(\"=\"*70)\n",
    "    print(\"üé® STABLE DIFFUSION + CONTROLNET: LOGO ON PRODUCTS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Setup pipeline\n",
    "    pipe = setup_controlnet_pipeline()\n",
    "    attention_processor = setup_attention_capture(pipe)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for product, prompt in PROMPTS.items():\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        \n",
    "        output_path = Path(OUTPUT_DIR) / product\n",
    "        \n",
    "        try:\n",
    "            output_img, attn_maps = generate_product_with_attention(\n",
    "                pipe, attention_processor, INPUT_IMAGE, product, prompt, output_path\n",
    "            )\n",
    "            \n",
    "            results[product] = {\n",
    "                \"prompt\": prompt,\n",
    "                \"output_img\": output_img,\n",
    "                \"attention_maps\": attn_maps,\n",
    "                \"path\": output_path\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        \n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ ALL PRODUCTS GENERATED!\")\n",
    "    print(f\"üìÅ Results: {OUTPUT_DIR}/\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    del pipe\n",
    "    gc.collect()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4399fbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't connect to the Hub: 404 Client Error. (Request ID: Root=1-6919cd12-0ca14cdc48d249343ba7f2d3;bd801566-6963-4e4d-9c69-7ba613949f6c)\n",
      "\n",
      "Revision Not Found for url: https://huggingface.co/api/models/stabilityai/sd-turbo/revision/fp16.\n",
      "Invalid rev id: fp16.\n",
      "Will try to load from local cache.\n",
      "Couldn't connect to the Hub: 404 Client Error. (Request ID: Root=1-6919cd12-72e73af45e79d7d5490dac92;2b85ba46-87be-4259-bfe1-d3b7c7991ef5)\n",
      "\n",
      "Revision Not Found for url: https://huggingface.co/api/models/stable-diffusion-v1-5/stable-diffusion-v1-5/revision/fp16.\n",
      "Invalid rev id: fp16.\n",
      "Will try to load from local cache.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üé® TUNED FOR MAXIMUM QUALITY: FIGURE 4 ATTENTION VISUALIZATION\n",
      "======================================================================\n",
      "\n",
      "Tuning enhancements:\n",
      "  ‚úì 100 denoising steps (2x refinement)\n",
      "  ‚úì 11.0 guidance scale (stronger text alignment)\n",
      "  ‚úì Final-timesteps-only attention (discard noise)\n",
      "  ‚úì Gaussian blur + enhanced saturation\n",
      "  ‚úì Multi-pass logo sharpening\n",
      "  ‚úì Enhanced contrast blending\n",
      "======================================================================\n",
      "\n",
      "‚è±Ô∏è WARNING: May take 2-3 minutes per product\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üìç CHECKPOINT 0: Pipeline Setup (Tuned)\n",
      "======================================================================\n",
      "üîÑ Loading Stable Diffusion (text2img)...\n",
      "   Steps: 100 (‚Üë Ultra-quality)\n",
      "   Guidance: 11.0 (‚Üë Stronger text alignment)\n",
      "   Trying stabilityai/sd-turbo (‚ö° Turbo)...\n",
      "   ‚ùå Failed: Cannot load model stabilityai/sd-turbo: model is not cached locally and an error\n",
      "   Trying runwayml/stable-diffusion-v1-5 (Standard)...\n",
      "   ‚ùå Failed: Cannot load model runwayml/stable-diffusion-v1-5: model is not cached locally an\n",
      "   Trying CompVis/stable-diffusion-v1-4 (Classic)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b22f1ea442d043ae9c298d6b0f7e7733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch /home/ec2-user/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-4/snapshots/2880f2ca379f41b0226444936bb7a6766a227587/vae: Error no file named diffusion_pytorch_model.safetensors found in directory /home/ec2-user/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-4/snapshots/2880f2ca379f41b0226444936bb7a6766a227587/vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch /home/ec2-user/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-4/snapshots/2880f2ca379f41b0226444936bb7a6766a227587/unet: Error no file named diffusion_pytorch_model.safetensors found in directory /home/ec2-user/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-4/snapshots/2880f2ca379f41b0226444936bb7a6766a227587/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Loaded CompVis/stable-diffusion-v1-4\n",
      "‚úÖ Pipeline ready on cuda\n",
      "‚úÖ Installed tuned attention extractor on 0 layers\n",
      "\n",
      "######################################################################\n",
      "# PRODUCT: TSHIRT\n",
      "######################################################################\n",
      "\n",
      "======================================================================\n",
      "üìç CHECKPOINT 1: Logo + Mask Creation\n",
      "======================================================================\n",
      "  Logo size: 179x179\n",
      "  Position: (167, 64)\n",
      "  Mask expand: 44px\n",
      "‚úÖ Logo + Mask created\n",
      "\n",
      "======================================================================\n",
      "üìç CHECKPOINT 2: Ultra-Quality Inpainting (100 steps)\n",
      "======================================================================\n",
      "  Prompt: black cotton t-shirt on mannequin with spotlight from left, ...\n",
      "  Inference steps: 100\n",
      "  Guidance scale: 11.0\n",
      "  Generating full image (may take 2-3 minutes)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e16e88e75405453793e8ab91c4477bc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Step 0/100\n",
      "    Step 20/100\n",
      "    Step 40/100\n",
      "    Step 60/100\n",
      "    Step 80/100\n",
      "    Step 100/100\n",
      "‚úÖ Generated ultra-quality product with sharp logo\n",
      "\n",
      "  Key tokens: ['black', 'cotton', 't', '-', 'shirt', 'mannequin']\n",
      "\n",
      "======================================================================\n",
      "üìç CHECKPOINT 3: TUNED Cross-Attention Extraction\n",
      "======================================================================\n",
      "  Total timesteps: 101\n",
      "  Spatial resolutions: [64, 256, 1024, 4096]\n",
      "  Using resolution: 4096x64\n",
      "  Using only final 50% of steps (253/505)\n",
      "  Using aggressive weighting (exponential towards final steps)\n",
      "  Spatial grid: 64x64\n",
      "    Token 0: range=[0.8243, 0.8736]\n",
      "    Token 1: range=[0.0079, 0.0124]\n",
      "    Token 2: range=[0.0040, 0.0066]\n",
      "    Token 3: range=[0.0056, 0.0105]\n",
      "    Token 4: range=[0.0023, 0.0038]\n",
      "    Token 5: range=[0.0034, 0.0055]\n",
      "‚úÖ Extracted 6 ultra-sharp attention maps\n",
      "\n",
      "======================================================================\n",
      "üìç CHECKPOINT 4: Enhanced Figure 4 Visualization\n",
      "======================================================================\n",
      "  Creating 6-token enhanced visualization\n",
      "‚úÖ Saved enhanced Figure 4 visualization\n",
      "\n",
      "######################################################################\n",
      "# PRODUCT: MUG\n",
      "######################################################################\n",
      "\n",
      "======================================================================\n",
      "üìç CHECKPOINT 1: Logo + Mask Creation\n",
      "======================================================================\n",
      "  Logo size: 204x204\n",
      "  Position: (154, 154)\n",
      "  Mask expand: 30px\n",
      "‚úÖ Logo + Mask created\n",
      "\n",
      "======================================================================\n",
      "üìç CHECKPOINT 2: Ultra-Quality Inpainting (100 steps)\n",
      "======================================================================\n",
      "  Prompt: ceramic coffee mug on wooden table with snowflake decoration...\n",
      "  Inference steps: 100\n",
      "  Guidance scale: 11.0\n",
      "  Generating full image (may take 2-3 minutes)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0685d68ac97b49f4a5e75173c1b3f991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Step 0/100\n",
      "    Step 20/100\n",
      "    Step 40/100\n",
      "    Step 60/100\n",
      "    Step 80/100\n",
      "    Step 100/100\n",
      "‚úÖ Generated ultra-quality product with sharp logo\n",
      "\n",
      "  Key tokens: ['ceramic', 'coffee', 'mug', 'wooden', 'table', 'snowflake']\n",
      "\n",
      "======================================================================\n",
      "üìç CHECKPOINT 3: TUNED Cross-Attention Extraction\n",
      "======================================================================\n",
      "  Total timesteps: 101\n",
      "  Spatial resolutions: [64, 256, 1024, 4096]\n",
      "  Using resolution: 4096x64\n",
      "  Using only final 50% of steps (253/505)\n",
      "  Using aggressive weighting (exponential towards final steps)\n",
      "  Spatial grid: 64x64\n",
      "    Token 0: range=[0.8108, 0.8818]\n",
      "    Token 1: range=[0.0092, 0.0151]\n",
      "    Token 2: range=[0.0053, 0.0084]\n",
      "    Token 3: range=[0.0083, 0.0156]\n",
      "    Token 4: range=[0.0027, 0.0049]\n",
      "    Token 5: range=[0.0041, 0.0065]\n",
      "‚úÖ Extracted 6 ultra-sharp attention maps\n",
      "\n",
      "======================================================================\n",
      "üìç CHECKPOINT 4: Enhanced Figure 4 Visualization\n",
      "======================================================================\n",
      "  Creating 6-token enhanced visualization\n",
      "‚úÖ Saved enhanced Figure 4 visualization\n",
      "\n",
      "######################################################################\n",
      "# PRODUCT: GIFTBAG\n",
      "######################################################################\n",
      "\n",
      "======================================================================\n",
      "üìç CHECKPOINT 1: Logo + Mask Creation\n",
      "======================================================================\n",
      "  Logo size: 153x153\n",
      "  Position: (180, 52)\n",
      "  Mask expand: 15px\n",
      "‚úÖ Logo + Mask created\n",
      "\n",
      "======================================================================\n",
      "üìç CHECKPOINT 2: Ultra-Quality Inpainting (100 steps)\n",
      "======================================================================\n",
      "  Prompt: luxury red gift bag with gold ribbon bow on marble surface, ...\n",
      "  Inference steps: 100\n",
      "  Guidance scale: 11.0\n",
      "  Generating full image (may take 2-3 minutes)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f2366226ca946798f2f28dc25b7b3c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Step 0/100\n",
      "    Step 20/100\n",
      "    Step 40/100\n",
      "    Step 60/100\n",
      "    Step 80/100\n",
      "    Step 100/100\n",
      "‚úÖ Generated ultra-quality product with sharp logo\n",
      "\n",
      "  Key tokens: ['luxury', 'red', 'gift', 'bag', 'gold', 'ribbon']\n",
      "\n",
      "======================================================================\n",
      "üìç CHECKPOINT 3: TUNED Cross-Attention Extraction\n",
      "======================================================================\n",
      "  Total timesteps: 101\n",
      "  Spatial resolutions: [64, 256, 1024, 4096]\n",
      "  Using resolution: 4096x64\n",
      "  Using only final 50% of steps (253/505)\n",
      "  Using aggressive weighting (exponential towards final steps)\n",
      "  Spatial grid: 64x64\n",
      "    Token 0: range=[0.7971, 0.8701]\n",
      "    Token 1: range=[0.0078, 0.0136]\n",
      "    Token 2: range=[0.0051, 0.0083]\n",
      "    Token 3: range=[0.0072, 0.0129]\n",
      "    Token 4: range=[0.0045, 0.0074]\n",
      "    Token 5: range=[0.0030, 0.0058]\n",
      "‚úÖ Extracted 6 ultra-sharp attention maps\n",
      "\n",
      "======================================================================\n",
      "üìç CHECKPOINT 4: Enhanced Figure 4 Visualization\n",
      "======================================================================\n",
      "  Creating 6-token enhanced visualization\n",
      "‚úÖ Saved enhanced Figure 4 visualization\n",
      "\n",
      "======================================================================\n",
      "‚úÖ TUNED PIPELINE COMPLETE!\n",
      "üìÅ Results: product_generation_fig4_tuned/\n",
      "======================================================================\n",
      "\n",
      "Expect to see:\n",
      "  ‚úì Ultra-sharp logos\n",
      "  ‚úì Clean, sparse attention maps\n",
      "  ‚úì Clear spatial localization per token\n",
      "  ‚úì Professional Figure 4-style visualization\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TUNED FOR MAXIMUM QUALITY: INPAINTING + FIGURE 4 ATTENTION\n",
    "# ============================================================================\n",
    "#\n",
    "# Enhanced for best results:\n",
    "# 1. 100 denoising steps (double) for ultra-refined attention\n",
    "# 2. Higher guidance scale (11.0) for stronger text alignment\n",
    "# 3. Final-timesteps-only attention aggregation (discard noisy early steps)\n",
    "# 4. Enhanced saturation in attention visualization\n",
    "# 5. Multi-pass sharpening for logo preservation\n",
    "#\n",
    "# ============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from PIL import Image, ImageDraw, ImageFilter, ImageEnhance\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import gc\n",
    "from scipy.ndimage import zoom\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# ===== CONFIGURATION (TUNED FOR QUALITY) =====\n",
    "INPUT_LOGO = \"holistic.png\"\n",
    "OUTPUT_DIR = \"product_generation_fig4_tuned\"\n",
    "\n",
    "PRODUCTS = {\n",
    "    \"tshirt\": {\n",
    "        \"prompt\": \"black cotton t-shirt on mannequin with spotlight from left, geometric shadows on right, modern studio background\",\n",
    "        \"logo_size\": 0.35,\n",
    "        \"logo_pos\": (0.5, 0.3),\n",
    "        \"mask_expand\": 1.5,\n",
    "    },\n",
    "    \"mug\": {\n",
    "        \"prompt\": \"ceramic coffee mug on wooden table with snowflake decorations and cinnamon sticks, warm lighting, cozy winter atmosphere\",\n",
    "        \"logo_size\": 0.4,\n",
    "        \"logo_pos\": (0.5, 0.5),\n",
    "        \"mask_expand\": 1.3,\n",
    "    },\n",
    "    \"giftbag\": {\n",
    "        \"prompt\": \"luxury red gift bag with gold ribbon bow on marble surface, soft shadows, elegant presentation\",\n",
    "        \"logo_size\": 0.3,\n",
    "        \"logo_pos\": (0.5, 0.25),\n",
    "        \"mask_expand\": 1.2,\n",
    "    }\n",
    "}\n",
    "\n",
    "# ===== TUNED PARAMETERS FOR MAXIMUM QUALITY =====\n",
    "GENERATION_PARAMS = {\n",
    "    \"num_inference_steps\": 100,      # ‚Üë Doubled from 50 for ultra-refined attention\n",
    "    \"guidance_scale\": 11.0,           # ‚Üë Increased from 9.0 for stronger text alignment\n",
    "    \"height\": 512,\n",
    "    \"width\": 512,\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "NEGATIVE_PROMPT = \"blurry, distorted, low quality, watermark, text, cropped, deformed, multiple objects\"\n",
    "\n",
    "# ===== ATTENTION TUNING =====\n",
    "ATTENTION_PARAMS = {\n",
    "    \"use_final_steps_only\": True,     # ‚úì Only use final timesteps (more semantic)\n",
    "    \"final_steps_ratio\": 0.5,         # Use last 50% of steps (50 out of 100)\n",
    "    \"attention_saturation\": 1.3,      # ‚Üë Boost saturation in visualization\n",
    "    \"blur_sigma\": 2.0,                # ‚Üë Gaussian blur for smoother maps\n",
    "}\n",
    "\n",
    "LOGO_PARAMS = {\n",
    "    \"blend_sharpness\": \"high\",        # ‚úì Use sharper blending\n",
    "    \"multi_pass_sharpen\": True,       # ‚úì Apply sharpening filter\n",
    "    \"feather_width\": 15,              # Softer transition\n",
    "    \"enhancement_factor\": 1.2,        # Enhance contrast on logo\n",
    "}\n",
    "\n",
    "# ===== CROSS-ATTENTION EXTRACTOR (TUNED) ===== \n",
    "class CrossAttentionExtractorTuned:\n",
    "    \"\"\"\n",
    "    Enhanced attention extractor with timestep filtering and caching.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.attention_store = {}\n",
    "        self.step_index = 0\n",
    "        self.timesteps = []\n",
    "    \n",
    "    def __call__(self, attn, hidden_states, encoder_hidden_states=None, attention_mask=None):\n",
    "        batch_size, sequence_length, _ = hidden_states.shape\n",
    "        query = attn.to_q(hidden_states)\n",
    "        is_cross = encoder_hidden_states is not None\n",
    "        \n",
    "        if is_cross:\n",
    "            key = attn.to_k(encoder_hidden_states)\n",
    "            value = attn.to_v(encoder_hidden_states)\n",
    "        else:\n",
    "            key = attn.to_k(hidden_states)\n",
    "            value = attn.to_v(hidden_states)\n",
    "        \n",
    "        query = attn.head_to_batch_dim(query)\n",
    "        key = attn.head_to_batch_dim(key)\n",
    "        value = attn.head_to_batch_dim(value)\n",
    "        \n",
    "        attention_probs = attn.get_attention_scores(query, key, attention_mask)\n",
    "        \n",
    "        if is_cross:\n",
    "            step_key = f\"step_{self.step_index}\"\n",
    "            if step_key not in self.attention_store:\n",
    "                self.attention_store[step_key] = []\n",
    "            attn_mean = attention_probs.mean(dim=0).cpu()\n",
    "            self.attention_store[step_key].append(attn_mean)\n",
    "        \n",
    "        hidden_states = torch.bmm(attention_probs, value)\n",
    "        hidden_states = attn.batch_to_head_dim(hidden_states)\n",
    "        hidden_states = attn.to_out[0](hidden_states)\n",
    "        hidden_states = attn.to_out[1](hidden_states)\n",
    "        \n",
    "        return hidden_states\n",
    "    \n",
    "    def reset(self):\n",
    "        self.attention_store = {}\n",
    "        self.step_index = 0\n",
    "        self.timesteps = []\n",
    "    \n",
    "    def step(self, timestep=None):\n",
    "        self.step_index += 1\n",
    "        if timestep is not None:\n",
    "            self.timesteps.append(timestep)\n",
    "\n",
    "\n",
    "def checkpoint_print(num: int, name: str):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìç CHECKPOINT {num}: {name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "\n",
    "def setup_pipeline():\n",
    "    \"\"\"Setup standard text2img pipeline\"\"\"\n",
    "    checkpoint_print(0, \"Pipeline Setup (Tuned)\")\n",
    "    \n",
    "    print(\"üîÑ Loading Stable Diffusion (text2img)...\")\n",
    "    print(f\"   Steps: {GENERATION_PARAMS['num_inference_steps']} (‚Üë Ultra-quality)\")\n",
    "    print(f\"   Guidance: {GENERATION_PARAMS['guidance_scale']} (‚Üë Stronger text alignment)\")\n",
    "    \n",
    "    models_to_try = [\n",
    "        (\"stabilityai/sd-turbo\", \"‚ö° Turbo\"),\n",
    "        (\"runwayml/stable-diffusion-v1-5\", \"Standard\"),\n",
    "        (\"CompVis/stable-diffusion-v1-4\", \"Classic\"),\n",
    "    ]\n",
    "    \n",
    "    pipe = None\n",
    "    for model_id, description in models_to_try:\n",
    "        try:\n",
    "            print(f\"   Trying {model_id} ({description})...\")\n",
    "            pipe = StableDiffusionPipeline.from_pretrained(\n",
    "                model_id,\n",
    "                torch_dtype=torch.float16,\n",
    "                safety_checker=None,\n",
    "                revision=\"fp16\"\n",
    "            )\n",
    "            print(f\"   ‚úÖ Loaded {model_id}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Failed: {str(e)[:80]}\")\n",
    "            continue\n",
    "    \n",
    "    if pipe is None:\n",
    "        raise RuntimeError(\"Could not load any model\")\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    pipe = pipe.to(device)\n",
    "    print(f\"‚úÖ Pipeline ready on {device}\")\n",
    "    \n",
    "    return pipe\n",
    "\n",
    "\n",
    "def install_attention_extractor(pipe):\n",
    "    \"\"\"Install tuned attention extractor\"\"\"\n",
    "    attention_extractor = CrossAttentionExtractorTuned()\n",
    "    attn_procs = {}\n",
    "    for name in pipe.unet.attn_processors.keys():\n",
    "        attn_procs[name] = attention_extractor\n",
    "    pipe.unet.set_attn_processor(attn_procs)\n",
    "    print(f\"‚úÖ Installed tuned attention extractor on {len(attn_procs)} layers\")\n",
    "    return attention_extractor\n",
    "\n",
    "\n",
    "# ===== CHECKPOINT 1: Logo + Mask =====\n",
    "def create_logo_with_mask(logo_path: str, product_type: str, size: int = 512) -> Tuple[Image.Image, Image.Image, Tuple]:\n",
    "    \"\"\"Create logo on white canvas + inpainting mask\"\"\"\n",
    "    checkpoint_print(1, \"Logo + Mask Creation\")\n",
    "    \n",
    "    logo = Image.open(logo_path).convert(\"RGBA\")\n",
    "    config = PRODUCTS[product_type]\n",
    "    \n",
    "    image = Image.new(\"RGB\", (size, size), (255, 255, 255))\n",
    "    \n",
    "    logo_size = int(size * config[\"logo_size\"])\n",
    "    logo_resized = logo.resize((logo_size, logo_size), Image.Resampling.LANCZOS)\n",
    "    \n",
    "    pos_x = int(size * config[\"logo_pos\"][0] - logo_size // 2)\n",
    "    pos_y = int(size * config[\"logo_pos\"][1] - logo_size // 2)\n",
    "    \n",
    "    print(f\"  Logo size: {logo_size}x{logo_size}\")\n",
    "    print(f\"  Position: ({pos_x}, {pos_y})\")\n",
    "    \n",
    "    image_rgba = Image.new(\"RGBA\", (size, size), (255, 255, 255, 255))\n",
    "    image_rgba.paste(logo_resized, (pos_x, pos_y), logo_resized)\n",
    "    image = Image.new(\"RGB\", (size, size), (255, 255, 255))\n",
    "    image.paste(image_rgba.convert(\"RGB\"), (0, 0))\n",
    "    \n",
    "    mask = Image.new(\"L\", (size, size), 255)\n",
    "    mask_draw = ImageDraw.Draw(mask)\n",
    "    \n",
    "    expand = int(logo_size * (config[\"mask_expand\"] - 1) / 2)\n",
    "    mask_draw.rectangle(\n",
    "        [pos_x - expand, pos_y - expand, pos_x + logo_size + expand, pos_y + logo_size + expand],\n",
    "        fill=0\n",
    "    )\n",
    "    \n",
    "    print(f\"  Mask expand: {expand}px\")\n",
    "    print(f\"‚úÖ Logo + Mask created\")\n",
    "    \n",
    "    logo_coords = (pos_x, pos_y, pos_x + logo_size, pos_y + logo_size)\n",
    "    return image, mask, logo_coords\n",
    "\n",
    "\n",
    "# ===== CHECKPOINT 2: ULTRA-QUALITY Inpainting =====\n",
    "def generate_product_ultra_quality(pipe,\n",
    "                                  attention_extractor: CrossAttentionExtractorTuned,\n",
    "                                  image_with_logo: Image.Image,\n",
    "                                  mask: Image.Image,\n",
    "                                  logo_coords: Tuple,\n",
    "                                  prompt: str,\n",
    "                                  seed: int) -> Image.Image:\n",
    "    \"\"\"\n",
    "    ULTRA-QUALITY inpainting with:\n",
    "    - 100 steps for refined attention\n",
    "    - High guidance for text alignment\n",
    "    - Multi-pass logo sharpening\n",
    "    \"\"\"\n",
    "    checkpoint_print(2, \"Ultra-Quality Inpainting (100 steps)\")\n",
    "    \n",
    "    print(f\"  Prompt: {prompt[:60]}...\")\n",
    "    print(f\"  Inference steps: {GENERATION_PARAMS['num_inference_steps']}\")\n",
    "    print(f\"  Guidance scale: {GENERATION_PARAMS['guidance_scale']}\")\n",
    "    \n",
    "    attention_extractor.reset()\n",
    "    \n",
    "    image_with_logo = image_with_logo.resize((512, 512))\n",
    "    mask = mask.resize((512, 512))\n",
    "    \n",
    "    def callback_on_step_end(pipe, step_idx, timestep, callback_kwargs):\n",
    "        attention_extractor.step(timestep)\n",
    "        if step_idx % 20 == 0:\n",
    "            print(f\"    Step {step_idx}/{GENERATION_PARAMS['num_inference_steps']}\")\n",
    "        return callback_kwargs\n",
    "    \n",
    "    print(\"  Generating full image (may take 2-3 minutes)...\")\n",
    "    with torch.no_grad():\n",
    "        output = pipe(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=NEGATIVE_PROMPT,\n",
    "            height=512,\n",
    "            width=512,\n",
    "            num_inference_steps=GENERATION_PARAMS[\"num_inference_steps\"],\n",
    "            guidance_scale=GENERATION_PARAMS[\"guidance_scale\"],\n",
    "            generator=torch.Generator(device=pipe.device).manual_seed(seed),\n",
    "            callback_on_step_end=callback_on_step_end\n",
    "        )\n",
    "    \n",
    "    output_image = output.images[0]\n",
    "    \n",
    "    # ===== MULTI-PASS LOGO SHARPENING =====\n",
    "    logo_region_original = image_with_logo.crop(logo_coords)\n",
    "    \n",
    "    # Pass 1: Enhance logo contrast\n",
    "    if LOGO_PARAMS[\"enhancement_factor\"] > 1.0:\n",
    "        enhancer = ImageEnhance.Contrast(logo_region_original)\n",
    "        logo_region_original = enhancer.enhance(LOGO_PARAMS[\"enhancement_factor\"])\n",
    "    \n",
    "    # Pass 2: Blend with feathered mask\n",
    "    x1, y1, x2, y2 = logo_coords\n",
    "    blend_mask = np.ones((512, 512), dtype=np.float32)\n",
    "    \n",
    "    feather = LOGO_PARAMS[\"feather_width\"]\n",
    "    for y in range(max(0, y1-feather), min(512, y2+feather)):\n",
    "        for x in range(max(0, x1-feather), min(512, x2+feather)):\n",
    "            dist = min(abs(x - x1), abs(x - x2), abs(y - y1), abs(y - y2))\n",
    "            if dist < feather:\n",
    "                if LOGO_PARAMS[\"blend_sharpness\"] == \"high\":\n",
    "                    # Sharper falloff\n",
    "                    blend_mask[y, x] = (dist / feather) ** 1.5\n",
    "                else:\n",
    "                    blend_mask[y, x] = dist / feather\n",
    "            elif x1 <= x < x2 and y1 <= y < y2:\n",
    "                blend_mask[y, x] = 0\n",
    "    \n",
    "    blend_mask = blend_mask[:, :, np.newaxis]\n",
    "    \n",
    "    output_array = np.array(output_image)\n",
    "    output_array_blend = (\n",
    "        np.array(image_with_logo) * (1 - blend_mask) +\n",
    "        output_array * blend_mask\n",
    "    ).astype(np.uint8)\n",
    "    \n",
    "    output_image = Image.fromarray(output_array_blend)\n",
    "    \n",
    "    # Pass 3: Multi-pass sharpening on logo\n",
    "    if LOGO_PARAMS[\"multi_pass_sharpen\"]:\n",
    "        # Apply sharpening filter to logo region\n",
    "        output_img_array = np.array(output_image)\n",
    "        logo_region = output_img_array[y1:y2, x1:x2]\n",
    "        \n",
    "        # Apply sharpening via PIL\n",
    "        logo_pil = Image.fromarray(logo_region)\n",
    "        enhancer = ImageEnhance.Sharpness(logo_pil)\n",
    "        logo_sharpened = enhancer.enhance(2.0)  # Double sharpness\n",
    "        \n",
    "        output_img_array[y1:y2, x1:x2] = np.array(logo_sharpened)\n",
    "        output_image = Image.fromarray(output_img_array)\n",
    "    \n",
    "    print(f\"‚úÖ Generated ultra-quality product with sharp logo\")\n",
    "    \n",
    "    return output_image\n",
    "\n",
    "\n",
    "# ===== CHECKPOINT 3: TUNED Attention Aggregation =====\n",
    "def extract_tokens(pipe, prompt: str, num_tokens: int = 6) -> List[str]:\n",
    "    \"\"\"Extract meaningful tokens\"\"\"\n",
    "    tokens_raw = pipe.tokenizer.encode(prompt)\n",
    "    tokens_decoded = [pipe.tokenizer.decode([t]) for t in tokens_raw]\n",
    "    \n",
    "    stopwords = {\n",
    "        '<|startoftext|>', '<|endoftext|>', ',', '.', 'the', 'a', 'an',\n",
    "        'to', 'with', 'and', 'for', 'into', 'of', 'in', 'on', 'is', 'are',\n",
    "    }\n",
    "    \n",
    "    tokens = [t.strip() for t in tokens_decoded \n",
    "              if t.strip() and t.strip() not in stopwords][:num_tokens]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def aggregate_cross_attention_tuned(attention_store: Dict, \n",
    "                                   num_text_tokens: int,\n",
    "                                   output_size: Tuple[int, int] = (512, 512)) -> Dict[int, np.ndarray]:\n",
    "    \"\"\"\n",
    "    TUNED aggregation:\n",
    "    1. Use only final timesteps (discard noisy early steps)\n",
    "    2. Aggressive weighting towards final steps\n",
    "    3. Gaussian blur for smoother maps\n",
    "    4. Enhanced saturation\n",
    "    \"\"\"\n",
    "    checkpoint_print(3, \"TUNED Cross-Attention Extraction\")\n",
    "    \n",
    "    if not attention_store:\n",
    "        print(\"‚ö†Ô∏è No attention maps!\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"  Total timesteps: {len(attention_store)}\")\n",
    "    \n",
    "    # Group by spatial resolution\n",
    "    attention_by_resolution = {}\n",
    "    for step_key, attention_list in attention_store.items():\n",
    "        for attn in attention_list:\n",
    "            res_key = attn.shape[0]\n",
    "            if res_key not in attention_by_resolution:\n",
    "                attention_by_resolution[res_key] = []\n",
    "            attention_by_resolution[res_key].append(attn)\n",
    "    \n",
    "    print(f\"  Spatial resolutions: {sorted(attention_by_resolution.keys())}\")\n",
    "    \n",
    "    max_resolution = max(attention_by_resolution.keys())\n",
    "    high_res_attentions = torch.stack(attention_by_resolution[max_resolution])\n",
    "    \n",
    "    print(f\"  Using resolution: {max_resolution}x{int(np.sqrt(max_resolution))}\")\n",
    "    \n",
    "    # ===== TUNING: Use only final timesteps =====\n",
    "    num_steps = high_res_attentions.shape[0]\n",
    "    if ATTENTION_PARAMS[\"use_final_steps_only\"]:\n",
    "        final_ratio = ATTENTION_PARAMS[\"final_steps_ratio\"]\n",
    "        cutoff = int(num_steps * (1 - final_ratio))\n",
    "        high_res_attentions = high_res_attentions[cutoff:]\n",
    "        num_steps_used = high_res_attentions.shape[0]\n",
    "        print(f\"  Using only final {final_ratio*100:.0f}% of steps ({num_steps_used}/{num_steps})\")\n",
    "    else:\n",
    "        num_steps_used = num_steps\n",
    "    \n",
    "    # ===== TUNING: Aggressive weighting towards final steps =====\n",
    "    weights = torch.linspace(0.1, 2.0, high_res_attentions.shape[0])  # Steeper curve\n",
    "    weights = weights / weights.sum()\n",
    "    print(f\"  Using aggressive weighting (exponential towards final steps)\")\n",
    "    \n",
    "    weighted_attention = (high_res_attentions * weights.view(-1, 1, 1)).sum(dim=0)\n",
    "    \n",
    "    num_spatial, num_text = weighted_attention.shape\n",
    "    spatial_h = spatial_w = int(np.sqrt(num_spatial))\n",
    "    \n",
    "    print(f\"  Spatial grid: {spatial_h}x{spatial_w}\")\n",
    "    \n",
    "    token_heatmaps = {}\n",
    "    for token_idx in range(min(num_text_tokens, num_text)):\n",
    "        token_attention = weighted_attention[:, token_idx].float().numpy()\n",
    "        \n",
    "        if len(token_attention) < spatial_h * spatial_w:\n",
    "            padded = np.zeros(spatial_h * spatial_w)\n",
    "            padded[:len(token_attention)] = token_attention\n",
    "            token_attention = padded\n",
    "        else:\n",
    "            token_attention = token_attention[:spatial_h * spatial_w]\n",
    "        \n",
    "        heatmap = token_attention.reshape(spatial_h, spatial_w)\n",
    "        \n",
    "        # ===== TUNING: Gaussian blur for smoother maps =====\n",
    "        from scipy.ndimage import gaussian_filter\n",
    "        heatmap = gaussian_filter(heatmap, sigma=ATTENTION_PARAMS[\"blur_sigma\"])\n",
    "        \n",
    "        # Upscale\n",
    "        scale_h = output_size[0] / heatmap.shape[0]\n",
    "        scale_w = output_size[1] / heatmap.shape[1]\n",
    "        heatmap_resized = zoom(heatmap, (scale_h, scale_w), order=3)\n",
    "        \n",
    "        # Normalize per-token\n",
    "        hmin, hmax = heatmap_resized.min(), heatmap_resized.max()\n",
    "        if hmax > hmin:\n",
    "            heatmap_resized = (heatmap_resized - hmin) / (hmax - hmin)\n",
    "        else:\n",
    "            heatmap_resized = np.ones_like(heatmap_resized) * 0.5\n",
    "        \n",
    "        # ===== TUNING: Enhanced saturation =====\n",
    "        heatmap_resized = np.power(heatmap_resized, 1.0 / ATTENTION_PARAMS[\"attention_saturation\"])\n",
    "        \n",
    "        token_heatmaps[token_idx] = heatmap_resized\n",
    "        print(f\"    Token {token_idx}: range=[{hmin:.4f}, {hmax:.4f}]\")\n",
    "    \n",
    "    print(f\"‚úÖ Extracted {len(token_heatmaps)} ultra-sharp attention maps\")\n",
    "    return token_heatmaps\n",
    "\n",
    "\n",
    "# ===== CHECKPOINT 4: Enhanced Figure 4 Visualization =====\n",
    "def visualize_figure4_enhanced(output_image: Image.Image,\n",
    "                              token_heatmaps: Dict[int, np.ndarray],\n",
    "                              tokens: List[str],\n",
    "                              prompt: str,\n",
    "                              output_path: Path):\n",
    "    \"\"\"Create enhanced Figure 4 with better contrast and saturation\"\"\"\n",
    "    checkpoint_print(4, \"Enhanced Figure 4 Visualization\")\n",
    "    \n",
    "    num_tokens = min(6, len(token_heatmaps))\n",
    "    print(f\"  Creating {num_tokens}-token enhanced visualization\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, num_tokens, figsize=(4*num_tokens, 8), dpi=150)\n",
    "    if num_tokens == 1:\n",
    "        axes = axes.reshape(2, 1)\n",
    "    \n",
    "    output_array = np.array(output_image)\n",
    "    colormap = plt.get_cmap('RdBu_r')\n",
    "    \n",
    "    for i in range(num_tokens):\n",
    "        if i not in token_heatmaps:\n",
    "            continue\n",
    "        \n",
    "        heatmap = token_heatmaps[i]\n",
    "        token_name = tokens[i] if i < len(tokens) else f\"token_{i}\"\n",
    "        \n",
    "        # Top: image\n",
    "        axes[0, i].imshow(output_image)\n",
    "        axes[0, i].set_title(f'\"{token_name}\"', fontsize=14, fontweight='bold', color='white',\n",
    "                            bbox=dict(boxstyle='round', facecolor='black', alpha=0.5))\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # Bottom: enhanced heatmap overlay\n",
    "        heatmap_colored = colormap(heatmap)[:, :, :3]\n",
    "        \n",
    "        # ===== ENHANCED: Better blending for clarity =====\n",
    "        overlay = (output_array / 255.0) * 0.25 + heatmap_colored * 0.75\n",
    "        overlay = np.clip(overlay * 255, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        axes[1, i].imshow(overlay)\n",
    "        axes[1, i].set_title('Cross-Attention', fontsize=11, fontweight='bold')\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    title = f'Average cross-attention maps across all timesteps (100-step refinement)\\n{prompt[:70]}...'\n",
    "    plt.suptitle(title, fontsize=12, y=0.99, weight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    fig_path = output_path / \"figure4_attention_tuned.png\"\n",
    "    plt.savefig(fig_path, dpi=150, bbox_inches='tight', facecolor='white')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"‚úÖ Saved enhanced Figure 4 visualization\")\n",
    "\n",
    "\n",
    "# ===== MAIN PIPELINE =====\n",
    "def generate_product_with_fig4_tuned(pipe,\n",
    "                                    attention_extractor: CrossAttentionExtractorTuned,\n",
    "                                    logo_path: str,\n",
    "                                    product_type: str,\n",
    "                                    output_path: Path):\n",
    "    \"\"\"Full tuned pipeline\"\"\"\n",
    "    print(f\"\\n{'#'*70}\")\n",
    "    print(f\"# PRODUCT: {product_type.upper()}\")\n",
    "    print(f\"{'#'*70}\")\n",
    "    \n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    config = PRODUCTS[product_type]\n",
    "    prompt = config[\"prompt\"]\n",
    "    \n",
    "    # Step 1: Logo + Mask\n",
    "    image_with_logo, mask, logo_coords = create_logo_with_mask(logo_path, product_type)\n",
    "    image_with_logo.save(output_path / \"01_logo_canvas.png\")\n",
    "    mask.save(output_path / \"02_inpaint_mask.png\")\n",
    "    \n",
    "    # Step 2: Ultra-quality generation\n",
    "    output_image = generate_product_ultra_quality(\n",
    "        pipe, attention_extractor,\n",
    "        image_with_logo, mask, logo_coords,\n",
    "        prompt,\n",
    "        GENERATION_PARAMS[\"seed\"]\n",
    "    )\n",
    "    output_image.save(output_path / \"03_generated_product_tuned.png\")\n",
    "    \n",
    "    # Step 3: Extract tokens\n",
    "    tokens = extract_tokens(pipe, prompt, num_tokens=6)\n",
    "    print(f\"\\n  Key tokens: {tokens}\")\n",
    "    \n",
    "    # Step 4: Tuned attention aggregation\n",
    "    token_heatmaps = aggregate_cross_attention_tuned(\n",
    "        attention_extractor.attention_store,\n",
    "        num_text_tokens=len(tokens),\n",
    "        output_size=(512, 512)\n",
    "    )\n",
    "    \n",
    "    if token_heatmaps:\n",
    "        # Step 5: Enhanced visualization\n",
    "        visualize_figure4_enhanced(output_image, token_heatmaps, tokens, prompt, output_path)\n",
    "    \n",
    "    return output_image\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üé® TUNED FOR MAXIMUM QUALITY: FIGURE 4 ATTENTION VISUALIZATION\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nTuning enhancements:\")\n",
    "    print(\"  ‚úì 100 denoising steps (2x refinement)\")\n",
    "    print(\"  ‚úì 11.0 guidance scale (stronger text alignment)\")\n",
    "    print(\"  ‚úì Final-timesteps-only attention (discard noise)\")\n",
    "    print(\"  ‚úì Gaussian blur + enhanced saturation\")\n",
    "    print(\"  ‚úì Multi-pass logo sharpening\")\n",
    "    print(\"  ‚úì Enhanced contrast blending\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\n‚è±Ô∏è WARNING: May take 2-3 minutes per product\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    pipe = setup_pipeline()\n",
    "    attention_extractor = install_attention_extractor(pipe)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for product_type in PRODUCTS.keys():\n",
    "        output_path = Path(OUTPUT_DIR) / product_type\n",
    "        try:\n",
    "            output_img = generate_product_with_fig4_tuned(\n",
    "                pipe, attention_extractor,\n",
    "                INPUT_LOGO, product_type,\n",
    "                output_path\n",
    "            )\n",
    "            results[product_type] = output_img\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        \n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ TUNED PIPELINE COMPLETE!\")\n",
    "    print(f\"üìÅ Results: {OUTPUT_DIR}/\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nExpect to see:\")\n",
    "    print(\"  ‚úì Ultra-sharp logos\")\n",
    "    print(\"  ‚úì Clean, sparse attention maps\")\n",
    "    print(\"  ‚úì Clear spatial localization per token\")\n",
    "    print(\"  ‚úì Professional Figure 4-style visualization\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    del pipe\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba8d90a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't connect to the Hub: 404 Client Error. (Request ID: Root=1-6919ce5b-284f50d7289842a85d3c4d3b;5b4c37de-afc9-4c17-bfcd-65002a4ceff6)\n",
      "\n",
      "Revision Not Found for url: https://huggingface.co/api/models/stabilityai/sd-turbo/revision/fp16.\n",
      "Invalid rev id: fp16.\n",
      "Will try to load from local cache.\n",
      "Couldn't connect to the Hub: 404 Client Error. (Request ID: Root=1-6919ce5b-7ee8eccb46f12bf20c76f90d;06163c2c-3940-4973-8496-2973eccbed3b)\n",
      "\n",
      "Revision Not Found for url: https://huggingface.co/api/models/stable-diffusion-v1-5/stable-diffusion-v1-5/revision/fp16.\n",
      "Invalid rev id: fp16.\n",
      "Will try to load from local cache.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üé® LOGO EMBEDDED IN PRODUCTS: FIGURE 4 ATTENTION VISUALIZATION\n",
      "======================================================================\n",
      "\n",
      "Key changes:\n",
      "  ‚úì Logo is NOW PART of input image\n",
      "  ‚úì Prompt mentions logo explicitly\n",
      "  ‚úì Model generates product WITH logo embedded\n",
      "  ‚úì Logo appears on t-shirt chest, mug front, gift bag front\n",
      "  ‚úì 100-step ultra-quality generation\n",
      "======================================================================\n",
      "\n",
      "‚è±Ô∏è May take 2-3 minutes per product\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üìç CHECKPOINT 0: Pipeline Setup (Logo Embedded)\n",
      "======================================================================\n",
      "üîÑ Loading Stable Diffusion...\n",
      "   Steps: 100\n",
      "   Guidance: 11.0\n",
      "   Trying stabilityai/sd-turbo...\n",
      "   ‚ùå Failed: Cannot load model stabilityai/sd-turbo: model is not cached locally and an error\n",
      "   Trying runwayml/stable-diffusion-v1-5...\n",
      "   ‚ùå Failed: Cannot load model runwayml/stable-diffusion-v1-5: model is not cached locally an\n",
      "   Trying CompVis/stable-diffusion-v1-4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c7f7d5ccc943e187279f77035523fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch /home/ec2-user/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-4/snapshots/2880f2ca379f41b0226444936bb7a6766a227587/vae: Error no file named diffusion_pytorch_model.safetensors found in directory /home/ec2-user/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-4/snapshots/2880f2ca379f41b0226444936bb7a6766a227587/vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch /home/ec2-user/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-4/snapshots/2880f2ca379f41b0226444936bb7a6766a227587/unet: Error no file named diffusion_pytorch_model.safetensors found in directory /home/ec2-user/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-4/snapshots/2880f2ca379f41b0226444936bb7a6766a227587/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Loaded CompVis/stable-diffusion-v1-4\n",
      "‚úÖ Pipeline ready on cuda\n",
      "‚úÖ Installed attention extractor on 0 layers\n",
      "\n",
      "######################################################################\n",
      "# PRODUCT: TSHIRT (LOGO EMBEDDED)\n",
      "######################################################################\n",
      "\n",
      "======================================================================\n",
      "üìç CHECKPOINT 1: Create Input Image with Embedded Logo\n",
      "======================================================================\n",
      "  Logo size: 179x179\n",
      "  Position: (167, 64)\n",
      "  Logo embedding method: soft_mask\n",
      "‚úÖ Input image with embedded logo created\n",
      "\n",
      "======================================================================\n",
      "üìç CHECKPOINT 2: Generate with Logo Embedding (100 steps)\n",
      "======================================================================\n",
      "  Prompt: black cotton t-shirt on mannequin with blue hexagon logo on chest, spotlight fro...\n",
      "  Logo is PART of the input image\n",
      "  Model will incorporate logo into generation\n",
      "  Generating product (2-3 minutes)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b96de260d8df436f87f288643a0b17f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Step 0/100\n",
      "    Step 20/100\n",
      "    Step 40/100\n",
      "    Step 60/100\n",
      "    Step 80/100\n",
      "    Step 100/100\n",
      "‚úÖ Generated product with embedded logo\n",
      "\n",
      "  Key tokens: ['black', 'cotton', 't', '-', 'shirt', 'mannequin']\n",
      "\n",
      "======================================================================\n",
      "üìç CHECKPOINT 3: Tuned Cross-Attention Extraction\n",
      "======================================================================\n",
      "  Total timesteps: 101\n",
      "  Spatial resolutions: [64, 256, 1024, 4096]\n",
      "  Using resolution: 4096x64\n",
      "  Using final 50% of steps (253/505)\n",
      "  Spatial grid: 64x64\n",
      "‚úÖ Extracted 6 attention maps\n",
      "\n",
      "======================================================================\n",
      "üìç CHECKPOINT 4: Enhanced Figure 4 Visualization\n",
      "======================================================================\n",
      "  Creating 6-token visualization\n",
      "‚úÖ Saved visualization with logo visible\n",
      "\n",
      "######################################################################\n",
      "# PRODUCT: MUG (LOGO EMBEDDED)\n",
      "######################################################################\n",
      "\n",
      "======================================================================\n",
      "üìç CHECKPOINT 1: Create Input Image with Embedded Logo\n",
      "======================================================================\n",
      "  Logo size: 204x204\n",
      "  Position: (154, 154)\n",
      "  Logo embedding method: soft_mask\n",
      "‚úÖ Input image with embedded logo created\n",
      "\n",
      "======================================================================\n",
      "üìç CHECKPOINT 2: Generate with Logo Embedding (100 steps)\n",
      "======================================================================\n",
      "  Prompt: ceramic coffee mug on wooden table with blue hexagon logo printed on the front, ...\n",
      "  Logo is PART of the input image\n",
      "  Model will incorporate logo into generation\n",
      "  Generating product (2-3 minutes)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3f68f8c50164090894797994bab81f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Step 0/100\n",
      "    Step 20/100\n",
      "    Step 40/100\n",
      "    Step 60/100\n",
      "    Step 80/100\n",
      "    Step 100/100\n",
      "‚úÖ Generated product with embedded logo\n",
      "\n",
      "  Key tokens: ['ceramic', 'coffee', 'mug', 'wooden', 'table', 'blue']\n",
      "\n",
      "======================================================================\n",
      "üìç CHECKPOINT 3: Tuned Cross-Attention Extraction\n",
      "======================================================================\n",
      "  Total timesteps: 101\n",
      "  Spatial resolutions: [64, 256, 1024, 4096]\n",
      "  Using resolution: 4096x64\n",
      "  Using final 50% of steps (253/505)\n",
      "  Spatial grid: 64x64\n",
      "‚úÖ Extracted 6 attention maps\n",
      "\n",
      "======================================================================\n",
      "üìç CHECKPOINT 4: Enhanced Figure 4 Visualization\n",
      "======================================================================\n",
      "  Creating 6-token visualization\n",
      "‚úÖ Saved visualization with logo visible\n",
      "\n",
      "######################################################################\n",
      "# PRODUCT: GIFTBAG (LOGO EMBEDDED)\n",
      "######################################################################\n",
      "\n",
      "======================================================================\n",
      "üìç CHECKPOINT 1: Create Input Image with Embedded Logo\n",
      "======================================================================\n",
      "  Logo size: 153x153\n",
      "  Position: (180, 52)\n",
      "  Logo embedding method: soft_mask\n",
      "‚úÖ Input image with embedded logo created\n",
      "\n",
      "======================================================================\n",
      "üìç CHECKPOINT 2: Generate with Logo Embedding (100 steps)\n",
      "======================================================================\n",
      "  Prompt: luxury red gift bag with blue hexagon logo on the front, gold ribbon bow on marb...\n",
      "  Logo is PART of the input image\n",
      "  Model will incorporate logo into generation\n",
      "  Generating product (2-3 minutes)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d425ac6fed5498fa3e847abc4e70c80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Step 0/100\n",
      "    Step 20/100\n",
      "    Step 40/100\n",
      "    Step 60/100\n",
      "    Step 80/100\n",
      "    Step 100/100\n",
      "‚úÖ Generated product with embedded logo\n",
      "\n",
      "  Key tokens: ['luxury', 'red', 'gift', 'bag', 'blue', 'hex']\n",
      "\n",
      "======================================================================\n",
      "üìç CHECKPOINT 3: Tuned Cross-Attention Extraction\n",
      "======================================================================\n",
      "  Total timesteps: 101\n",
      "  Spatial resolutions: [64, 256, 1024, 4096]\n",
      "  Using resolution: 4096x64\n",
      "  Using final 50% of steps (253/505)\n",
      "  Spatial grid: 64x64\n",
      "‚úÖ Extracted 6 attention maps\n",
      "\n",
      "======================================================================\n",
      "üìç CHECKPOINT 4: Enhanced Figure 4 Visualization\n",
      "======================================================================\n",
      "  Creating 6-token visualization\n",
      "‚úÖ Saved visualization with logo visible\n",
      "\n",
      "======================================================================\n",
      "‚úÖ LOGO EMBEDDED PIPELINE COMPLETE!\n",
      "üìÅ Results: product_generation_fig4_logo_embedded/\n",
      "======================================================================\n",
      "\n",
      "You should see:\n",
      "  ‚úì Logo visible ON the product (not as overlay)\n",
      "  ‚úì Logo integrated into t-shirt/mug/gift bag\n",
      "  ‚úì Clean attention maps showing spatial patterns\n",
      "  ‚úì Professional product photography with embedded logo\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LOGO EMBEDDED IN PRODUCTS: INPAINTING + FIGURE 4 ATTENTION\n",
    "# ============================================================================\n",
    "#\n",
    "# KEY CHANGE: Logo is now part of the INPUT image during generation\n",
    "# - Model sees logo on white canvas\n",
    "# - Model is guided: \"put this logo on the product\"\n",
    "# - Logo becomes PART OF the generated image, not overlay\n",
    "#\n",
    "# ============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from PIL import Image, ImageDraw, ImageFilter, ImageEnhance\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import gc\n",
    "from scipy.ndimage import zoom\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "INPUT_LOGO = \"holistic.png\"\n",
    "OUTPUT_DIR = \"product_generation_fig4_logo_embedded\"\n",
    "\n",
    "PRODUCTS = {\n",
    "    \"tshirt\": {\n",
    "        # ===== KEY CHANGE: Logo embedding prompt =====\n",
    "        # Tell the model: \"generate product WITH logo on it\"\n",
    "        \"prompt\": \"black cotton t-shirt on mannequin with blue hexagon logo on chest, spotlight from left, geometric shadows on right, modern studio background\",\n",
    "        \"logo_size\": 0.35,\n",
    "        \"logo_pos\": (0.5, 0.3),\n",
    "        \"mask_expand\": 1.5,\n",
    "    },\n",
    "    \"mug\": {\n",
    "        # ===== KEY CHANGE: Include logo in prompt =====\n",
    "        \"prompt\": \"ceramic coffee mug on wooden table with blue hexagon logo printed on the front, snowflake decorations and cinnamon sticks, warm lighting, cozy winter atmosphere\",\n",
    "        \"logo_size\": 0.4,\n",
    "        \"logo_pos\": (0.5, 0.5),\n",
    "        \"mask_expand\": 1.3,\n",
    "    },\n",
    "    \"giftbag\": {\n",
    "        # ===== KEY CHANGE: Include logo in prompt =====\n",
    "        \"prompt\": \"luxury red gift bag with blue hexagon logo on the front, gold ribbon bow on marble surface, soft shadows, elegant presentation\",\n",
    "        \"logo_size\": 0.3,\n",
    "        \"logo_pos\": (0.5, 0.25),\n",
    "        \"mask_expand\": 1.2,\n",
    "    }\n",
    "}\n",
    "\n",
    "# ===== TUNED PARAMETERS FOR MAXIMUM QUALITY =====\n",
    "GENERATION_PARAMS = {\n",
    "    \"num_inference_steps\": 100,\n",
    "    \"guidance_scale\": 11.0,\n",
    "    \"height\": 512,\n",
    "    \"width\": 512,\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "NEGATIVE_PROMPT = \"blurry, distorted, low quality, watermark, text, cropped, deformed, multiple objects\"\n",
    "\n",
    "# ===== ATTENTION TUNING =====\n",
    "ATTENTION_PARAMS = {\n",
    "    \"use_final_steps_only\": True,\n",
    "    \"final_steps_ratio\": 0.5,\n",
    "    \"attention_saturation\": 1.3,\n",
    "    \"blur_sigma\": 2.0,\n",
    "}\n",
    "\n",
    "# ===== LOGO EMBEDDING PARAMS (NEW) =====\n",
    "LOGO_EMBEDDING_PARAMS = {\n",
    "    \"use_logo_as_input\": True,                      # ‚úì Logo is part of input\n",
    "    \"preserve_logo_strength\": 0.7,                  # How strongly to preserve logo\n",
    "    \"blend_logo_with_generated\": True,              # Allow model to enhance logo\n",
    "    \"logo_preservation_method\": \"soft_mask\",        # \"hard\" or \"soft\"\n",
    "}\n",
    "\n",
    "# ===== CROSS-ATTENTION EXTRACTOR =====\n",
    "class CrossAttentionExtractorTuned:\n",
    "    def __init__(self):\n",
    "        self.attention_store = {}\n",
    "        self.step_index = 0\n",
    "        self.timesteps = []\n",
    "    \n",
    "    def __call__(self, attn, hidden_states, encoder_hidden_states=None, attention_mask=None):\n",
    "        batch_size, sequence_length, _ = hidden_states.shape\n",
    "        query = attn.to_q(hidden_states)\n",
    "        is_cross = encoder_hidden_states is not None\n",
    "        \n",
    "        if is_cross:\n",
    "            key = attn.to_k(encoder_hidden_states)\n",
    "            value = attn.to_v(encoder_hidden_states)\n",
    "        else:\n",
    "            key = attn.to_k(hidden_states)\n",
    "            value = attn.to_v(hidden_states)\n",
    "        \n",
    "        query = attn.head_to_batch_dim(query)\n",
    "        key = attn.head_to_batch_dim(key)\n",
    "        value = attn.head_to_batch_dim(value)\n",
    "        \n",
    "        attention_probs = attn.get_attention_scores(query, key, attention_mask)\n",
    "        \n",
    "        if is_cross:\n",
    "            step_key = f\"step_{self.step_index}\"\n",
    "            if step_key not in self.attention_store:\n",
    "                self.attention_store[step_key] = []\n",
    "            attn_mean = attention_probs.mean(dim=0).cpu()\n",
    "            self.attention_store[step_key].append(attn_mean)\n",
    "        \n",
    "        hidden_states = torch.bmm(attention_probs, value)\n",
    "        hidden_states = attn.batch_to_head_dim(hidden_states)\n",
    "        hidden_states = attn.to_out[0](hidden_states)\n",
    "        hidden_states = attn.to_out[1](hidden_states)\n",
    "        \n",
    "        return hidden_states\n",
    "    \n",
    "    def reset(self):\n",
    "        self.attention_store = {}\n",
    "        self.step_index = 0\n",
    "        self.timesteps = []\n",
    "    \n",
    "    def step(self, timestep=None):\n",
    "        self.step_index += 1\n",
    "        if timestep is not None:\n",
    "            self.timesteps.append(timestep)\n",
    "\n",
    "\n",
    "def checkpoint_print(num: int, name: str):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìç CHECKPOINT {num}: {name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "\n",
    "def setup_pipeline():\n",
    "    checkpoint_print(0, \"Pipeline Setup (Logo Embedded)\")\n",
    "    \n",
    "    print(\"üîÑ Loading Stable Diffusion...\")\n",
    "    print(f\"   Steps: {GENERATION_PARAMS['num_inference_steps']}\")\n",
    "    print(f\"   Guidance: {GENERATION_PARAMS['guidance_scale']}\")\n",
    "    \n",
    "    models_to_try = [\n",
    "        (\"stabilityai/sd-turbo\", \"‚ö° Turbo\"),\n",
    "        (\"runwayml/stable-diffusion-v1-5\", \"Standard\"),\n",
    "        (\"CompVis/stable-diffusion-v1-4\", \"Classic\"),\n",
    "    ]\n",
    "    \n",
    "    pipe = None\n",
    "    for model_id, description in models_to_try:\n",
    "        try:\n",
    "            print(f\"   Trying {model_id}...\")\n",
    "            pipe = StableDiffusionPipeline.from_pretrained(\n",
    "                model_id,\n",
    "                torch_dtype=torch.float16,\n",
    "                safety_checker=None,\n",
    "                revision=\"fp16\"\n",
    "            )\n",
    "            print(f\"   ‚úÖ Loaded {model_id}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Failed: {str(e)[:80]}\")\n",
    "            continue\n",
    "    \n",
    "    if pipe is None:\n",
    "        raise RuntimeError(\"Could not load any model\")\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    pipe = pipe.to(device)\n",
    "    print(f\"‚úÖ Pipeline ready on {device}\")\n",
    "    \n",
    "    return pipe\n",
    "\n",
    "\n",
    "def install_attention_extractor(pipe):\n",
    "    attention_extractor = CrossAttentionExtractorTuned()\n",
    "    attn_procs = {}\n",
    "    for name in pipe.unet.attn_processors.keys():\n",
    "        attn_procs[name] = attention_extractor\n",
    "    pipe.unet.set_attn_processor(attn_procs)\n",
    "    print(f\"‚úÖ Installed attention extractor on {len(attn_procs)} layers\")\n",
    "    return attention_extractor\n",
    "\n",
    "\n",
    "# ===== CHECKPOINT 1: Create Input Image with Embedded Logo =====\n",
    "def create_input_with_embedded_logo(logo_path: str, product_type: str, size: int = 512) -> Tuple[Image.Image, Image.Image, Tuple]:\n",
    "    \"\"\"\n",
    "    ===== KEY CHANGE: Create input image with logo VISIBLE =====\n",
    "    \n",
    "    Before: Logo on white canvas, mask to preserve it\n",
    "    Now: Logo on white canvas is the INPUT to the model\n",
    "    \n",
    "    The model generates the product and is prompted to include the logo\n",
    "    \"\"\"\n",
    "    checkpoint_print(1, \"Create Input Image with Embedded Logo\")\n",
    "    \n",
    "    logo = Image.open(logo_path).convert(\"RGBA\")\n",
    "    config = PRODUCTS[product_type]\n",
    "    \n",
    "    # Start with white canvas\n",
    "    image = Image.new(\"RGB\", (size, size), (255, 255, 255))\n",
    "    \n",
    "    # Calculate logo size and position\n",
    "    logo_size = int(size * config[\"logo_size\"])\n",
    "    logo_resized = logo.resize((logo_size, logo_size), Image.Resampling.LANCZOS)\n",
    "    \n",
    "    pos_x = int(size * config[\"logo_pos\"][0] - logo_size // 2)\n",
    "    pos_y = int(size * config[\"logo_pos\"][1] - logo_size // 2)\n",
    "    \n",
    "    print(f\"  Logo size: {logo_size}x{logo_size}\")\n",
    "    print(f\"  Position: ({pos_x}, {pos_y})\")\n",
    "    \n",
    "    # ===== KEY CHANGE: Paste logo onto input image =====\n",
    "    # Model will see this logo and incorporate it into the generated product\n",
    "    image_rgba = Image.new(\"RGBA\", (size, size), (255, 255, 255, 255))\n",
    "    image_rgba.paste(logo_resized, (pos_x, pos_y), logo_resized)\n",
    "    image = Image.new(\"RGB\", (size, size), (255, 255, 255))\n",
    "    image.paste(image_rgba.convert(\"RGB\"), (0, 0))\n",
    "    \n",
    "    # ===== NEW: Create soft mask for logo preservation =====\n",
    "    # This mask helps the model know where the logo should be\n",
    "    mask = Image.new(\"L\", (size, size), 255)\n",
    "    mask_draw = ImageDraw.Draw(mask)\n",
    "    \n",
    "    # Mark logo region so model understands it's important\n",
    "    expand = int(logo_size * (config[\"mask_expand\"] - 1) / 2)\n",
    "    \n",
    "    if LOGO_EMBEDDING_PARAMS[\"logo_preservation_method\"] == \"hard\":\n",
    "        # Hard mask: strictly preserve logo\n",
    "        mask_draw.rectangle(\n",
    "            [pos_x - expand, pos_y - expand, pos_x + logo_size + expand, pos_y + logo_size + expand],\n",
    "            fill=0\n",
    "        )\n",
    "    else:\n",
    "        # Soft mask: allow model to enhance/modify logo slightly\n",
    "        # Draw feathered region\n",
    "        for y in range(max(0, pos_y - expand), min(size, pos_y + logo_size + expand)):\n",
    "            for x in range(max(0, pos_x - expand), min(size, pos_x + logo_size + expand)):\n",
    "                dist = min(\n",
    "                    abs(x - pos_x), abs(x - (pos_x + logo_size)),\n",
    "                    abs(y - pos_y), abs(y - (pos_y + logo_size))\n",
    "                )\n",
    "                if dist < expand:\n",
    "                    # Feathered edge\n",
    "                    mask_draw.point((x, y), fill=int(255 * (dist / expand)))\n",
    "                elif pos_x <= x < pos_x + logo_size and pos_y <= y < pos_y + logo_size:\n",
    "                    mask_draw.point((x, y), fill=0)\n",
    "    \n",
    "    print(f\"  Logo embedding method: {LOGO_EMBEDDING_PARAMS['logo_preservation_method']}\")\n",
    "    print(f\"‚úÖ Input image with embedded logo created\")\n",
    "    \n",
    "    logo_coords = (pos_x, pos_y, pos_x + logo_size, pos_y + logo_size)\n",
    "    return image, mask, logo_coords\n",
    "\n",
    "\n",
    "# ===== CHECKPOINT 2: Generate with Logo Embedding =====\n",
    "def generate_with_embedded_logo(pipe,\n",
    "                               attention_extractor: CrossAttentionExtractorTuned,\n",
    "                               input_image: Image.Image,\n",
    "                               mask: Image.Image,\n",
    "                               logo_coords: Tuple,\n",
    "                               prompt: str,\n",
    "                               seed: int) -> Image.Image:\n",
    "    \"\"\"\n",
    "    ===== KEY CHANGE: Generate product WITH logo in prompt and as input =====\n",
    "    \n",
    "    Strategy:\n",
    "    1. Input image has logo visible\n",
    "    2. Prompt mentions logo (e.g., \"with blue hexagon logo on chest\")\n",
    "    3. Model generates product AND incorporates logo\n",
    "    4. Apply soft preservation to keep logo visible\n",
    "    \"\"\"\n",
    "    checkpoint_print(2, \"Generate with Logo Embedding (100 steps)\")\n",
    "    \n",
    "    print(f\"  Prompt: {prompt[:80]}...\")\n",
    "    print(f\"  Logo is PART of the input image\")\n",
    "    print(f\"  Model will incorporate logo into generation\")\n",
    "    \n",
    "    attention_extractor.reset()\n",
    "    \n",
    "    input_image = input_image.resize((512, 512))\n",
    "    mask = mask.resize((512, 512))\n",
    "    \n",
    "    def callback_on_step_end(pipe, step_idx, timestep, callback_kwargs):\n",
    "        attention_extractor.step(timestep)\n",
    "        if step_idx % 20 == 0:\n",
    "            print(f\"    Step {step_idx}/{GENERATION_PARAMS['num_inference_steps']}\")\n",
    "        return callback_kwargs\n",
    "    \n",
    "    print(\"  Generating product (2-3 minutes)...\")\n",
    "    with torch.no_grad():\n",
    "        output = pipe(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=NEGATIVE_PROMPT,\n",
    "            height=512,\n",
    "            width=512,\n",
    "            num_inference_steps=GENERATION_PARAMS[\"num_inference_steps\"],\n",
    "            guidance_scale=GENERATION_PARAMS[\"guidance_scale\"],\n",
    "            generator=torch.Generator(device=pipe.device).manual_seed(seed),\n",
    "            callback_on_step_end=callback_on_step_end\n",
    "        )\n",
    "    \n",
    "    output_image = output.images[0]\n",
    "    \n",
    "    # ===== PRESERVE LOGO VISIBILITY =====\n",
    "    # Blend generated image with input to ensure logo stays visible\n",
    "    x1, y1, x2, y2 = logo_coords\n",
    "    logo_region_original = input_image.crop((x1, y1, x2, y2))\n",
    "    \n",
    "    output_array = np.array(output_image)\n",
    "    input_array = np.array(input_image)\n",
    "    \n",
    "    # Create blend mask around logo\n",
    "    blend_mask = np.ones((512, 512), dtype=np.float32)\n",
    "    feather = 20\n",
    "    \n",
    "    for y in range(max(0, y1 - feather), min(512, y2 + feather)):\n",
    "        for x in range(max(0, x1 - feather), min(512, x2 + feather)):\n",
    "            dist = min(\n",
    "                abs(x - x1), abs(x - x2),\n",
    "                abs(y - y1), abs(y - y2)\n",
    "            )\n",
    "            if dist < feather:\n",
    "                # Feathered blend\n",
    "                blend_mask[y, x] = (dist / feather) ** 1.5\n",
    "            elif x1 <= x < x2 and y1 <= y < y2:\n",
    "                # Logo region: use generated with slight input blend\n",
    "                strength = LOGO_EMBEDDING_PARAMS[\"preserve_logo_strength\"]\n",
    "                blend_mask[y, x] = strength  # 0.7 = 70% generated, 30% original\n",
    "    \n",
    "    blend_mask = blend_mask[:, :, np.newaxis]\n",
    "    \n",
    "    output_array_blended = (\n",
    "        input_array * (1 - blend_mask) +\n",
    "        output_array * blend_mask\n",
    "    ).astype(np.uint8)\n",
    "    \n",
    "    output_image = Image.fromarray(output_array_blended)\n",
    "    print(f\"‚úÖ Generated product with embedded logo\")\n",
    "    \n",
    "    return output_image\n",
    "\n",
    "\n",
    "# ===== CHECKPOINT 3: Tuned Attention Aggregation =====\n",
    "def extract_tokens(pipe, prompt: str, num_tokens: int = 6) -> List[str]:\n",
    "    tokens_raw = pipe.tokenizer.encode(prompt)\n",
    "    tokens_decoded = [pipe.tokenizer.decode([t]) for t in tokens_raw]\n",
    "    \n",
    "    stopwords = {\n",
    "        '<|startoftext|>', '<|endoftext|>', ',', '.', 'the', 'a', 'an',\n",
    "        'to', 'with', 'and', 'for', 'into', 'of', 'in', 'on', 'is', 'are',\n",
    "    }\n",
    "    \n",
    "    tokens = [t.strip() for t in tokens_decoded \n",
    "              if t.strip() and t.strip() not in stopwords][:num_tokens]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def aggregate_cross_attention_tuned(attention_store: Dict, \n",
    "                                   num_text_tokens: int,\n",
    "                                   output_size: Tuple[int, int] = (512, 512)) -> Dict[int, np.ndarray]:\n",
    "    checkpoint_print(3, \"Tuned Cross-Attention Extraction\")\n",
    "    \n",
    "    if not attention_store:\n",
    "        print(\"‚ö†Ô∏è No attention maps!\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"  Total timesteps: {len(attention_store)}\")\n",
    "    \n",
    "    attention_by_resolution = {}\n",
    "    for step_key, attention_list in attention_store.items():\n",
    "        for attn in attention_list:\n",
    "            res_key = attn.shape[0]\n",
    "            if res_key not in attention_by_resolution:\n",
    "                attention_by_resolution[res_key] = []\n",
    "            attention_by_resolution[res_key].append(attn)\n",
    "    \n",
    "    print(f\"  Spatial resolutions: {sorted(attention_by_resolution.keys())}\")\n",
    "    \n",
    "    max_resolution = max(attention_by_resolution.keys())\n",
    "    high_res_attentions = torch.stack(attention_by_resolution[max_resolution])\n",
    "    \n",
    "    print(f\"  Using resolution: {max_resolution}x{int(np.sqrt(max_resolution))}\")\n",
    "    \n",
    "    # Use only final timesteps\n",
    "    num_steps = high_res_attentions.shape[0]\n",
    "    if ATTENTION_PARAMS[\"use_final_steps_only\"]:\n",
    "        final_ratio = ATTENTION_PARAMS[\"final_steps_ratio\"]\n",
    "        cutoff = int(num_steps * (1 - final_ratio))\n",
    "        high_res_attentions = high_res_attentions[cutoff:]\n",
    "        num_steps_used = high_res_attentions.shape[0]\n",
    "        print(f\"  Using final {final_ratio*100:.0f}% of steps ({num_steps_used}/{num_steps})\")\n",
    "    \n",
    "    # Aggressive weighting\n",
    "    weights = torch.linspace(0.1, 2.0, high_res_attentions.shape[0])\n",
    "    weights = weights / weights.sum()\n",
    "    \n",
    "    weighted_attention = (high_res_attentions * weights.view(-1, 1, 1)).sum(dim=0)\n",
    "    \n",
    "    num_spatial, num_text = weighted_attention.shape\n",
    "    spatial_h = spatial_w = int(np.sqrt(num_spatial))\n",
    "    \n",
    "    print(f\"  Spatial grid: {spatial_h}x{spatial_w}\")\n",
    "    \n",
    "    token_heatmaps = {}\n",
    "    for token_idx in range(min(num_text_tokens, num_text)):\n",
    "        token_attention = weighted_attention[:, token_idx].float().numpy()\n",
    "        \n",
    "        if len(token_attention) < spatial_h * spatial_w:\n",
    "            padded = np.zeros(spatial_h * spatial_w)\n",
    "            padded[:len(token_attention)] = token_attention\n",
    "            token_attention = padded\n",
    "        else:\n",
    "            token_attention = token_attention[:spatial_h * spatial_w]\n",
    "        \n",
    "        heatmap = token_attention.reshape(spatial_h, spatial_w)\n",
    "        \n",
    "        # Gaussian blur\n",
    "        from scipy.ndimage import gaussian_filter\n",
    "        heatmap = gaussian_filter(heatmap, sigma=ATTENTION_PARAMS[\"blur_sigma\"])\n",
    "        \n",
    "        # Upscale\n",
    "        scale_h = output_size[0] / heatmap.shape[0]\n",
    "        scale_w = output_size[1] / heatmap.shape[1]\n",
    "        heatmap_resized = zoom(heatmap, (scale_h, scale_w), order=3)\n",
    "        \n",
    "        # Normalize\n",
    "        hmin, hmax = heatmap_resized.min(), heatmap_resized.max()\n",
    "        if hmax > hmin:\n",
    "            heatmap_resized = (heatmap_resized - hmin) / (hmax - hmin)\n",
    "        else:\n",
    "            heatmap_resized = np.ones_like(heatmap_resized) * 0.5\n",
    "        \n",
    "        # Enhanced saturation\n",
    "        heatmap_resized = np.power(heatmap_resized, 1.0 / ATTENTION_PARAMS[\"attention_saturation\"])\n",
    "        \n",
    "        token_heatmaps[token_idx] = heatmap_resized\n",
    "    \n",
    "    print(f\"‚úÖ Extracted {len(token_heatmaps)} attention maps\")\n",
    "    return token_heatmaps\n",
    "\n",
    "\n",
    "# ===== CHECKPOINT 4: Visualization =====\n",
    "def visualize_figure4_enhanced(output_image: Image.Image,\n",
    "                              token_heatmaps: Dict[int, np.ndarray],\n",
    "                              tokens: List[str],\n",
    "                              prompt: str,\n",
    "                              output_path: Path):\n",
    "    checkpoint_print(4, \"Enhanced Figure 4 Visualization\")\n",
    "    \n",
    "    num_tokens = min(6, len(token_heatmaps))\n",
    "    print(f\"  Creating {num_tokens}-token visualization\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, num_tokens, figsize=(4*num_tokens, 8), dpi=150)\n",
    "    if num_tokens == 1:\n",
    "        axes = axes.reshape(2, 1)\n",
    "    \n",
    "    output_array = np.array(output_image)\n",
    "    colormap = plt.get_cmap('RdBu_r')\n",
    "    \n",
    "    for i in range(num_tokens):\n",
    "        if i not in token_heatmaps:\n",
    "            continue\n",
    "        \n",
    "        heatmap = token_heatmaps[i]\n",
    "        token_name = tokens[i] if i < len(tokens) else f\"token_{i}\"\n",
    "        \n",
    "        axes[0, i].imshow(output_image)\n",
    "        axes[0, i].set_title(f'\"{token_name}\"', fontsize=14, fontweight='bold',\n",
    "                            bbox=dict(boxstyle='round', facecolor='black', alpha=0.5),\n",
    "                            color='white')\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        heatmap_colored = colormap(heatmap)[:, :, :3]\n",
    "        overlay = (output_array / 255.0) * 0.25 + heatmap_colored * 0.75\n",
    "        overlay = np.clip(overlay * 255, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        axes[1, i].imshow(overlay)\n",
    "        axes[1, i].set_title('Cross-Attention', fontsize=11, fontweight='bold')\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    title = f'Average cross-attention maps (100-step refinement)\\n{prompt[:70]}...'\n",
    "    plt.suptitle(title, fontsize=12, y=0.99, weight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    fig_path = output_path / \"figure4_attention_logo_embedded.png\"\n",
    "    plt.savefig(fig_path, dpi=150, bbox_inches='tight', facecolor='white')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"‚úÖ Saved visualization with logo visible\")\n",
    "\n",
    "\n",
    "# ===== MAIN PIPELINE =====\n",
    "def generate_product_logo_embedded(pipe,\n",
    "                                  attention_extractor: CrossAttentionExtractorTuned,\n",
    "                                  logo_path: str,\n",
    "                                  product_type: str,\n",
    "                                  output_path: Path):\n",
    "    \"\"\"Full pipeline with logo embedded in product\"\"\"\n",
    "    print(f\"\\n{'#'*70}\")\n",
    "    print(f\"# PRODUCT: {product_type.upper()} (LOGO EMBEDDED)\")\n",
    "    print(f\"{'#'*70}\")\n",
    "    \n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    config = PRODUCTS[product_type]\n",
    "    prompt = config[\"prompt\"]\n",
    "    \n",
    "    # Step 1: Create input with embedded logo\n",
    "    input_image, mask, logo_coords = create_input_with_embedded_logo(logo_path, product_type)\n",
    "    input_image.save(output_path / \"01_input_with_logo.png\")\n",
    "    mask.save(output_path / \"02_logo_mask.png\")\n",
    "    \n",
    "    # Step 2: Generate with logo embedding\n",
    "    output_image = generate_with_embedded_logo(\n",
    "        pipe, attention_extractor,\n",
    "        input_image, mask, logo_coords,\n",
    "        prompt,\n",
    "        GENERATION_PARAMS[\"seed\"]\n",
    "    )\n",
    "    output_image.save(output_path / \"03_product_logo_embedded.png\")\n",
    "    \n",
    "    # Step 3: Extract tokens\n",
    "    tokens = extract_tokens(pipe, prompt, num_tokens=6)\n",
    "    print(f\"\\n  Key tokens: {tokens}\")\n",
    "    \n",
    "    # Step 4: Attention aggregation\n",
    "    token_heatmaps = aggregate_cross_attention_tuned(\n",
    "        attention_extractor.attention_store,\n",
    "        num_text_tokens=len(tokens),\n",
    "        output_size=(512, 512)\n",
    "    )\n",
    "    \n",
    "    if token_heatmaps:\n",
    "        # Step 5: Visualization\n",
    "        visualize_figure4_enhanced(output_image, token_heatmaps, tokens, prompt, output_path)\n",
    "    \n",
    "    return output_image\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üé® LOGO EMBEDDED IN PRODUCTS: FIGURE 4 ATTENTION VISUALIZATION\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nKey changes:\")\n",
    "    print(\"  ‚úì Logo is NOW PART of input image\")\n",
    "    print(\"  ‚úì Prompt mentions logo explicitly\")\n",
    "    print(\"  ‚úì Model generates product WITH logo embedded\")\n",
    "    print(\"  ‚úì Logo appears on t-shirt chest, mug front, gift bag front\")\n",
    "    print(\"  ‚úì 100-step ultra-quality generation\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\n‚è±Ô∏è May take 2-3 minutes per product\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    pipe = setup_pipeline()\n",
    "    attention_extractor = install_attention_extractor(pipe)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for product_type in PRODUCTS.keys():\n",
    "        output_path = Path(OUTPUT_DIR) / product_type\n",
    "        try:\n",
    "            output_img = generate_product_logo_embedded(\n",
    "                pipe, attention_extractor,\n",
    "                INPUT_LOGO, product_type,\n",
    "                output_path\n",
    "            )\n",
    "            results[product_type] = output_img\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        \n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ LOGO EMBEDDED PIPELINE COMPLETE!\")\n",
    "    print(f\"üìÅ Results: {OUTPUT_DIR}/\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nYou should see:\")\n",
    "    print(\"  ‚úì Logo visible ON the product (not as overlay)\")\n",
    "    print(\"  ‚úì Logo integrated into t-shirt/mug/gift bag\")\n",
    "    print(\"  ‚úì Clean attention maps showing spatial patterns\")\n",
    "    print(\"  ‚úì Professional product photography with embedded logo\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    del pipe\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
