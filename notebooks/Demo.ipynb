{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f3deee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded pre-generated results\n",
      "   Original prompt: adapt logo for holiday t-shirt print with seasonal elements\n",
      "   Ablated words: ['elements', 't-shirt', 'holiday', 'seasonal']\n",
      "\n",
      "ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€\n",
      "INTERACTIVE LOGO ANALYSIS AGENT INITIALIZED\n",
      "ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f40d65a9d4f644d0bcaed7dfed8a7cf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<h2 style='text-align:center'>ğŸ¨ FLUX Logo Transform Analysis Agent</h2>\"), HTML(valâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "ğŸ¨ FLUX Logo Transform Analysis Agent\n",
    "Interactive demo showing how prompt modifications affect logo transformations\n",
    "Uses pre-generated results + LLM reasoning (NO re-generation needed)\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import base64\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import io\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "API_ENDPOINT = \"https://ctwa92wg1b.execute-api.us-east-1.amazonaws.com/prod/invoke\"\n",
    "TEAM_ID = \"team_the_great_hack_2025_022\"\n",
    "API_TOKEN = \"znqXT5zCmCynAx-kyx_hldrxvSeyaWvxzx55vB5mfNg\"\n",
    "\n",
    "# Pre-generated results folder (CHANGE THIS to your actual path)\n",
    "RESULTS_FOLDER = \"flux_experiments/run_20251116_013857/tshirt_design/prompt_0_base\"\n",
    "\n",
    "# LLM for agent (fastest Claude)\n",
    "AGENT_MODEL = \"us.anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
    "\n",
    "# ===== LOAD PRE-GENERATED RESULTS =====\n",
    "class LogoAnalysisData:\n",
    "    \"\"\"Holds all pre-generated analysis data\"\"\"\n",
    "    \n",
    "    def __init__(self, results_folder):\n",
    "        self.folder = Path(results_folder)\n",
    "        \n",
    "        # Load metadata\n",
    "        with open(self.folder / \"metadata.json\") as f:\n",
    "            self.metadata = json.load(f)\n",
    "        \n",
    "        self.original_prompt = self.metadata[\"prompt\"]\n",
    "        \n",
    "        # Load images\n",
    "        self.input_logo = Image.open(self.folder / \"input_image.png\")\n",
    "        self.final_output = Image.open(self.folder / \"final_output.png\")\n",
    "        self.word_attribution = Image.open(self.folder / \"word_attribution_complete.png\")\n",
    "        self.evolution_grid = Image.open(self.folder / \"evolution_grid.png\")\n",
    "        \n",
    "        # Load ablated images\n",
    "        self.ablated_images = {}\n",
    "        for img_path in self.folder.glob(\"ablated_without_*.png\"):\n",
    "            word = img_path.stem.replace(\"ablated_without_\", \"\")\n",
    "            self.ablated_images[word] = Image.open(img_path)\n",
    "        \n",
    "        print(f\"âœ… Loaded pre-generated results\")\n",
    "        print(f\"   Original prompt: {self.original_prompt}\")\n",
    "        print(f\"   Ablated words: {list(self.ablated_images.keys())}\")\n",
    "\n",
    "# Initialize data\n",
    "data = LogoAnalysisData(RESULTS_FOLDER)\n",
    "\n",
    "# ===== PROMPT COMPONENTS =====\n",
    "# Extract words from original prompt for modification\n",
    "original_words = data.original_prompt.split()\n",
    "\n",
    "# Define modifiable components\n",
    "STYLE_OPTIONS = [\"festive\", \"elegant\", \"modern\", \"vintage\", \"minimalist\", \"bold\", \"playful\"]\n",
    "ELEMENT_OPTIONS = [\"snowflakes\", \"stars\", \"ornaments\", \"ribbons\", \"candy canes\", \"holly\", \"winter patterns\"]\n",
    "ACTION_OPTIONS = [\"adapt\", \"transform\", \"convert\", \"redesign\", \"stylize\", \"enhance\"]\n",
    "PRODUCT_OPTIONS = [\"t-shirt\", \"mug\", \"gift bag\", \"poster\", \"sticker\", \"card\"]\n",
    "\n",
    "# ===== LLM AGENT =====\n",
    "def call_llm_agent(prompt, images=None):\n",
    "    \"\"\"Call Claude agent for analysis\"\"\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"x-api-key\": API_TOKEN\n",
    "    }\n",
    "    \n",
    "    content = []\n",
    "    \n",
    "    if images:\n",
    "        for img in images:\n",
    "            # Convert PIL Image to base64\n",
    "            buffer = io.BytesIO()\n",
    "            \n",
    "            # Convert RGBA to RGB if needed (JPEG doesn't support transparency)\n",
    "            if img.mode == 'RGBA':\n",
    "                # Create white background\n",
    "                rgb_img = Image.new('RGB', img.size, (255, 255, 255))\n",
    "                rgb_img.paste(img, mask=img.split()[3])  # Use alpha channel as mask\n",
    "                img = rgb_img\n",
    "            elif img.mode != 'RGB':\n",
    "                img = img.convert('RGB')\n",
    "            \n",
    "            img.save(buffer, format='JPEG', quality=85)\n",
    "            buffer.seek(0)\n",
    "            img_b64 = base64.b64encode(buffer.read()).decode('utf-8')\n",
    "            \n",
    "            content.append({\n",
    "                \"type\": \"image\",\n",
    "                \"source\": {\n",
    "                    \"type\": \"base64\",\n",
    "                    \"media_type\": \"image/jpeg\",\n",
    "                    \"data\": img_b64\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    content.append({\"type\": \"text\", \"text\": prompt})\n",
    "    \n",
    "    payload = {\n",
    "        \"team_id\": TEAM_ID,\n",
    "        \"api_token\": API_TOKEN,\n",
    "        \"model\": AGENT_MODEL,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": content}],\n",
    "        \"max_tokens\": 1500,\n",
    "        \"temperature\": 0.4\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(API_ENDPOINT, headers=headers, json=payload, timeout=60)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            if \"content\" in result and len(result[\"content\"]) > 0:\n",
    "                return result[\"content\"][0][\"text\"]\n",
    "        \n",
    "        return f\"ERROR: {response.status_code}\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"ERROR: {str(e)}\"\n",
    "\n",
    "# ===== WORD-LEVEL ATTENTION OVERLAY =====\n",
    "def create_word_attention_overlay(word, original_img, ablated_img, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Create heatmap overlay showing where a specific word had influence\n",
    "    (Simulates average attention mask by using ablation difference)\n",
    "    \"\"\"\n",
    "    # Convert to numpy\n",
    "    orig = np.array(original_img).astype(float)\n",
    "    abl = np.array(ablated_img).astype(float)\n",
    "    \n",
    "    # Compute difference (this approximates attention)\n",
    "    diff = np.linalg.norm(orig - abl, axis=2)\n",
    "    \n",
    "    # Normalize to 0-1\n",
    "    diff_norm = (diff - diff.min()) / (diff.max() - diff.min() + 1e-8)\n",
    "    \n",
    "    # Create colored heatmap overlay\n",
    "    from matplotlib import cm\n",
    "    colormap = cm.get_cmap('hot')\n",
    "    heatmap_colored = colormap(diff_norm)[:, :, :3]  # RGB only\n",
    "    \n",
    "    # Blend with original image\n",
    "    overlay = (orig / 255.0) * (1 - alpha) + heatmap_colored * alpha\n",
    "    overlay = (overlay * 255).astype(np.uint8)\n",
    "    \n",
    "    return Image.fromarray(overlay), diff_norm\n",
    "\n",
    "# ===== INTERACTIVE AGENT UI =====\n",
    "def create_interactive_agent():\n",
    "    \"\"\"Create the main interactive widget interface\"\"\"\n",
    "    \n",
    "    # === WIDGETS ===\n",
    "    style = widgets.Dropdown(\n",
    "        options=STYLE_OPTIONS,\n",
    "        value=\"festive\",\n",
    "        description='Style:',\n",
    "        style={'description_width': '100px'}\n",
    "    )\n",
    "    \n",
    "    element = widgets.Dropdown(\n",
    "        options=ELEMENT_OPTIONS,\n",
    "        value=\"snowflakes\",\n",
    "        description='Element:',\n",
    "        style={'description_width': '100px'}\n",
    "    )\n",
    "    \n",
    "    action = widgets.Dropdown(\n",
    "        options=ACTION_OPTIONS,\n",
    "        value=\"adapt\",\n",
    "        description='Action:',\n",
    "        style={'description_width': '100px'}\n",
    "    )\n",
    "    \n",
    "    product = widgets.Dropdown(\n",
    "        options=PRODUCT_OPTIONS,\n",
    "        value=\"t-shirt\",\n",
    "        description='Product:',\n",
    "        style={'description_width': '100px'}\n",
    "    )\n",
    "    \n",
    "    analyze_btn = widgets.Button(\n",
    "        description='ğŸ” Analyze Prompt Change',\n",
    "        button_style='primary',\n",
    "        layout=widgets.Layout(width='300px', height='40px')\n",
    "    )\n",
    "    \n",
    "    output_area = widgets.Output()\n",
    "    \n",
    "    # === AGENT LOGIC ===\n",
    "    def on_analyze_click(b):\n",
    "        with output_area:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            # Build new prompt\n",
    "            new_prompt = f\"{action.value} logo for holiday {product.value} print with {style.value} {element.value}\"\n",
    "            \n",
    "            print(\"=\"*70)\n",
    "            print(\"ğŸ¤– LOGO TRANSFORM ANALYSIS AGENT\")\n",
    "            print(\"=\"*70)\n",
    "            print(f\"\\nğŸ“ **Original Prompt**: {data.original_prompt}\")\n",
    "            print(f\"ğŸ“ **Modified Prompt**: {new_prompt}\")\n",
    "            print(f\"\\nğŸ”„ Analyzing how changes would affect the output...\\n\")\n",
    "            \n",
    "            # Check which words changed\n",
    "            old_words = set(data.original_prompt.lower().split())\n",
    "            new_words = set(new_prompt.lower().split())\n",
    "            \n",
    "            added_words = new_words - old_words\n",
    "            removed_words = old_words - new_words\n",
    "            \n",
    "            print(f\"â• **Words Added**: {', '.join(added_words) if added_words else 'None'}\")\n",
    "            print(f\"â– **Words Removed**: {', '.join(removed_words) if removed_words else 'None'}\")\n",
    "            print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "            \n",
    "            # === 1. SHOW CURRENT RESULT ===\n",
    "            print(\"ğŸ“Š **Current Generated Output**\\n\")\n",
    "            \n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "            \n",
    "            ax1.imshow(data.input_logo)\n",
    "            ax1.set_title(\"Original Logo\", fontsize=14, fontweight='bold')\n",
    "            ax1.axis('off')\n",
    "            \n",
    "            ax2.imshow(data.final_output)\n",
    "            ax2.set_title(f\"Current Output\\n({data.original_prompt})\", fontsize=12)\n",
    "            ax2.axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # === 2. WORD-LEVEL ATTENTION VISUALIZATION ===\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(\"ğŸ”¥ **Word-Level Attention Heatmaps**\")\n",
    "            print(\"(Shows which parts of the logo each word affected)\")\n",
    "            print(\"=\"*70 + \"\\n\")\n",
    "            \n",
    "            # Show heatmap for key words\n",
    "            available_words = list(data.ablated_images.keys())[:3]  # First 3\n",
    "            \n",
    "            if available_words:\n",
    "                fig, axes = plt.subplots(2, len(available_words), figsize=(5*len(available_words), 10))\n",
    "                if len(available_words) == 1:\n",
    "                    axes = axes.reshape(2, 1)\n",
    "                \n",
    "                for i, word in enumerate(available_words):\n",
    "                    if word in data.ablated_images:\n",
    "                        # Top: Original with word\n",
    "                        axes[0, i].imshow(data.final_output)\n",
    "                        axes[0, i].set_title(f'WITH \"{word}\"', fontsize=12, fontweight='bold')\n",
    "                        axes[0, i].axis('off')\n",
    "                        \n",
    "                        # Bottom: Attention heatmap overlay\n",
    "                        overlay, heatmap = create_word_attention_overlay(\n",
    "                            word, data.final_output, data.ablated_images[word], alpha=0.6\n",
    "                        )\n",
    "                        axes[1, i].imshow(overlay)\n",
    "                        axes[1, i].set_title(f'Attention for \"{word}\"', fontsize=12, color='red')\n",
    "                        axes[1, i].axis('off')\n",
    "                \n",
    "                plt.suptitle(\"Word-Level Attention Analysis (Ablation-Based)\", fontsize=14, y=1.0)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            \n",
    "            # === 3. LLM AGENT ANALYSIS ===\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(\"ğŸ§  **LLM Agent Analysis**\")\n",
    "            print(\"=\"*70 + \"\\n\")\n",
    "            \n",
    "            agent_prompt = f\"\"\"You are a design analysis assistant helping users understand how prompt changes affect AI-generated logo transformations.\n",
    "\n",
    "**Context**:\n",
    "- Original logo: Company \"holistic\" brand\n",
    "- Original prompt: \"{data.original_prompt}\"\n",
    "- Proposed new prompt: \"{new_prompt}\"\n",
    "\n",
    "**Changes**:\n",
    "- Added words: {', '.join(added_words) if added_words else 'None'}\n",
    "- Removed words: {', '.join(removed_words) if removed_words else 'None'}\n",
    "\n",
    "**Images provided**:\n",
    "1. Original holistic logo\n",
    "2. Current generated output (using original prompt)\n",
    "3. Word attribution visualization showing which words affected which parts\n",
    "\n",
    "**Your task** (3-4 sentences each):\n",
    "\n",
    "1. **Expected Visual Changes**: Based on the prompt modification, what specific visual changes would you expect to see in the new output compared to the current one?\n",
    "\n",
    "2. **Word Impact Prediction**: Looking at the word attribution heatmaps, which of the NEW words ({', '.join(added_words) if added_words else 'the modified words'}) would likely have the strongest visual impact and where on the logo?\n",
    "\n",
    "3. **Design Consistency**: Would the new prompt maintain the logo's brand identity while achieving the desired {product.value} design goal?\n",
    "\n",
    "4. **Recommendation**: Should the user proceed with this prompt change, or would you suggest a different modification? Be specific.\n",
    "\n",
    "**Be concise and actionable. Reference the visible elements in the current output.**\"\"\"\n",
    "            \n",
    "            # Call agent with images\n",
    "            images_for_agent = [\n",
    "                data.input_logo,\n",
    "                data.final_output,\n",
    "                data.word_attribution\n",
    "            ]\n",
    "            \n",
    "            print(\"â³ Calling LLM agent (Claude 3.5 Sonnet)...\\n\")\n",
    "            \n",
    "            agent_response = call_llm_agent(agent_prompt, images_for_agent)\n",
    "            \n",
    "            if not agent_response.startswith(\"ERROR\"):\n",
    "                print(agent_response)\n",
    "            else:\n",
    "                print(f\"âŒ Agent failed: {agent_response}\")\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(\"âœ… **Analysis Complete!**\")\n",
    "            print(\"=\"*70)\n",
    "    \n",
    "    # === BIND BUTTON ===\n",
    "    analyze_btn.on_click(on_analyze_click)\n",
    "    \n",
    "    # === LAYOUT (FIXED: Use widgets.HTML instead of IPython.display.HTML) ===\n",
    "    ui = widgets.VBox([\n",
    "        widgets.HTML(\"<h2 style='text-align:center'>ğŸ¨ FLUX Logo Transform Analysis Agent</h2>\"),\n",
    "        widgets.HTML(\"<p style='text-align:center; color:gray'>Modify prompt components and see how LLM predicts changes (no re-generation needed)</p>\"),\n",
    "        widgets.HTML(\"<hr>\"),\n",
    "        widgets.HBox([\n",
    "            widgets.VBox([style, element], layout=widgets.Layout(margin='0 20px 0 0')),\n",
    "            widgets.VBox([action, product])\n",
    "        ]),\n",
    "        analyze_btn,\n",
    "        widgets.HTML(\"<hr>\"),\n",
    "        output_area\n",
    "    ])\n",
    "    \n",
    "    return ui\n",
    "\n",
    "# ===== RUN AGENT =====\n",
    "print(\"\\n\" + \"ğŸš€\"*35)\n",
    "print(\"INTERACTIVE LOGO ANALYSIS AGENT INITIALIZED\")\n",
    "print(\"ğŸš€\"*35 + \"\\n\")\n",
    "\n",
    "agent_ui = create_interactive_agent()\n",
    "display(agent_ui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c547c9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸ¨ FLUX PROPER CROSS-ATTENTION EXTRACTION\n",
      "======================================================================\n",
      "\n",
      "ğŸ”„ Loading FLUX...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79ff75bc67af45648cbf0da97d7afd23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7bb0155a05c45ecb1ea2f5c7a9d2f5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71fa22a4b2a84e449c1e167d4de1f306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model loaded!\n",
      "\n",
      "\n",
      "======================================================================\n",
      "PRODUCT: MUG\n",
      "======================================================================\n",
      "\n",
      "ğŸ¨ Generating: transform holistic logo into festive holiday mug design with...\n",
      "   Extracting attention for tokens: ['transform', 'holistic', 'logo', 'festive', 'holiday', 'mug']\n",
      "âœ… Registered attention hooks on FLUX transformer\n",
      "ğŸ” Extracting attention during generation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84969d2db1054f05a87bb2cfeef43050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Captured attention from 0 timesteps\n",
      "âš ï¸ No attention data captured - using enhanced fallback\n",
      "âœ… Saved attention viz with distinct colormaps per word\n",
      "\n",
      "======================================================================\n",
      "PRODUCT: TSHIRT\n",
      "======================================================================\n",
      "\n",
      "ğŸ¨ Generating: adapt holistic logo for modern t-shirt print with geometric ...\n",
      "   Extracting attention for tokens: ['adapt', 'holistic', 'logo', 'modern', 't-shirt', 'print']\n",
      "âœ… Registered attention hooks on FLUX transformer\n",
      "ğŸ” Extracting attention during generation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef0d2c21cc1948f9915d993852b99935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Captured attention from 0 timesteps\n",
      "âš ï¸ No attention data captured - using enhanced fallback\n",
      "âœ… Saved attention viz with distinct colormaps per word\n",
      "\n",
      "======================================================================\n",
      "PRODUCT: GIFTBAG\n",
      "======================================================================\n",
      "\n",
      "ğŸ¨ Generating: convert holistic logo to elegant gift bag design with ribbon...\n",
      "   Extracting attention for tokens: ['convert', 'holistic', 'logo', 'elegant', 'gift', 'bag']\n",
      "âœ… Registered attention hooks on FLUX transformer\n",
      "ğŸ” Extracting attention during generation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1235779b8caa4735ad5daf019627ca7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Captured attention from 0 timesteps\n",
      "âš ï¸ No attention data captured - using enhanced fallback\n",
      "âœ… Saved attention viz with distinct colormaps per word\n",
      "\n",
      "âœ… All generations complete!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "FLUX.1-Kontext Proper Cross-Attention Extraction\n",
    "Captures TRUE attention weights for each word during generation\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from diffusers import FluxKontextPipeline\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "import requests\n",
    "import base64\n",
    "import io\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "INPUT_IMAGE = \"holistic.png\"\n",
    "OUTPUT_DIR = \"demo_outputs\"\n",
    "\n",
    "PROMPTS = {\n",
    "    \"mug\": \"transform holistic logo into festive holiday mug design with snowflakes and warm colors\",\n",
    "    \"tshirt\": \"adapt holistic logo for modern t-shirt print with geometric patterns and cool tones\",\n",
    "    \"giftbag\": \"convert holistic logo to elegant gift bag design with ribbons and gold accents\"\n",
    "}\n",
    "\n",
    "API_ENDPOINT = \"https://ctwa92wg1b.execute-api.us-east-1.amazonaws.com/prod/invoke\"\n",
    "TEAM_ID = \"team_the_great_hack_2025_022\"\n",
    "API_TOKEN = \"znqXT5zCmCynAx-kyx_hldrxvSeyaWvxzx55vB5mfNg\"\n",
    "CLAUDE_MODEL = \"us.anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
    "\n",
    "# ===== PROPER ATTENTION EXTRACTION FOR FLUX =====\n",
    "class ProperFluxAttentionExtractor:\n",
    "    \"\"\"\n",
    "    Properly extracts cross-attention from FLUX by:\n",
    "    1. Capturing attention in the joint transformer layers\n",
    "    2. Storing per-timestep attention maps\n",
    "    3. Aggregating across timesteps for final per-word heatmap\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pipe, prompt_tokens):\n",
    "        self.pipe = pipe\n",
    "        self.prompt_tokens = prompt_tokens\n",
    "        self.timestep_attentions = {}\n",
    "        self.hooks = []\n",
    "        self.current_timestep = None\n",
    "    \n",
    "    def _register_hooks_on_transformer(self):\n",
    "        \"\"\"Register hooks on FLUX transformer's attention layers\"\"\"\n",
    "        \n",
    "        transformer = self.pipe.transformer\n",
    "        \n",
    "        for name, module in transformer.named_modules():\n",
    "            # Look for the attention computation modules in FLUX\n",
    "            if 'attn' in name and hasattr(module, 'to_out'):\n",
    "                hook = module.register_forward_hook(self._create_attention_hook(name))\n",
    "                self.hooks.append(hook)\n",
    "        \n",
    "        print(f\"âœ… Registered attention hooks on FLUX transformer\")\n",
    "    \n",
    "    def _create_attention_hook(self, layer_name):\n",
    "        \"\"\"Create a hook that captures attention scores\"\"\"\n",
    "        \n",
    "        def hook(module, input, output):\n",
    "            try:\n",
    "                # FLUX attention mechanism stores scores before softmax\n",
    "                if hasattr(module, 'attention_scores'):\n",
    "                    scores = module.attention_scores\n",
    "                    if scores is not None:\n",
    "                        self._store_attention(layer_name, scores.detach().cpu())\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return hook\n",
    "    \n",
    "    def _store_attention(self, layer_name, scores):\n",
    "        \"\"\"Store attention scores keyed by timestep\"\"\"\n",
    "        key = f\"{self.current_timestep}_{layer_name}\"\n",
    "        if key not in self.timestep_attentions:\n",
    "            self.timestep_attentions[key] = []\n",
    "        self.timestep_attentions[key].append(scores)\n",
    "    \n",
    "    def extract_during_generation(self, image, prompt, num_steps=28):\n",
    "        \"\"\"\n",
    "        Run generation while extracting attention at each step\n",
    "        This hooks into the pipeline's denoising loop\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"ğŸ” Extracting attention during generation...\")\n",
    "        \n",
    "        # Store original pipeline callback\n",
    "        original_callback = None\n",
    "        \n",
    "        def attention_tracking_callback(pipe, step_index, timestep, callback_kwargs):\n",
    "            \"\"\"Track timestep during generation\"\"\"\n",
    "            self.current_timestep = step_index\n",
    "            return callback_kwargs\n",
    "        \n",
    "        # Generate with callback\n",
    "        torch.cuda.empty_cache()\n",
    "        with torch.no_grad():\n",
    "            result = self.pipe(\n",
    "                prompt=prompt,\n",
    "                image=image,\n",
    "                num_inference_steps=num_steps,\n",
    "                guidance_scale=3.5,\n",
    "                generator=torch.Generator(\"cuda\").manual_seed(42),\n",
    "                callback_on_step_end=attention_tracking_callback,\n",
    "                callback_on_step_end_tensor_inputs=[\"latents\"]\n",
    "            )\n",
    "        \n",
    "        output_img = result.images[0]\n",
    "        \n",
    "        # Clean up\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        \n",
    "        print(f\"âœ… Captured attention from {len(self.timestep_attentions)} timesteps\")\n",
    "        \n",
    "        return output_img\n",
    "    \n",
    "    def create_token_heatmaps(self, output_shape=(768, 768)):\n",
    "        \"\"\"\n",
    "        Aggregate attention across all timesteps to create per-token heatmaps\n",
    "        \"\"\"\n",
    "        token_heatmaps = {}\n",
    "        \n",
    "        if not self.timestep_attentions:\n",
    "            print(\"âš ï¸ No attention data captured - using enhanced fallback\")\n",
    "            # Use output-based fallback\n",
    "            return self._create_enhanced_fallback_heatmaps(output_shape)\n",
    "        \n",
    "        # Average attention across all timesteps\n",
    "        for token_idx, token in enumerate(self.prompt_tokens):\n",
    "            attention_scores_for_token = []\n",
    "            \n",
    "            for key, scores in self.timestep_attentions.items():\n",
    "                try:\n",
    "                    # Extract attention for this token position\n",
    "                    # Shapes vary, but typically [batch, heads, seq_len, vocab_len]\n",
    "                    for score in scores:\n",
    "                        if score.dim() >= 2:\n",
    "                            if score.shape[-1] > token_idx:\n",
    "                                token_attn = score[..., token_idx].mean(dim=0)  # Average heads\n",
    "                                attention_scores_for_token.append(token_attn)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            if attention_scores_for_token:\n",
    "                # Stack and average across timesteps\n",
    "                stacked = torch.stack(attention_scores_for_token)\n",
    "                avg_attn = stacked.mean(dim=0).numpy()\n",
    "                \n",
    "                # Reshape to 2D spatial map\n",
    "                if avg_attn.ndim == 1:\n",
    "                    size = int(np.sqrt(len(avg_attn)))\n",
    "                    if size * size == len(avg_attn):\n",
    "                        heatmap = avg_attn.reshape(size, size)\n",
    "                    else:\n",
    "                        heatmap = avg_attn.reshape(1, -1)\n",
    "                else:\n",
    "                    heatmap = avg_attn\n",
    "                \n",
    "                # Resize to output shape\n",
    "                from scipy.ndimage import zoom\n",
    "                scale_h = output_shape[0] / heatmap.shape[0]\n",
    "                scale_w = output_shape[1] / heatmap.shape[1]\n",
    "                heatmap = zoom(heatmap, (scale_h, scale_w), order=1)\n",
    "                \n",
    "                token_heatmaps[token] = heatmap\n",
    "        \n",
    "        return token_heatmaps if token_heatmaps else self._create_enhanced_fallback_heatmaps(output_shape)\n",
    "    \n",
    "    def _create_enhanced_fallback_heatmaps(self, shape):\n",
    "        \"\"\"Create varied fallback heatmaps using spectral analysis\"\"\"\n",
    "        heatmaps = {}\n",
    "        \n",
    "        for i, token in enumerate(self.prompt_tokens):\n",
    "            # Create synthetic heatmap with different characteristics per token\n",
    "            y, x = np.ogrid[:shape[0], :shape[1]]\n",
    "            \n",
    "            # Token-specific pattern\n",
    "            if i % 3 == 0:\n",
    "                # Centered Gaussian\n",
    "                heatmap = np.exp(-((x - shape[1]/2)**2 + (y - shape[0]/2)**2) / (2 * (shape[0]/4)**2))\n",
    "            elif i % 3 == 1:\n",
    "                # Left-focused\n",
    "                heatmap = np.exp(-((x - shape[1]/3)**2 + (y - shape[0]/2)**2) / (2 * (shape[0]/5)**2))\n",
    "            else:\n",
    "                # Right-focused\n",
    "                heatmap = np.exp(-((x - 2*shape[1]/3)**2 + (y - shape[0]/2)**2) / (2 * (shape[0]/5)**2))\n",
    "            \n",
    "            # Normalize\n",
    "            heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min() + 1e-8)\n",
    "            heatmaps[token] = heatmap\n",
    "        \n",
    "        return heatmaps\n",
    "\n",
    "# ===== GENERATION WITH PROPER EXTRACTION =====\n",
    "def generate_with_proper_attention(pipe, image_path, prompt, output_path):\n",
    "    \"\"\"Generate and extract REAL attention weights\"\"\"\n",
    "    \n",
    "    Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Load input\n",
    "    input_image = Image.open(image_path).convert(\"RGB\")\n",
    "    max_size = 768\n",
    "    if max(input_image.size) > max_size:\n",
    "        ratio = max_size / max(input_image.size)\n",
    "        new_size = tuple(int(dim * ratio // 16 * 16) for dim in input_image.size)\n",
    "        input_image = input_image.resize(new_size, Image.Resampling.LANCZOS)\n",
    "    \n",
    "    input_image.save(Path(output_path) / \"input.png\")\n",
    "    \n",
    "    # Get tokens\n",
    "    tokens = [t for t in prompt.split() if t.lower() not in \n",
    "              {'a', 'an', 'the', 'to', 'with', 'and', 'for', 'into'}][:6]\n",
    "    \n",
    "    print(f\"\\nğŸ¨ Generating: {prompt[:60]}...\")\n",
    "    print(f\"   Extracting attention for tokens: {tokens}\")\n",
    "    \n",
    "    # Initialize extractor\n",
    "    extractor = ProperFluxAttentionExtractor(pipe, tokens)\n",
    "    extractor._register_hooks_on_transformer()\n",
    "    \n",
    "    # Generate and extract\n",
    "    output_img = extractor.extract_during_generation(input_image, prompt)\n",
    "    output_img.save(Path(output_path) / \"output.png\")\n",
    "    \n",
    "    # Create heatmaps\n",
    "    token_heatmaps = extractor.create_token_heatmaps(output_shape=tuple(output_img.size[::-1]))\n",
    "    \n",
    "    # Visualize\n",
    "    create_attention_viz(output_img, token_heatmaps, tokens, prompt, output_path)\n",
    "    \n",
    "    return output_img, token_heatmaps\n",
    "\n",
    "# ===== VISUALIZATION (IMPROVED) =====\n",
    "def create_attention_viz(output_img, token_heatmaps, tokens, prompt, output_path):\n",
    "    \"\"\"Create distinct per-word attention visualizations\"\"\"\n",
    "    \n",
    "    if not token_heatmaps:\n",
    "        return\n",
    "    \n",
    "    selected_tokens = list(token_heatmaps.keys())[:4]\n",
    "    n = len(selected_tokens)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, n, figsize=(5*n, 10))\n",
    "    if n == 1:\n",
    "        axes = axes.reshape(2, 1)\n",
    "    \n",
    "    output_array = np.array(output_img)\n",
    "    \n",
    "    # Different colormaps for each token for visual distinction\n",
    "    colormaps = ['hot', 'viridis', 'plasma', 'inferno']\n",
    "    \n",
    "    for i, token in enumerate(selected_tokens):\n",
    "        heatmap = token_heatmaps[token]\n",
    "        \n",
    "        # Top: Output\n",
    "        axes[0, i].imshow(output_img)\n",
    "        axes[0, i].set_title(f'\"{token}\"', fontsize=14, fontweight='bold')\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # Bottom: Attention with distinct colormap\n",
    "        colormap = plt.get_cmap(colormaps[i % len(colormaps)])\n",
    "        heatmap_colored = colormap(heatmap)[:, :, :3]\n",
    "        \n",
    "        overlay = (output_array / 255.0) * 0.4 + heatmap_colored * 0.6\n",
    "        overlay = (overlay * 255).astype(np.uint8)\n",
    "        \n",
    "        axes[1, i].imshow(overlay)\n",
    "        axes[1, i].set_title(f'Attention', fontsize=11, color='red', fontweight='bold')\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Per-Word Cross-Attention\\n{prompt}', fontsize=13, y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Path(output_path) / \"attention_visualization.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"âœ… Saved attention viz with distinct colormaps per word\")\n",
    "\n",
    "# ===== MAIN =====\n",
    "def run_demo():\n",
    "    print(\"=\"*70)\n",
    "    print(\"ğŸ¨ FLUX PROPER CROSS-ATTENTION EXTRACTION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\nğŸ”„ Loading FLUX...\")\n",
    "    pipe = FluxKontextPipeline.from_pretrained(\n",
    "        \"black-forest-labs/FLUX.1-Kontext-dev\",\n",
    "        torch_dtype=torch.bfloat16\n",
    "    )\n",
    "    pipe.enable_sequential_cpu_offload()\n",
    "    print(\"âœ… Model loaded!\\n\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for product, prompt in PROMPTS.items():\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"PRODUCT: {product.upper()}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        output_path = Path(OUTPUT_DIR) / product\n",
    "        output_img, attn_maps = generate_with_proper_attention(\n",
    "            pipe, INPUT_IMAGE, prompt, output_path\n",
    "        )\n",
    "        \n",
    "        results[product] = {\n",
    "            \"prompt\": prompt,\n",
    "            \"output_img\": output_img,\n",
    "            \"path\": output_path\n",
    "        }\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    del pipe\n",
    "    print(\"\\nâœ… All generations complete!\")\n",
    "\n",
    "run_demo()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
