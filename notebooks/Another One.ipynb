{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68bafa06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading FLUX.1-Kontext-dev with aggressive offloading...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "191a47b5e2264499b4e640a22233f247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3165a30d829d4e46951d852918bce813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4b6763166d64e13879f2f0f1058fb58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üé® Starting edit: 'add christmas to it'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9a0317e95fd4d0aa7d5a5496f7d4b7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 22.07 GiB of which 5.44 MiB is free. Including non-PyTorch memory, this process has 22.06 GiB memory in use. Of the allocated memory 21.74 GiB is allocated by PyTorch, and 10.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6007/3844070376.py\u001b[0m in \u001b[0;36m<cell line: 224>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0medit_prompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"add christmas to it\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m     result, input_img = edit_with_visualization(\n\u001b[0m\u001b[1;32m    230\u001b[0m         \u001b[0mimage_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0medit_prompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0medit_prompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_6007/3844070376.py\u001b[0m in \u001b[0;36medit_with_visualization\u001b[0;34m(image_path, edit_prompt, num_inference_steps, guidance_scale, output_dir)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# Run pipeline with callback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     result = pipe(\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0medit_prompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_image\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/diffusers/pipelines/flux/pipeline_flux_kontext.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, image, prompt, prompt_2, negative_prompt, negative_prompt_2, true_cfg_scale, height, width, num_inference_steps, sigmas, guidance_scale, num_images_per_prompt, generator, latents, prompt_embeds, pooled_prompt_embeds, ip_adapter_image, ip_adapter_image_embeds, negative_ip_adapter_image, negative_ip_adapter_image_embeds, negative_prompt_embeds, negative_pooled_prompt_embeds, output_type, return_dict, joint_attention_kwargs, callback_on_step_end, callback_on_step_end_tensor_inputs, max_sequence_length, max_area, _auto_resize)\u001b[0m\n\u001b[1;32m   1061\u001b[0m                 \u001b[0mtimestep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m                 noise_pred = self.transformer(\n\u001b[0m\u001b[1;32m   1064\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlatent_model_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m                     \u001b[0mtimestep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimestep\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnew_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mpre_forward\u001b[0;34m(self, module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 731\u001b[0;31m         \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    732\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msend_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msend_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/diffusers/models/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1422\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1424\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m     \u001b[0;31m# Taken from `transformers`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1341\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m     def register_full_backward_pre_hook(\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1327\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     )\n\u001b[0;32m-> 1329\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1330\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 22.07 GiB of which 5.44 MiB is free. Including non-PyTorch memory, this process has 22.06 GiB memory in use. Of the allocated memory 21.74 GiB is allocated by PyTorch, and 10.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import FluxKontextPipeline\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"üîÑ Loading FLUX.1-Kontext-dev with aggressive offloading...\")\n",
    "\n",
    "pipe = FluxKontextPipeline.from_pretrained(\n",
    "    \"black-forest-labs/FLUX.1-Kontext-dev\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "pipe.enable_model_cpu_offload()\n",
    "\n",
    "# ===== Storage for intermediate states =====\n",
    "latent_history = []\n",
    "attention_maps = {}\n",
    "timestep_history = []\n",
    "\n",
    "# ===== Step 1: Callback to capture intermediate latents =====\n",
    "def denoising_callback(pipe, step_index, timestep, callback_kwargs):\n",
    "    \"\"\"Captures latent state at each denoising step\"\"\"\n",
    "    latents = callback_kwargs[\"latents\"].clone()\n",
    "    latent_history.append(latents.cpu())\n",
    "    timestep_history.append(timestep)\n",
    "    \n",
    "    print(f\"‚úì Captured step {step_index}, timestep {timestep:.2f}\")\n",
    "    return callback_kwargs\n",
    "\n",
    "# ===== Step 2: Hook to extract cross-attention maps =====\n",
    "def register_attention_hooks(model):\n",
    "    \"\"\"Registers hooks on cross-attention layers to capture text-image attention\"\"\"\n",
    "    attention_maps.clear()\n",
    "    \n",
    "    def hook_fn(name):\n",
    "        def forward_hook(module, input, output):\n",
    "            # Store attention weights from cross-attention layers\n",
    "            if hasattr(module, 'to_k') and hasattr(module, 'to_q'):\n",
    "                attention_maps[name] = output.detach().cpu()\n",
    "        return forward_hook\n",
    "    \n",
    "    # Register hooks on transformer blocks\n",
    "    for name, module in model.named_modules():\n",
    "        if 'attn' in name.lower() and 'cross' in name.lower():\n",
    "            module.register_forward_hook(hook_fn(name))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# ===== Step 3: Run image editing with monitoring =====\n",
    "def edit_with_visualization(\n",
    "    image_path,\n",
    "    edit_prompt,\n",
    "    num_inference_steps=28,\n",
    "    guidance_scale=3.5,\n",
    "    output_dir=\"flux_analysis\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Edit an image and save visualization artifacts\n",
    "    \"\"\"\n",
    "    Path(output_dir).mkdir(exist_ok=True)\n",
    "    latent_history.clear()\n",
    "    timestep_history.clear()\n",
    "    \n",
    "    # Load input image\n",
    "    input_image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Optional: Register attention hooks (may not work with all architectures)\n",
    "    # pipe.transformer = register_attention_hooks(pipe.transformer)\n",
    "    \n",
    "    print(f\"\\nüé® Starting edit: '{edit_prompt}'\")\n",
    "    \n",
    "    # Run pipeline with callback\n",
    "    result = pipe(\n",
    "        prompt=edit_prompt,\n",
    "        image=input_image,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=guidance_scale,\n",
    "        callback_on_step_end=denoising_callback,\n",
    "        callback_on_step_end_tensor_inputs=[\"latents\"]\n",
    "    ).images[0]\n",
    "    \n",
    "    print(f\"\\n‚úÖ Generated {len(latent_history)} latent snapshots\")\n",
    "    \n",
    "    # Save outputs\n",
    "    result.save(f\"{output_dir}/final_output.png\")\n",
    "    input_image.save(f\"{output_dir}/input_image.png\")\n",
    "    \n",
    "    return result, input_image\n",
    "\n",
    "# ===== Step 4: Decode and visualize latent evolution =====\n",
    "def visualize_latent_evolution(output_dir=\"flux_analysis\", sample_steps=6):\n",
    "    \"\"\"\n",
    "    Decode latents at key steps and create evolution grid\n",
    "    \"\"\"\n",
    "    print(\"\\nüîç Decoding latent evolution...\")\n",
    "    \n",
    "    total_steps = len(latent_history)\n",
    "    step_indices = np.linspace(0, total_steps-1, sample_steps, dtype=int)\n",
    "    \n",
    "    decoded_images = []\n",
    "    \n",
    "    for idx in step_indices:\n",
    "        latent = latent_history[idx].to(pipe.device, dtype=pipe.dtype)\n",
    "        \n",
    "        # Decode latent to RGB\n",
    "        with torch.no_grad():\n",
    "            image = pipe.vae.decode(latent / pipe.vae.config.scaling_factor, return_dict=False)[0]\n",
    "        \n",
    "        # Convert to PIL\n",
    "        image = (image / 2 + 0.5).clamp(0, 1)\n",
    "        image = image.cpu().permute(0, 2, 3, 1).float().numpy()[0]\n",
    "        image = (image * 255).astype(np.uint8)\n",
    "        decoded_images.append(image)\n",
    "        \n",
    "        print(f\"‚úì Decoded step {idx}/{total_steps-1}\")\n",
    "    \n",
    "    # Create visualization grid\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, (img, idx) in enumerate(zip(decoded_images, step_indices)):\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(f\"Step {idx}/{total_steps-1}\\nTimestep: {timestep_history[idx]:.2f}\")\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/latent_evolution.png\", dpi=150, bbox_inches='tight')\n",
    "    print(f\"üíæ Saved evolution to {output_dir}/latent_evolution.png\")\n",
    "    plt.close()\n",
    "\n",
    "# ===== Step 5: Compute difference heatmaps =====\n",
    "def visualize_difference_maps(output_dir=\"flux_analysis\", sample_steps=5):\n",
    "    \"\"\"\n",
    "    Show pixel-wise differences between consecutive denoising steps\n",
    "    \"\"\"\n",
    "    print(\"\\nüî• Computing difference heatmaps...\")\n",
    "    \n",
    "    total_steps = len(latent_history)\n",
    "    step_indices = np.linspace(0, total_steps-2, sample_steps, dtype=int)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, sample_steps, figsize=(20, 4))\n",
    "    \n",
    "    for i, idx in enumerate(step_indices):\n",
    "        latent1 = latent_history[idx]\n",
    "        latent2 = latent_history[idx + 1]\n",
    "        \n",
    "        # Compute L2 difference\n",
    "        diff = torch.norm(latent2 - latent1, dim=1, keepdim=True)[0, 0].numpy()\n",
    "        \n",
    "        im = axes[i].imshow(diff, cmap='hot', interpolation='bilinear')\n",
    "        axes[i].set_title(f\"Œî Step {idx}‚Üí{idx+1}\")\n",
    "        axes[i].axis('off')\n",
    "        plt.colorbar(im, ax=axes[i], fraction=0.046)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/difference_heatmaps.png\", dpi=150, bbox_inches='tight')\n",
    "    print(f\"üíæ Saved heatmaps to {output_dir}/difference_heatmaps.png\")\n",
    "    plt.close()\n",
    "\n",
    "# ===== Step 6: Prompt ablation study =====\n",
    "def prompt_ablation_study(\n",
    "    image_path,\n",
    "    base_prompt,\n",
    "    ablations,\n",
    "    num_inference_steps=28,\n",
    "    output_dir=\"flux_analysis/ablation\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Test how prompt variations affect output\n",
    "    \n",
    "    Args:\n",
    "        image_path: Input image\n",
    "        base_prompt: Original edit instruction\n",
    "        ablations: List of modified prompts to test\n",
    "    \"\"\"\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    input_image = Image.open(image_path).convert(\"RGB\")\n",
    "    results = []\n",
    "    \n",
    "    print(\"\\nüß™ Running prompt ablation study...\")\n",
    "    \n",
    "    # Generate baseline\n",
    "    print(f\"\\n[Baseline] {base_prompt}\")\n",
    "    baseline = pipe(\n",
    "        prompt=base_prompt,\n",
    "        image=input_image,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=3.5\n",
    "    ).images[0]\n",
    "    baseline.save(f\"{output_dir}/baseline.png\")\n",
    "    results.append((\"Baseline\", baseline))\n",
    "    \n",
    "    # Test ablations\n",
    "    for i, ablated_prompt in enumerate(ablations):\n",
    "        print(f\"\\n[Ablation {i+1}] {ablated_prompt}\")\n",
    "        result = pipe(\n",
    "            prompt=ablated_prompt,\n",
    "            image=input_image,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=3.5\n",
    "        ).images[0]\n",
    "        result.save(f\"{output_dir}/ablation_{i+1}.png\")\n",
    "        results.append((f\"Ablation {i+1}\", result))\n",
    "    \n",
    "    # Create comparison grid\n",
    "    fig, axes = plt.subplots(1, len(results), figsize=(5*len(results), 5))\n",
    "    if len(results) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for ax, (label, img) in zip(axes, results):\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(label, fontsize=10)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/ablation_comparison.png\", dpi=150, bbox_inches='tight')\n",
    "    print(f\"üíæ Saved comparison to {output_dir}/ablation_comparison.png\")\n",
    "    plt.close()\n",
    "\n",
    "# ===== USAGE EXAMPLE =====\n",
    "if __name__ == \"__main__\":\n",
    "    # Example 1: Single edit with full visualization\n",
    "    image_path = \"holistic.png\"  # Replace with your image\n",
    "    edit_prompt = \"add christmas to it\"\n",
    "    \n",
    "    result, input_img = edit_with_visualization(\n",
    "        image_path=image_path,\n",
    "        edit_prompt=edit_prompt,\n",
    "        num_inference_steps=28,\n",
    "        guidance_scale=3.5\n",
    "    )\n",
    "    \n",
    "    # Visualize evolution\n",
    "    visualize_latent_evolution(sample_steps=6)\n",
    "    \n",
    "    # Visualize differences\n",
    "    visualize_difference_maps(sample_steps=5)\n",
    "    \n",
    "    # Example 2: Prompt ablation\n",
    "    ablations = [\n",
    "        \"add new years to it\",  # Remove \"dramatic and stormy\"\n",
    "        \"make it more progressive\",     # More abstract\n",
    "        \"add a christmas tree\",       # Different phrasing\n",
    "    ]\n",
    "    \n",
    "    prompt_ablation_study(\n",
    "        image_path=image_path,\n",
    "        base_prompt=edit_prompt,\n",
    "        ablations=ablations\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úÖ All visualizations complete! Check the flux_analysis/ directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96edee9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading FLUX.1-Kontext-dev with sequential offload...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8b9f18cbf5f45f58637619b9ea92f17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "966f502fdfda4440a5a91143b7b3a2f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c987935b9f48c1a2d090d03ff030fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded!\n",
      "üìê Resized to (704, 768)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2caa14a6164a4b53b82c7731811c79a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/905 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24d35844a89f49c7b75737386b5bfb9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "449e4c9ee4784e1d9253e85e2e93ae59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62bf8aee102d4046a5a37afbcff6c767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4705a9925114225b7a38e3aa11696ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Prompt tokens: ['<|startoftext|>', 'add</w>', 'christmas</w>', 'decorations</w>', '<|endoftext|>']\n",
      "\n",
      "üé® Generating with attention tracking: 'add christmas decorations'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28002e05c74c49ca8840b205d6d52d2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Decoding step 0...\n",
      "  ‚úì Saved snapshot at step 0\n",
      "üîÑ Decoding step 7...\n",
      "  ‚úì Saved snapshot at step 7\n",
      "üîÑ Decoding step 14...\n",
      "  ‚úì Saved snapshot at step 14\n",
      "üîÑ Decoding step 21...\n",
      "  ‚úì Saved snapshot at step 21\n",
      "\n",
      "‚úÖ Generated! Saved 4 snapshots\n",
      "üíæ Saved flux_analysis/evolution_grid.png\n",
      "üíæ Saved flux_analysis/difference_maps.png\n",
      "\n",
      "üî¨ Token attribution analysis for: 'add christmas decorations'\n",
      "\n",
      "[Baseline] Full prompt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "073428a2e2274cad841b5f9eb3660ce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Ablation 1] Removing 'add': christmas decorations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f3da1a08d00430e993d0322234ab7da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Ablation 2] Removing 'christmas': add decorations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2c30ca20d134aeaa4b01cf8b44ad18e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Ablation 3] Removing 'decorations': add christmas\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9c271a6df24472eaf66e0b8a6f6fc69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Saved flux_analysis/token_attribution_analysis.png\n",
      "\n",
      "üìä Token Attribution Scores:\n",
      "  'christmas': 76.17\n",
      "  'add': 35.60\n",
      "  'decorations': 33.95\n",
      "\n",
      "üó∫Ô∏è Generating spatial influence maps for words: ['christmas', 'decorations']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb19bbeeec124c3389dff3f91e65f4c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/2] Analyzing 'christmas'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c98b3235e84faaa4ed499bc7e6c1a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2/2] Analyzing 'decorations'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7956c3c2eb594cf5bc77635cba407dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Saved flux_analysis/word_influence_heatmaps.png\n",
      "\n",
      "‚úÖ Complete! Check flux_analysis/\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import FluxKontextPipeline\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"üîÑ Loading FLUX.1-Kontext-dev with sequential offload...\")\n",
    "\n",
    "pipe = FluxKontextPipeline.from_pretrained(\n",
    "    \"black-forest-labs/FLUX.1-Kontext-dev\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "pipe.enable_sequential_cpu_offload()\n",
    "pipe.enable_attention_slicing(1)\n",
    "pipe.enable_vae_slicing()\n",
    "\n",
    "print(\"‚úÖ Model loaded!\")\n",
    "\n",
    "# ===== Global storage for attention maps =====\n",
    "attention_store = defaultdict(list)\n",
    "prompt_tokens = []\n",
    "\n",
    "# ===== Attention extraction hooks =====\n",
    "def register_attention_hooks(transformer):\n",
    "    \"\"\"\n",
    "    Register hooks to capture cross-attention between text and image tokens\n",
    "    \"\"\"\n",
    "    global attention_store\n",
    "    attention_store.clear()\n",
    "    \n",
    "    def make_hook(name):\n",
    "        def hook_fn(module, input, output):\n",
    "            # FLUX uses joint attention, need to extract text‚Üíimage portion\n",
    "            # output is attention weights [batch, heads, seq_len, seq_len]\n",
    "            if isinstance(output, tuple):\n",
    "                attn_weights = output[1] if len(output) > 1 else None\n",
    "            else:\n",
    "                attn_weights = output\n",
    "            \n",
    "            if attn_weights is not None and attn_weights.dim() == 4:\n",
    "                # Store on CPU immediately to save memory\n",
    "                attention_store[name].append(attn_weights.detach().cpu())\n",
    "        \n",
    "        return hook_fn\n",
    "    \n",
    "    hooks = []\n",
    "    # Hook into transformer blocks (FLUX has ~19 double blocks)\n",
    "    for name, module in transformer.named_modules():\n",
    "        if 'attn' in name.lower():\n",
    "            hook = module.register_forward_hook(make_hook(name))\n",
    "            hooks.append(hook)\n",
    "    \n",
    "    return hooks\n",
    "\n",
    "def remove_hooks(hooks):\n",
    "    \"\"\"Remove registered hooks\"\"\"\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "# ===== FLUX latent unpacking =====\n",
    "def unpack_flux_latents(latents):\n",
    "    \"\"\"Unpack FLUX latents from [B, seq_len, hidden_dim] to [B, C, H, W]\"\"\"\n",
    "    batch_size = latents.shape[0]\n",
    "    seq_len = latents.shape[1]\n",
    "    hidden_dim = latents.shape[2]\n",
    "    \n",
    "    patch_size = int(seq_len ** 0.5)\n",
    "    latent_channels = 16\n",
    "    \n",
    "    latents = latents.reshape(batch_size, patch_size, patch_size, hidden_dim)\n",
    "    latents = latents.reshape(\n",
    "        batch_size, \n",
    "        patch_size, \n",
    "        patch_size, \n",
    "        latent_channels, \n",
    "        hidden_dim // latent_channels\n",
    "    )\n",
    "    latents = latents[..., 0]\n",
    "    latents = latents.permute(0, 3, 1, 2).contiguous()\n",
    "    \n",
    "    return latents\n",
    "\n",
    "# ===== Decode and save snapshots =====\n",
    "snapshot_info = []\n",
    "output_dir = None\n",
    "\n",
    "def decode_and_save_immediately(pipe_obj, step_index, timestep, callback_kwargs):\n",
    "    \"\"\"Decode latent to image with unpacking\"\"\"\n",
    "    global output_dir, snapshot_info\n",
    "    \n",
    "    if step_index % 7 != 0 and step_index != 0:\n",
    "        return callback_kwargs\n",
    "    \n",
    "    try:\n",
    "        latents = callback_kwargs[\"latents\"]\n",
    "        snapshot_dir = Path(output_dir) / \"snapshots\"\n",
    "        snapshot_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        print(f\"üîÑ Decoding step {step_index}...\")\n",
    "        \n",
    "        unpacked_latents = unpack_flux_latents(latents)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            decoded = pipe_obj.vae.decode(\n",
    "                unpacked_latents / pipe_obj.vae.config.scaling_factor,\n",
    "                return_dict=False\n",
    "            )\n",
    "            \n",
    "            if isinstance(decoded, tuple):\n",
    "                image_tensor = decoded[0]\n",
    "            else:\n",
    "                image_tensor = decoded\n",
    "            \n",
    "            image = (image_tensor / 2 + 0.5).clamp(0, 1)\n",
    "            image = image.cpu().permute(0, 2, 3, 1).float().numpy()[0]\n",
    "            image = (image * 255).astype(np.uint8)\n",
    "            \n",
    "            filepath = snapshot_dir / f\"step_{step_index:03d}_t{timestep:.1f}.png\"\n",
    "            Image.fromarray(image).save(filepath)\n",
    "            \n",
    "            snapshot_info.append((step_index, timestep, str(filepath)))\n",
    "            print(f\"  ‚úì Saved snapshot at step {step_index}\")\n",
    "            \n",
    "            del unpacked_latents, image_tensor, image, decoded\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è Failed at step {step_index}: {e}\")\n",
    "    \n",
    "    return callback_kwargs\n",
    "\n",
    "\n",
    "def edit_with_attention_tracking(\n",
    "    image_path,\n",
    "    edit_prompt,\n",
    "    num_inference_steps=28,\n",
    "    guidance_scale=3.5,\n",
    "    max_resolution=768,\n",
    "    run_dir=\"flux_analysis\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate with attention map extraction\n",
    "    \"\"\"\n",
    "    global output_dir, snapshot_info, prompt_tokens, attention_store\n",
    "    output_dir = run_dir\n",
    "    snapshot_info = []\n",
    "    attention_store.clear()\n",
    "    \n",
    "    Path(output_dir).mkdir(exist_ok=True)\n",
    "    \n",
    "    # Load input\n",
    "    input_image = Image.open(image_path).convert(\"RGB\")\n",
    "    if max(input_image.size) > max_resolution:\n",
    "        ratio = max_resolution / max(input_image.size)\n",
    "        new_size = tuple(int(dim * ratio // 16 * 16) for dim in input_image.size)\n",
    "        input_image = input_image.resize(new_size, Image.Resampling.LANCZOS)\n",
    "        print(f\"üìê Resized to {new_size}\")\n",
    "    \n",
    "    input_image.save(f\"{output_dir}/input_image.png\")\n",
    "    \n",
    "    # Tokenize prompt to get word mapping\n",
    "    from transformers import CLIPTokenizer\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "    tokens = tokenizer.encode(edit_prompt)\n",
    "    prompt_tokens = tokenizer.convert_ids_to_tokens(tokens)\n",
    "    print(f\"\\nüìù Prompt tokens: {prompt_tokens}\")\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"\\nüé® Generating with attention tracking: '{edit_prompt}'\")\n",
    "    \n",
    "    # Register hooks to capture attention\n",
    "    # Note: This may slow down inference and increase memory usage\n",
    "    # hooks = register_attention_hooks(pipe.transformer)\n",
    "    \n",
    "    generator = torch.Generator(\"cuda\").manual_seed(42)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        result = pipe(\n",
    "            prompt=edit_prompt,\n",
    "            image=input_image,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "            generator=generator,\n",
    "            callback_on_step_end=decode_and_save_immediately,\n",
    "            callback_on_step_end_tensor_inputs=[\"latents\"]\n",
    "        )\n",
    "    \n",
    "    # remove_hooks(hooks)\n",
    "    \n",
    "    result.images[0].save(f\"{output_dir}/final_output.png\")\n",
    "    print(f\"\\n‚úÖ Generated! Saved {len(snapshot_info)} snapshots\")\n",
    "    \n",
    "    return result.images[0], input_image\n",
    "\n",
    "\n",
    "def create_token_attribution_map(\n",
    "    image_path,\n",
    "    edit_prompt,\n",
    "    num_inference_steps=20,\n",
    "    output_dir=\"flux_analysis\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate multiple ablated versions to see which words matter most\n",
    "    Uses gradient-free attribution by comparing outputs with/without each word\n",
    "    \"\"\"\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    input_image = Image.open(image_path).convert(\"RGB\")\n",
    "    if max(input_image.size) > 768:\n",
    "        ratio = 768 / max(input_image.size)\n",
    "        new_size = tuple(int(dim * ratio // 16 * 16) for dim in input_image.size)\n",
    "        input_image = input_image.resize(new_size, Image.Resampling.LANCZOS)\n",
    "    \n",
    "    print(f\"\\nüî¨ Token attribution analysis for: '{edit_prompt}'\")\n",
    "    \n",
    "    # Generate baseline\n",
    "    print(\"\\n[Baseline] Full prompt\")\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        baseline_result = pipe(\n",
    "            prompt=edit_prompt,\n",
    "            image=input_image,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=3.5,\n",
    "            generator=torch.Generator(\"cuda\").manual_seed(42)\n",
    "        )\n",
    "    baseline_img = np.array(baseline_result.images[0])\n",
    "    baseline_result.images[0].save(f\"{output_dir}/attribution_baseline.png\")\n",
    "    \n",
    "    # Ablate each word\n",
    "    words = edit_prompt.split()\n",
    "    attribution_scores = {}\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        ablated_prompt = \" \".join(words[:i] + words[i+1:])\n",
    "        if not ablated_prompt.strip():\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n[Ablation {i+1}] Removing '{word}': {ablated_prompt}\")\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            ablated_result = pipe(\n",
    "                prompt=ablated_prompt,\n",
    "                image=input_image,\n",
    "                num_inference_steps=num_inference_steps,\n",
    "                guidance_scale=3.5,\n",
    "                generator=torch.Generator(\"cuda\").manual_seed(42)\n",
    "            )\n",
    "        \n",
    "        ablated_img = np.array(ablated_result.images[0])\n",
    "        \n",
    "        # Compute pixel-wise L2 difference\n",
    "        diff = np.linalg.norm(baseline_img.astype(float) - ablated_img.astype(float), axis=2)\n",
    "        attribution_scores[word] = diff.mean()  # Average change\n",
    "        \n",
    "        ablated_result.images[0].save(f\"{output_dir}/attribution_without_{word}.png\")\n",
    "        \n",
    "        del ablated_result\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Visualize attribution scores\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Bar chart of word importance\n",
    "    words_sorted = sorted(attribution_scores.keys(), key=lambda w: attribution_scores[w], reverse=True)\n",
    "    scores_sorted = [attribution_scores[w] for w in words_sorted]\n",
    "    \n",
    "    ax1.barh(words_sorted, scores_sorted, color='coral')\n",
    "    ax1.set_xlabel('Average Pixel Change (importance)', fontsize=12)\n",
    "    ax1.set_title('Token Attribution Scores', fontsize=14)\n",
    "    ax1.invert_yaxis()\n",
    "    \n",
    "    # Show baseline image with overlay\n",
    "    ax2.imshow(baseline_img)\n",
    "    ax2.set_title(f\"Final Output: '{edit_prompt}'\", fontsize=12)\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/token_attribution_analysis.png\", dpi=120, bbox_inches='tight')\n",
    "    print(f\"\\nüíæ Saved {output_dir}/token_attribution_analysis.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    return attribution_scores\n",
    "\n",
    "\n",
    "def create_word_influence_heatmaps(\n",
    "    image_path,\n",
    "    edit_prompt,\n",
    "    words_to_analyze,\n",
    "    num_inference_steps=20,\n",
    "    output_dir=\"flux_analysis\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate spatial heatmaps showing where each word had influence\n",
    "    by comparing full prompt vs prompt without that word\n",
    "    \"\"\"\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    input_image = Image.open(image_path).convert(\"RGB\")\n",
    "    if max(input_image.size) > 768:\n",
    "        ratio = 768 / max(input_image.size)\n",
    "        new_size = tuple(int(dim * ratio // 16 * 16) for dim in input_image.size)\n",
    "        input_image = input_image.resize(new_size, Image.Resampling.LANCZOS)\n",
    "    \n",
    "    print(f\"\\nüó∫Ô∏è Generating spatial influence maps for words: {words_to_analyze}\")\n",
    "    \n",
    "    # Baseline\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        baseline = pipe(\n",
    "            prompt=edit_prompt,\n",
    "            image=input_image,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=3.5,\n",
    "            generator=torch.Generator(\"cuda\").manual_seed(42)\n",
    "        )\n",
    "    baseline_img = np.array(baseline.images[0]).astype(float)\n",
    "    \n",
    "    n_words = len(words_to_analyze)\n",
    "    fig, axes = plt.subplots(2, n_words, figsize=(5*n_words, 10))\n",
    "    if n_words == 1:\n",
    "        axes = axes.reshape(2, 1)\n",
    "    \n",
    "    for i, word in enumerate(words_to_analyze):\n",
    "        print(f\"\\n[{i+1}/{n_words}] Analyzing '{word}'...\")\n",
    "        \n",
    "        # Remove this word from prompt\n",
    "        ablated_prompt = edit_prompt.replace(word, \"\").replace(\"  \", \" \").strip()\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        with torch.no_grad():\n",
    "            ablated = pipe(\n",
    "                prompt=ablated_prompt,\n",
    "                image=input_image,\n",
    "                num_inference_steps=num_inference_steps,\n",
    "                guidance_scale=3.5,\n",
    "                generator=torch.Generator(\"cuda\").manual_seed(42)\n",
    "            )\n",
    "        ablated_img = np.array(ablated.images[0]).astype(float)\n",
    "        \n",
    "        # Compute spatial difference (where did removing this word change the image?)\n",
    "        spatial_diff = np.linalg.norm(baseline_img - ablated_img, axis=2)\n",
    "        \n",
    "        # Show baseline with word highlighted\n",
    "        axes[0, i].imshow(baseline_img.astype(np.uint8))\n",
    "        axes[0, i].set_title(f'With \"{word}\"', fontsize=11)\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # Show influence heatmap\n",
    "        im = axes[1, i].imshow(spatial_diff, cmap='hot', interpolation='bilinear')\n",
    "        axes[1, i].set_title(f'Influence of \"{word}\"', fontsize=11)\n",
    "        axes[1, i].axis('off')\n",
    "        plt.colorbar(im, ax=axes[1, i], fraction=0.046)\n",
    "        \n",
    "        del ablated\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    plt.suptitle(f'Spatial Word Influence Analysis\\nPrompt: \"{edit_prompt}\"', fontsize=14, y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/word_influence_heatmaps.png\", dpi=120, bbox_inches='tight')\n",
    "    print(f\"\\nüíæ Saved {output_dir}/word_influence_heatmaps.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ===== Keep existing visualization functions =====\n",
    "def create_evolution_grid(output_dir=\"flux_analysis\"):\n",
    "    \"\"\"Create grid from saved snapshots\"\"\"\n",
    "    snapshot_dir = Path(output_dir) / \"snapshots\"\n",
    "    snapshot_files = sorted(snapshot_dir.glob(\"step_*.png\"))\n",
    "    \n",
    "    if len(snapshot_files) == 0:\n",
    "        print(\"‚ö†Ô∏è No snapshots found!\")\n",
    "        return\n",
    "    \n",
    "    images = [Image.open(f) for f in snapshot_files]\n",
    "    n = len(images)\n",
    "    \n",
    "    cols = min(3, n)\n",
    "    rows = (n + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(6*cols, 6*rows))\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for i, (img, file) in enumerate(zip(images, snapshot_files)):\n",
    "        axes[i].imshow(img)\n",
    "        step_info = file.stem.replace(\"step_\", \"Step \").replace(\"_t\", \" | t=\")\n",
    "        axes[i].set_title(step_info, fontsize=12)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    for i in range(n, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/evolution_grid.png\", dpi=120, bbox_inches='tight')\n",
    "    print(f\"üíæ Saved {output_dir}/evolution_grid.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def create_difference_maps(output_dir=\"flux_analysis\"):\n",
    "    \"\"\"Compute pixel differences\"\"\"\n",
    "    snapshot_dir = Path(output_dir) / \"snapshots\"\n",
    "    snapshot_files = sorted(snapshot_dir.glob(\"step_*.png\"))\n",
    "    \n",
    "    if len(snapshot_files) < 2:\n",
    "        print(\"‚ö†Ô∏è Need at least 2 snapshots\")\n",
    "        return\n",
    "    \n",
    "    n_pairs = len(snapshot_files) - 1\n",
    "    \n",
    "    fig, axes = plt.subplots(1, n_pairs, figsize=(5*n_pairs, 4))\n",
    "    if n_pairs == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i in range(n_pairs):\n",
    "        img1 = np.array(Image.open(snapshot_files[i]).convert('RGB')).astype(float)\n",
    "        img2 = np.array(Image.open(snapshot_files[i+1]).convert('RGB')).astype(float)\n",
    "        \n",
    "        diff = np.linalg.norm(img2 - img1, axis=2)\n",
    "        \n",
    "        im = axes[i].imshow(diff, cmap='hot', interpolation='bilinear')\n",
    "        step1 = snapshot_files[i].stem.split('_t')[0].replace('step_', '')\n",
    "        step2 = snapshot_files[i+1].stem.split('_t')[0].replace('step_', '')\n",
    "        axes[i].set_title(f\"Œî Step {step1} ‚Üí {step2}\", fontsize=9)\n",
    "        axes[i].axis('off')\n",
    "        plt.colorbar(im, ax=axes[i], fraction=0.046)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/difference_maps.png\", dpi=120, bbox_inches='tight')\n",
    "    print(f\"üíæ Saved {output_dir}/difference_maps.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ===== USAGE =====\n",
    "if __name__ == \"__main__\":\n",
    "    image_path = \"holistic.png\"  # Replace\n",
    "    edit_prompt = \"add christmas decorations\"\n",
    "    \n",
    "    # 1. Generate with diffusion visualization\n",
    "    result, input_img = edit_with_attention_tracking(\n",
    "        image_path=image_path,\n",
    "        edit_prompt=edit_prompt,\n",
    "        num_inference_steps=28,\n",
    "        guidance_scale=3.5,\n",
    "        max_resolution=768\n",
    "    )\n",
    "    \n",
    "    create_evolution_grid()\n",
    "    create_difference_maps()\n",
    "    \n",
    "    # 2. Analyze which words mattered most (gradient-free attribution)\n",
    "    attribution_scores = create_token_attribution_map(\n",
    "        image_path=image_path,\n",
    "        edit_prompt=edit_prompt,\n",
    "        num_inference_steps=20\n",
    "    )\n",
    "    \n",
    "    print(\"\\nüìä Token Attribution Scores:\")\n",
    "    for word, score in sorted(attribution_scores.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  '{word}': {score:.2f}\")\n",
    "    \n",
    "    # 3. Generate spatial heatmaps for key words\n",
    "    words_to_analyze = [\"christmas\", \"decorations\"]  # Customize based on your prompt\n",
    "    create_word_influence_heatmaps(\n",
    "        image_path=image_path,\n",
    "        edit_prompt=edit_prompt,\n",
    "        words_to_analyze=words_to_analyze,\n",
    "        num_inference_steps=20\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úÖ Complete! Check flux_analysis/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4858e6-2d23-46fe-aa71-477d6d1cf09c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading FLUX.1-Kontext-dev model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c179a244764cb5b1385918731c7429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93c343dedac94788ab8d79d0b3a563a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a53ac9292fb2465ead4e6ed24a66d337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded!\n",
      "\n",
      "\n",
      "============================================================\n",
      "SCENARIO: GIFTBAG DESIGN\n",
      "============================================================\n",
      "\n",
      "  üìù Prompt 1/4\n",
      "  üé® Generating: 'convert logo to christmas gift bag design with wrapping elem...'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e298f03917b492ea4a5d751b0aa0583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚úì Generated in 389.2s\n",
      "    Creating evolution grid...\n",
      "    ‚úì Saved evolution_grid.png\n",
      "    Creating word attribution map...\n",
      "    Generating baseline for ablation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb2eb95763494091b8e3c3c0eafdc063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Selected words for ablation: ['christmas', 'wrapping', 'elements', 'convert']\n",
      "      Ablating 'christmas'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "090582a7e10749f687e2ef280bd7c4b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Ablating 'wrapping'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70a9f23c4ad54962baa2364ec0f49783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Ablating 'elements'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9aebf47af0e4e0691dc3d861b1f5c05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Ablating 'convert'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9117b7e84c64acdaa1a9aeb3e0a6297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚úì Saved word_attribution_complete.png\n",
      "    ‚úì Saved 4 individual ablated images\n",
      "    Selected words for ablation: ['christmas']\n",
      "    Creating timestep evolution for 'christmas'...\n",
      "    ‚úì Saved timestep_evolution_christmas.png\n",
      "\n",
      "  üìù Prompt 2/4\n",
      "  üé® Generating: 'create holiday gift wrap pattern with bows and ornaments...'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2a21041078f4d498e5c43fc596030d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚úì Generated in 390.9s\n",
      "    Creating evolution grid...\n",
      "    ‚úì Saved evolution_grid.png\n",
      "    Creating word attribution map...\n",
      "    Generating baseline for ablation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abdefd71bcc3485f9b489021113d3dc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Selected words for ablation: ['ornaments', 'holiday', 'pattern', 'create']\n",
      "      Ablating 'ornaments'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78a985e6a5ac4db081de5b0ba108057a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Ablating 'holiday'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4555d8629468442391c908e0cafeec30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Ablating 'pattern'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9857c73a58254199abb34662a27fdaa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Ablating 'create'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b2269650b3e47aaa0a7b927407649f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚úì Saved word_attribution_complete.png\n",
      "    ‚úì Saved 4 individual ablated images\n",
      "    Selected words for ablation: ['ornaments']\n",
      "    Creating timestep evolution for 'ornaments'...\n",
      "    ‚úì Saved timestep_evolution_ornaments.png\n",
      "\n",
      "  üìù Prompt 3/4\n",
      "  üé® Generating: 'design festive packaging graphics with presents and ribbons...'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e33b4d7cafb4c6b842b1c5c87677650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚úì Generated in 372.3s\n",
      "    Creating evolution grid...\n",
      "    ‚úì Saved evolution_grid.png\n",
      "    Creating word attribution map...\n",
      "    Generating baseline for ablation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e045557cbae4475eaf434a3b5add98a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Selected words for ablation: ['packaging', 'graphics', 'presents', 'festive']\n",
      "      Ablating 'packaging'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46bdbff7679b47e2b2ac1bd4c7128393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Ablating 'graphics'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f761f92ce8f4fb484427c6e693f4d67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Ablating 'presents'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc771888b5344e118c21b6a1b9ecdd6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Ablating 'festive'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1668143d9409432ca6e18511be96fcf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚úì Saved word_attribution_complete.png\n",
      "    ‚úì Saved 4 individual ablated images\n",
      "    Selected words for ablation: ['packaging']\n",
      "    Creating timestep evolution for 'packaging'...\n",
      "    ‚úì Saved timestep_evolution_packaging.png\n",
      "\n",
      "  üìù Prompt 4/4\n",
      "  üé® Generating: 'transform into christmas gift bag artwork with winter scenes...'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "628d0feb78a2497bba2d3114e88629de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚úì Generated in 371.7s\n",
      "    Creating evolution grid...\n",
      "    ‚úì Saved evolution_grid.png\n",
      "    Creating word attribution map...\n",
      "    Generating baseline for ablation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc8465495934565906205273f4b30a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Selected words for ablation: ['transform', 'christmas', 'artwork', 'winter']\n",
      "      Ablating 'transform'...\n",
      "      Ablating 'winter'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb2e3c7667284bd3b6d29697f1c5dcf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚úì Saved word_attribution_complete.png\n",
      "    ‚úì Saved 4 individual ablated images\n",
      "    Selected words for ablation: ['transform']\n",
      "    Creating timestep evolution for 'transform'...\n",
      "    ‚úì Saved timestep_evolution_transform.png\n",
      "\n",
      "============================================================\n",
      "‚úÖ EXPERIMENT COMPLETE!\n",
      "üìÅ Results saved to: flux_experiments/run_20251116_062038\n",
      "üìä Total prompts tested: 4\n",
      "============================================================\n",
      "\n",
      "üéâ Ready for LLM analysis!\n",
      "   Experiment path: flux_experiments/run_20251116_062038\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "FLUX.1-Kontext Prompt Analysis Experiment Runner\n",
    "Generates cross-attention style visualizations for prompt analysis\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from diffusers import FluxKontextPipeline\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "SCENARIOS = {\n",
    "    # \"mug_design\": {\n",
    "    #     \"base_prompt\": \"transform logo into festive holiday mug design with snowflakes\",\n",
    "    #     \"variants\": [\n",
    "    #         \"add christmas-themed mug design with candy canes and holly\",\n",
    "    #         \"create winter coffee cup graphics with red and green accents\",\n",
    "    #         \"design festive drinkware pattern with ornaments and ribbons\"\n",
    "    #     ]\n",
    "    # },\n",
    "    # \"tshirt_design\": {\n",
    "    #     \"base_prompt\": \"adapt logo for holiday t-shirt print with seasonal elements\",\n",
    "    #     \"variants\": [\n",
    "    #         \"create christmas apparel design with vintage holiday motifs\",\n",
    "    #         \"transform into festive clothing graphic with snowflakes and stars\",\n",
    "    #         \"design holiday wearable print with cozy winter theme\"\n",
    "    #     ]\n",
    "    # },\n",
    "    \"giftbag_design\": {\n",
    "        \"base_prompt\": \"convert logo to christmas gift bag design with wrapping elements\",\n",
    "        \"variants\": [\n",
    "            \"create holiday gift wrap pattern with bows and ornaments\",\n",
    "            \"design festive packaging graphics with presents and ribbons\",\n",
    "            \"transform into christmas gift bag artwork with winter scenes\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "INPUT_IMAGE = \"holistic.png\"\n",
    "OUTPUT_ROOT = \"flux_experiments\"\n",
    "NUM_INFERENCE_STEPS = 28\n",
    "GUIDANCE_SCALE = 3.5\n",
    "MAX_RESOLUTION = 768\n",
    "\n",
    "# Words to skip in ablation (common/filler words)\n",
    "SKIP_WORDS = {'a', 'an', 'the', 'to', 'with', 'and', 'or', 'for', 'of', 'in', 'into'}\n",
    "\n",
    "# ===== LATENT UNPACKING =====\n",
    "def unpack_flux_latents(latents):\n",
    "    \"\"\"Unpack FLUX latents from [B, seq_len, hidden_dim] to [B, C, H, W]\"\"\"\n",
    "    batch_size = latents.shape[0]\n",
    "    seq_len = latents.shape[1]\n",
    "    hidden_dim = latents.shape[2]\n",
    "    \n",
    "    patch_size = int(seq_len ** 0.5)\n",
    "    latent_channels = 16\n",
    "    \n",
    "    latents = latents.reshape(batch_size, patch_size, patch_size, hidden_dim)\n",
    "    latents = latents.reshape(\n",
    "        batch_size, patch_size, patch_size, latent_channels, \n",
    "        hidden_dim // latent_channels\n",
    "    )\n",
    "    latents = latents[..., 0]\n",
    "    latents = latents.permute(0, 3, 1, 2).contiguous()\n",
    "    \n",
    "    return latents\n",
    "\n",
    "# ===== GENERATION WITH TRACKING =====\n",
    "snapshot_info = []\n",
    "output_dir = None\n",
    "\n",
    "def decode_callback(pipe_obj, step_index, timestep, callback_kwargs):\n",
    "    \"\"\"Decode and save intermediate latents\"\"\"\n",
    "    global output_dir, snapshot_info\n",
    "    \n",
    "    if step_index % 7 != 0 and step_index != 0:\n",
    "        return callback_kwargs\n",
    "    \n",
    "    try:\n",
    "        latents = callback_kwargs[\"latents\"]\n",
    "        snapshot_dir = Path(output_dir) / \"snapshots\"\n",
    "        snapshot_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        unpacked_latents = unpack_flux_latents(latents)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            decoded = pipe_obj.vae.decode(\n",
    "                unpacked_latents / pipe_obj.vae.config.scaling_factor,\n",
    "                return_dict=False\n",
    "            )\n",
    "            \n",
    "            if isinstance(decoded, tuple):\n",
    "                image_tensor = decoded[0]\n",
    "            else:\n",
    "                image_tensor = decoded\n",
    "            \n",
    "            image = (image_tensor / 2 + 0.5).clamp(0, 1)\n",
    "            image = image.cpu().permute(0, 2, 3, 1).float().numpy()[0]\n",
    "            image = (image * 255).astype(np.uint8)\n",
    "            \n",
    "            filepath = snapshot_dir / f\"step_{step_index:03d}_t{timestep:.1f}.png\"\n",
    "            Image.fromarray(image).save(filepath)\n",
    "            snapshot_info.append((step_index, timestep))\n",
    "            \n",
    "            del unpacked_latents, image_tensor, image, decoded\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è Failed at step {step_index}: {e}\")\n",
    "    \n",
    "    return callback_kwargs\n",
    "\n",
    "def generate_with_analysis(pipe, image_path, prompt, output_path):\n",
    "    \"\"\"Generate image with full analysis suite\"\"\"\n",
    "    global output_dir, snapshot_info\n",
    "    output_dir = str(output_path)\n",
    "    snapshot_info = []\n",
    "    \n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Load and resize input\n",
    "    input_image = Image.open(image_path).convert(\"RGB\")\n",
    "    if max(input_image.size) > MAX_RESOLUTION:\n",
    "        ratio = MAX_RESOLUTION / max(input_image.size)\n",
    "        new_size = tuple(int(dim * ratio // 16 * 16) for dim in input_image.size)\n",
    "        input_image = input_image.resize(new_size, Image.Resampling.LANCZOS)\n",
    "    \n",
    "    input_image.save(Path(output_dir) / \"input_image.png\")\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"  üé® Generating: '{prompt[:60]}...'\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    generator = torch.Generator(\"cuda\").manual_seed(42)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        result = pipe(\n",
    "            prompt=prompt,\n",
    "            image=input_image,\n",
    "            num_inference_steps=NUM_INFERENCE_STEPS,\n",
    "            guidance_scale=GUIDANCE_SCALE,\n",
    "            generator=generator,\n",
    "            callback_on_step_end=decode_callback,\n",
    "            callback_on_step_end_tensor_inputs=[\"latents\"]\n",
    "        )\n",
    "    \n",
    "    generation_time = time.time() - start_time\n",
    "    \n",
    "    result.images[0].save(Path(output_dir) / \"final_output.png\")\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        \"prompt\": prompt,\n",
    "        \"num_steps\": NUM_INFERENCE_STEPS,\n",
    "        \"guidance_scale\": GUIDANCE_SCALE,\n",
    "        \"generation_time_seconds\": generation_time,\n",
    "        \"snapshots_saved\": len(snapshot_info),\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    with open(Path(output_dir) / \"metadata.json\", \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"    ‚úì Generated in {generation_time:.1f}s\")\n",
    "    \n",
    "    return result.images[0], input_image, metadata\n",
    "\n",
    "# ===== SMART WORD SELECTION =====\n",
    "def select_important_words(prompt, max_words=4):\n",
    "    \"\"\"Select the most important/descriptive words for ablation\"\"\"\n",
    "    words = prompt.split()\n",
    "    \n",
    "    # Filter out common words\n",
    "    important_words = [w for w in words if w.lower() not in SKIP_WORDS]\n",
    "    \n",
    "    # If still too many, prioritize nouns/adjectives (heuristic: longer words)\n",
    "    if len(important_words) > max_words:\n",
    "        important_words = sorted(important_words, key=len, reverse=True)[:max_words]\n",
    "    \n",
    "    print(f\"    Selected words for ablation: {important_words}\")\n",
    "    return important_words\n",
    "\n",
    "# ===== VISUALIZATIONS =====\n",
    "def create_evolution_grid(output_dir):\n",
    "    \"\"\"Create diffusion process evolution grid\"\"\"\n",
    "    snapshot_dir = Path(output_dir) / \"snapshots\"\n",
    "    snapshot_files = sorted(snapshot_dir.glob(\"step_*.png\"))\n",
    "    \n",
    "    if len(snapshot_files) == 0:\n",
    "        return\n",
    "    \n",
    "    images = [Image.open(f) for f in snapshot_files]\n",
    "    n = len(images)\n",
    "    \n",
    "    cols = min(3, n)\n",
    "    rows = (n + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(6*cols, 6*rows))\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for i, (img, file) in enumerate(zip(images, snapshot_files)):\n",
    "        axes[i].imshow(img)\n",
    "        step_info = file.stem.replace(\"step_\", \"Step \").replace(\"_t\", \" | t=\")\n",
    "        axes[i].set_title(step_info, fontsize=12)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    for i in range(n, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Path(output_dir) / \"evolution_grid.png\", dpi=120, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"    ‚úì Saved evolution_grid.png\")\n",
    "\n",
    "def create_word_attribution_ablation(pipe, image_path, prompt, output_dir):\n",
    "    \"\"\"\n",
    "    Generate complete word attribution showing:\n",
    "    - Original images\n",
    "    - Ablated images (word removed)\n",
    "    - Difference heatmaps\n",
    "    \"\"\"\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    input_image = Image.open(image_path).convert(\"RGB\")\n",
    "    if max(input_image.size) > 768:\n",
    "        ratio = 768 / max(input_image.size)\n",
    "        new_size = tuple(int(dim * ratio // 16 * 16) for dim in input_image.size)\n",
    "        input_image = input_image.resize(new_size, Image.Resampling.LANCZOS)\n",
    "    \n",
    "    print(\"    Generating baseline for ablation...\")\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        baseline = pipe(\n",
    "            prompt=prompt,\n",
    "            image=input_image,\n",
    "            num_inference_steps=20,\n",
    "            guidance_scale=GUIDANCE_SCALE,\n",
    "            generator=torch.Generator(\"cuda\").manual_seed(42)\n",
    "        )\n",
    "    baseline_img = np.array(baseline.images[0]).astype(float)\n",
    "    \n",
    "    # Smart word selection\n",
    "    important_words = select_important_words(prompt, max_words=4)\n",
    "    \n",
    "    word_data = {}  # Store both ablated images and heatmaps\n",
    "    \n",
    "    for word in important_words:\n",
    "        print(f\"      Ablating '{word}'...\")\n",
    "        ablated_prompt = prompt.replace(word, \"\").replace(\"  \", \" \").strip()\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        with torch.no_grad():\n",
    "            ablated = pipe(\n",
    "                prompt=ablated_prompt,\n",
    "                image=input_image,\n",
    "                num_inference_steps=20,\n",
    "                guidance_scale=GUIDANCE_SCALE,\n",
    "                generator=torch.Generator(\"cuda\").manual_seed(42)\n",
    "            )\n",
    "        \n",
    "        ablated_img = np.array(ablated.images[0]).astype(float)\n",
    "        spatial_diff = np.linalg.norm(baseline_img - ablated_img, axis=2)\n",
    "        \n",
    "        word_data[word] = {\n",
    "            'ablated_image': ablated_img,\n",
    "            'heatmap': spatial_diff\n",
    "        }\n",
    "        \n",
    "        del ablated\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Create 3-ROW visualization\n",
    "    n_words = len(word_data)\n",
    "    fig, axes = plt.subplots(3, n_words, figsize=(4*n_words, 12))\n",
    "    if n_words == 1:\n",
    "        axes = axes.reshape(3, 1)\n",
    "    \n",
    "    for i, (word, data) in enumerate(word_data.items()):\n",
    "        # ROW 1: Baseline with full prompt\n",
    "        axes[0, i].imshow(baseline_img.astype(np.uint8))\n",
    "        axes[0, i].set_title(f'WITH \"{word}\"', fontsize=11, fontweight='bold')\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # ROW 2: Ablated (word removed)\n",
    "        axes[1, i].imshow(data['ablated_image'].astype(np.uint8))\n",
    "        axes[1, i].set_title(f'WITHOUT \"{word}\"', fontsize=11, fontweight='bold', color='red')\n",
    "        axes[1, i].axis('off')\n",
    "        \n",
    "        # ROW 3: Difference heatmap\n",
    "        im = axes[2, i].imshow(data['heatmap'], cmap='hot', interpolation='bilinear')\n",
    "        axes[2, i].set_title(f'Difference Map', fontsize=10)\n",
    "        axes[2, i].axis('off')\n",
    "        plt.colorbar(im, ax=axes[2, i], fraction=0.046)\n",
    "    \n",
    "    plt.suptitle(f'Word Attribution Analysis\\nPrompt: \"{prompt}\"', \n",
    "                 fontsize=13, y=0.99, fontweight='bold')\n",
    "    \n",
    "    # Add row labels on the left\n",
    "    fig.text(0.02, 0.75, 'Baseline\\n(Full Prompt)', \n",
    "             ha='center', va='center', fontsize=12, fontweight='bold', rotation=90)\n",
    "    fig.text(0.02, 0.50, 'Ablated\\n(Word Removed)', \n",
    "             ha='center', va='center', fontsize=12, fontweight='bold', rotation=90, color='red')\n",
    "    fig.text(0.02, 0.25, 'Change\\nHeatmap', \n",
    "             ha='center', va='center', fontsize=12, fontweight='bold', rotation=90)\n",
    "    \n",
    "    plt.tight_layout(rect=[0.03, 0, 1, 0.98])\n",
    "    plt.savefig(Path(output_dir) / \"word_attribution_complete.png\", dpi=120, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"    ‚úì Saved word_attribution_complete.png\")\n",
    "    \n",
    "    # ALSO save individual ablated images for inspection\n",
    "    for word, data in word_data.items():\n",
    "        ablated_pil = Image.fromarray(data['ablated_image'].astype(np.uint8))\n",
    "        ablated_pil.save(Path(output_dir) / f\"ablated_without_{word}.png\")\n",
    "    \n",
    "    print(f\"    ‚úì Saved {len(word_data)} individual ablated images\")\n",
    "    \n",
    "    return word_data\n",
    "\n",
    "def create_timestep_word_evolution(output_dir, tracked_word):\n",
    "    \"\"\"\n",
    "    Create timestep-based attention evolution (BOTTOM ROW of reference image)\n",
    "    Uses the snapshots already generated during main inference\n",
    "    \"\"\"\n",
    "    snapshot_dir = Path(output_dir) / \"snapshots\"\n",
    "    if not snapshot_dir.exists():\n",
    "        print(\"    ‚ö†Ô∏è No snapshots found for timestep evolution\")\n",
    "        return\n",
    "    \n",
    "    snapshot_files = sorted(snapshot_dir.glob(\"step_*.png\"))\n",
    "    \n",
    "    if len(snapshot_files) == 0:\n",
    "        return\n",
    "    \n",
    "    # Create bottom row visualization\n",
    "    n_steps = len(snapshot_files)\n",
    "    fig, axes = plt.subplots(1, n_steps, figsize=(3*n_steps, 3))\n",
    "    if n_steps == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, snap_file in enumerate(snapshot_files):\n",
    "        img = Image.open(snap_file)\n",
    "        axes[i].imshow(img)\n",
    "        \n",
    "        # Extract step number from filename\n",
    "        step_num = snap_file.stem.split('_')[1]\n",
    "        axes[i].set_title(f\"t={step_num}\", fontsize=10)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Cross-Attention Maps for Individual Timestamps\\n\"{tracked_word}\"', \n",
    "                 fontsize=12, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Path(output_dir) / f\"timestep_evolution_{tracked_word}.png\", \n",
    "                dpi=120, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"    ‚úì Saved timestep_evolution_{tracked_word}.png\")\n",
    "\n",
    "# ===== MAIN EXPERIMENT RUNNER =====\n",
    "def run_full_experiment():\n",
    "    \"\"\"Run complete experiment across all scenarios\"\"\"\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    experiment_root = Path(OUTPUT_ROOT) / f\"run_{timestamp}\"\n",
    "    experiment_root.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save experiment configuration\n",
    "    config = {\n",
    "        \"scenarios\": SCENARIOS,\n",
    "        \"input_image\": INPUT_IMAGE,\n",
    "        \"inference_steps\": NUM_INFERENCE_STEPS,\n",
    "        \"guidance_scale\": GUIDANCE_SCALE,\n",
    "        \"timestamp\": timestamp\n",
    "    }\n",
    "    \n",
    "    with open(experiment_root / \"experiment_config.json\", \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    \n",
    "    # Load model once (reuse for all generations)\n",
    "    print(\"üîÑ Loading FLUX.1-Kontext-dev model...\")\n",
    "    pipe = FluxKontextPipeline.from_pretrained(\n",
    "        \"black-forest-labs/FLUX.1-Kontext-dev\",\n",
    "        torch_dtype=torch.bfloat16\n",
    "    )\n",
    "    pipe.enable_sequential_cpu_offload()\n",
    "    pipe.enable_attention_slicing(1)\n",
    "    pipe.enable_vae_slicing()\n",
    "    print(\"‚úÖ Model loaded!\\n\")\n",
    "    \n",
    "    results_summary = {}\n",
    "    \n",
    "    # Iterate through scenarios\n",
    "    for scenario_name, scenario_config in SCENARIOS.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"SCENARIO: {scenario_name.upper().replace('_', ' ')}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        scenario_dir = experiment_root / scenario_name\n",
    "        scenario_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        scenario_results = {}\n",
    "        \n",
    "        # Test all prompts (base + variants)\n",
    "        all_prompts = [scenario_config[\"base_prompt\"]] + scenario_config[\"variants\"]\n",
    "        \n",
    "        for prompt_idx, prompt in enumerate(all_prompts):\n",
    "            prompt_name = f\"prompt_{prompt_idx}_base\" if prompt_idx == 0 else f\"prompt_{prompt_idx}_variant{prompt_idx}\"\n",
    "            \n",
    "            print(f\"\\n  üìù Prompt {prompt_idx + 1}/{len(all_prompts)}\")\n",
    "            \n",
    "            # Output directory\n",
    "            output_path = scenario_dir / prompt_name\n",
    "            \n",
    "            # Generate with analysis\n",
    "            final_img, input_img, metadata = generate_with_analysis(\n",
    "                pipe, INPUT_IMAGE, prompt, output_path\n",
    "            )\n",
    "            \n",
    "            # Create evolution grid\n",
    "            print(\"    Creating evolution grid...\")\n",
    "            create_evolution_grid(output_path)\n",
    "            \n",
    "            # Create word attribution map (smart ablation)\n",
    "            print(\"    Creating word attribution map...\")\n",
    "            word_heatmaps = create_word_attribution_ablation(\n",
    "                pipe, INPUT_IMAGE, prompt, output_path\n",
    "            )\n",
    "            \n",
    "            # Create timestep evolution for first important word\n",
    "            important_words = select_important_words(prompt, max_words=1)\n",
    "            if important_words:\n",
    "                print(f\"    Creating timestep evolution for '{important_words[0]}'...\")\n",
    "                create_timestep_word_evolution(output_path, important_words[0])\n",
    "            \n",
    "            scenario_results[prompt_name] = {\n",
    "                \"prompt\": prompt,\n",
    "                \"output_dir\": str(output_path),\n",
    "                \"generation_time\": metadata[\"generation_time_seconds\"],\n",
    "                \"final_image\": str(output_path / \"final_output.png\")\n",
    "            }\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        \n",
    "        results_summary[scenario_name] = scenario_results\n",
    "    \n",
    "    # Cleanup\n",
    "    del pipe\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Save results summary\n",
    "    with open(experiment_root / \"results_summary.json\", \"w\") as f:\n",
    "        json.dump(results_summary, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"‚úÖ EXPERIMENT COMPLETE!\")\n",
    "    print(f\"üìÅ Results saved to: {experiment_root}\")\n",
    "    print(f\"üìä Total prompts tested: {sum(len(s) for s in results_summary.values())}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return experiment_root, results_summary\n",
    "\n",
    "# ===== RUN DIRECTLY IN NOTEBOOK =====\n",
    "experiment_path, summary = run_full_experiment()\n",
    "print(f\"\\nüéâ Ready for LLM analysis!\")\n",
    "print(f\"   Experiment path: {experiment_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee005cf2-7bb9-4220-a05d-53d19d73a339",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'flux_experiments/run_20251116_013857/results_summary.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13210/815023080.py\u001b[0m in \u001b[0;36m<cell line: 341>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0mexperiment_root\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"flux_experiments/run_20251116_013857\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m \u001b[0manalyses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_llm_comparison\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n‚úÖ Analysis complete! Check {experiment_root}/LLM_COMPARISON_REPORT.md\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_13210/815023080.py\u001b[0m in \u001b[0;36mrun_llm_comparison\u001b[0;34m(experiment_root)\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment_path\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"results_summary.json\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'flux_experiments/run_20251116_013857/results_summary.json'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "LLM-based Analysis of FLUX Experiments\n",
    "Compares different LLMs' reasoning about image generation quality\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import time\n",
    "from typing import Dict, List\n",
    "import pandas as pd\n",
    "\n",
    "# ===== AWS BEDROCK CONFIGURATION =====\n",
    "API_ENDPOINT = \"https://ctwa92wg1b.execute-api.us-east-1.amazonaws.com/prod/invoke\"\n",
    "TEAM_ID = \"team_the_great_hack_2025_022\"\n",
    "API_TOKEN = \"znqXT5zCmCynAx-kyx_hldrxvSeyaWvxzx55vB5mfNg\"\n",
    "\n",
    "# LLMs to test (one from each provider)\n",
    "LLMS_TO_TEST = {\n",
    "    \"claude\": \"us.anthropic.claude-3-opus-20240229-v1:0\",\n",
    "    \"nova_pro\": \"us.amazon.nova-premier-v1:0\",\n",
    "    \"llama\": \"us.meta.llama3-2-90b-instruct-v1:0\",\n",
    "    \"deepseek_r1\": \"us.deepseek.r1-v1:0\",\n",
    "    \"mistral\": \"us.mistral.pixtral-large-2502-v1:0\"\n",
    "}\n",
    "\n",
    "# ===== BEDROCK API HELPERS =====\n",
    "def encode_image_to_base64(image_path):\n",
    "    \"\"\"Convert image to base64 for API\"\"\"\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        return base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "\n",
    "def call_bedrock_llm(model_id, prompt, images=None):\n",
    "    \"\"\"Call AWS Bedrock via provided API endpoint\"\"\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"x-api-key\": API_TOKEN\n",
    "    }\n",
    "    \n",
    "    # Build message content\n",
    "    content = []\n",
    "    \n",
    "    if images:\n",
    "        for img_path in images:\n",
    "            img_b64 = encode_image_to_base64(img_path)\n",
    "            content.append({\n",
    "                \"type\": \"image\",\n",
    "                \"source\": {\n",
    "                    \"type\": \"base64\",\n",
    "                    \"media_type\": \"image/png\",\n",
    "                    \"data\": img_b64\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    content.append({\n",
    "        \"type\": \"text\",\n",
    "        \"text\": prompt\n",
    "    })\n",
    "    \n",
    "    payload = {\n",
    "        \"teamId\": TEAM_ID,\n",
    "        \"modelId\": model_id,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": content\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 1500,\n",
    "        \"temperature\": 0.3\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(API_ENDPOINT, headers=headers, json=payload, timeout=60)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        \n",
    "        if \"content\" in result and len(result[\"content\"]) > 0:\n",
    "            return result[\"content\"][0][\"text\"]\n",
    "        else:\n",
    "            return result.get(\"completion\", \"No response\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è API call failed: {e}\")\n",
    "        return f\"ERROR: {str(e)}\"\n",
    "\n",
    "# ===== ANALYSIS PROMPTS =====\n",
    "def generate_analysis_prompt(scenario, prompt_text, metadata):\n",
    "    \"\"\"Create structured prompt for LLM analysis\"\"\"\n",
    "    \n",
    "    return f\"\"\"You are an expert in AI-generated image quality assessment and prompt engineering for diffusion models.\n",
    "\n",
    "**Task**: Analyze the quality of a logo transformation for Christmas merchandise design.\n",
    "\n",
    "**Context**:\n",
    "- Original design goal: {scenario.replace('_', ' ')}\n",
    "- Text prompt used: \"{prompt_text}\"\n",
    "- Model: FLUX.1-Kontext-dev (FP16)\n",
    "- Generation time: {metadata.get('generation_time_seconds', 'N/A')}s\n",
    "- Inference steps: {metadata.get('num_steps', 'N/A')}\n",
    "\n",
    "**Images provided**:\n",
    "1. Input logo (original)\n",
    "2. Generated output (with prompt applied)\n",
    "3. Word attribution visualization (3 rows showing: baseline, ablated images, difference heatmaps)\n",
    "4. Evolution grid (diffusion process over time)\n",
    "5. Timestep evolution (showing how attention changes across denoising steps)\n",
    "\n",
    "The word attribution image shows:\n",
    "- Row 1: Image WITH each word (baseline)\n",
    "- Row 2: Image WITHOUT each word (what changes when word is removed)\n",
    "- Row 3: Heatmap showing spatial differences\n",
    "\n",
    "**Evaluate the following aspects** (be concise, 2-3 sentences each):\n",
    "\n",
    "1. **Prompt Adherence**: Did the model accurately follow the text instructions? What elements were correctly added?\n",
    "\n",
    "2. **Logo Preservation**: Is the original logo still recognizable and intact? Were any critical brand elements lost?\n",
    "\n",
    "3. **Design Suitability**: Would this work well for the intended merchandise ({scenario.replace('_', ' ')})? Consider practical printing/manufacturing.\n",
    "\n",
    "4. **Creative Execution**: How well did the model interpret \"Christmas\" or \"festive\" elements? Are they tasteful and appropriate?\n",
    "\n",
    "5. **Technical Quality**: Are there visual artifacts, distortions, or inconsistencies? Is the resolution/detail sufficient?\n",
    "\n",
    "6. **Word Attribution Insights**: Based on the ablation study (row 2 of attribution image), which words had the most significant impact? Were any redundant?\n",
    "\n",
    "7. **Prompt Improvement Suggestions**: What 2-3 specific changes to the prompt would improve the output? Reference the word attribution results.\n",
    "\n",
    "Provide your analysis in a structured format. Be direct and actionable.\"\"\"\n",
    "\n",
    "# ===== ANALYSIS RUNNER =====\n",
    "def analyze_single_result(\n",
    "    llm_name,\n",
    "    model_id,\n",
    "    scenario_name,\n",
    "    prompt_text,\n",
    "    result_dir,\n",
    "    metadata\n",
    "):\n",
    "    \"\"\"Run LLM analysis on a single experiment result\"\"\"\n",
    "    \n",
    "    result_path = Path(result_dir)\n",
    "    \n",
    "    # Collect images for analysis\n",
    "    images_to_analyze = []\n",
    "    \n",
    "    # Required images\n",
    "    input_img = result_path / \"input_image.png\"\n",
    "    output_img = result_path / \"final_output.png\"\n",
    "    \n",
    "    # Try new 3-row attribution first, fallback to old 2-row\n",
    "    attribution_img = result_path / \"word_attribution_complete.png\"\n",
    "    if not attribution_img.exists():\n",
    "        attribution_img = result_path / \"cross_attention_style_map.png\"\n",
    "    \n",
    "    evolution_img = result_path / \"evolution_grid.png\"\n",
    "    \n",
    "    # Optional timestep evolution\n",
    "    timestep_imgs = list(result_path.glob(\"timestep_evolution_*.png\"))\n",
    "    \n",
    "    # Add images in order of importance\n",
    "    for img_path in [input_img, output_img, attribution_img, evolution_img]:\n",
    "        if img_path.exists():\n",
    "            images_to_analyze.append(str(img_path))\n",
    "    \n",
    "    # Add timestep evolution if exists\n",
    "    if timestep_imgs:\n",
    "        images_to_analyze.append(str(timestep_imgs[0]))\n",
    "    \n",
    "    # Generate prompt\n",
    "    analysis_prompt = generate_analysis_prompt(\n",
    "        scenario_name, prompt_text, metadata\n",
    "    )\n",
    "    \n",
    "    print(f\"      Calling {llm_name}...\")\n",
    "    print(f\"        Images: {len(images_to_analyze)}\")\n",
    "    \n",
    "    # Call LLM\n",
    "    response = call_bedrock_llm(model_id, analysis_prompt, images_to_analyze)\n",
    "    \n",
    "    # Parse and structure response\n",
    "    analysis_result = {\n",
    "        \"llm\": llm_name,\n",
    "        \"model_id\": model_id,\n",
    "        \"scenario\": scenario_name,\n",
    "        \"prompt\": prompt_text,\n",
    "        \"analysis\": response,\n",
    "        \"images_analyzed\": len(images_to_analyze),\n",
    "        \"timestamp\": time.time()\n",
    "    }\n",
    "    \n",
    "    return analysis_result\n",
    "\n",
    "def run_llm_comparison(experiment_root):\n",
    "    \"\"\"Run all LLMs on all experiment results and compare\"\"\"\n",
    "    \n",
    "    experiment_path = Path(experiment_root)\n",
    "    \n",
    "    # Load experiment config and results\n",
    "    with open(experiment_path / \"experiment_config.json\") as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    with open(experiment_path / \"results_summary.json\") as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    # Storage for all analyses\n",
    "    all_analyses = []\n",
    "    \n",
    "    # Iterate through scenarios\n",
    "    for scenario_name, scenario_data in results.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ANALYZING SCENARIO: {scenario_name.upper()}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Iterate through prompts\n",
    "        for prompt_name, prompt_data in scenario_data.items():\n",
    "            print(f\"\\n  Prompt: {prompt_data['prompt'][:60]}...\")\n",
    "            \n",
    "            result_dir = prompt_data['output_dir']\n",
    "            \n",
    "            # Load metadata\n",
    "            metadata_path = Path(result_dir) / \"metadata.json\"\n",
    "            if metadata_path.exists():\n",
    "                with open(metadata_path) as f:\n",
    "                    metadata = json.load(f)\n",
    "            else:\n",
    "                metadata = {}\n",
    "            \n",
    "            # Run each LLM\n",
    "            for llm_name, llm_model_id in LLMS_TO_TEST.items():\n",
    "                try:\n",
    "                    analysis = analyze_single_result(\n",
    "                        llm_name,\n",
    "                        llm_model_id,\n",
    "                        scenario_name,\n",
    "                        prompt_data['prompt'],\n",
    "                        result_dir,\n",
    "                        metadata\n",
    "                    )\n",
    "                    \n",
    "                    all_analyses.append(analysis)\n",
    "                    \n",
    "                    # Save individual analysis\n",
    "                    analysis_dir = Path(result_dir) / \"llm_analysis\"\n",
    "                    analysis_dir.mkdir(exist_ok=True)\n",
    "                    \n",
    "                    with open(analysis_dir / f\"{llm_name}_analysis.json\", \"w\") as f:\n",
    "                        json.dump(analysis, f, indent=2)\n",
    "                    \n",
    "                    print(f\"        ‚úì {llm_name} completed\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"        ‚úó {llm_name} failed: {e}\")\n",
    "                \n",
    "                time.sleep(1)  # Rate limiting\n",
    "    \n",
    "    # Save complete analysis results\n",
    "    with open(experiment_path / \"all_llm_analyses.json\", \"w\") as f:\n",
    "        json.dump(all_analyses, f, indent=2)\n",
    "    \n",
    "    # Create comparison report\n",
    "    create_llm_comparison_report(all_analyses, experiment_path)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"‚úÖ LLM ANALYSIS COMPLETE!\")\n",
    "    print(f\"üìä {len(all_analyses)} analyses generated\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return all_analyses\n",
    "\n",
    "def create_llm_comparison_report(analyses, output_dir):\n",
    "    \"\"\"Generate markdown report comparing LLM reasoning quality\"\"\"\n",
    "    \n",
    "    report_path = Path(output_dir) / \"LLM_COMPARISON_REPORT.md\"\n",
    "    \n",
    "    with open(report_path, \"w\") as f:\n",
    "        f.write(\"# LLM Analysis Comparison Report\\n\\n\")\n",
    "        f.write(f\"**Generated**: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "        f.write(f\"**Model Used**: FLUX.1-Kontext-dev (FP16)\\n\\n\")\n",
    "        f.write(\"---\\n\\n\")\n",
    "        \n",
    "        # Group by scenario\n",
    "        scenarios = {}\n",
    "        for analysis in analyses:\n",
    "            scenario = analysis['scenario']\n",
    "            if scenario not in scenarios:\n",
    "                scenarios[scenario] = []\n",
    "            scenarios[scenario].append(analysis)\n",
    "        \n",
    "        for scenario, scenario_analyses in scenarios.items():\n",
    "            f.write(f\"\\n## üé® {scenario.upper().replace('_', ' ')}\\n\\n\")\n",
    "            \n",
    "            # Group by prompt within scenario\n",
    "            prompts = {}\n",
    "            for analysis in scenario_analyses:\n",
    "                prompt = analysis['prompt']\n",
    "                if prompt not in prompts:\n",
    "                    prompts[prompt] = []\n",
    "                prompts[prompt].append(analysis)\n",
    "            \n",
    "            for prompt, prompt_analyses in prompts.items():\n",
    "                f.write(f\"\\n### üìù Prompt: \\\"{prompt}\\\"\\n\\n\")\n",
    "                \n",
    "                # Show each LLM's analysis\n",
    "                for analysis in prompt_analyses:\n",
    "                    f.write(f\"#### ü§ñ {analysis['llm'].upper()}\\n\\n\")\n",
    "                    f.write(f\"*Images analyzed: {analysis.get('images_analyzed', 'N/A')}*\\n\\n\")\n",
    "                    f.write(f\"``````\\n\\n\")\n",
    "                    f.write(\"---\\n\\n\")\n",
    "        \n",
    "        # Summary statistics\n",
    "        f.write(\"\\n## üìä Summary\\n\\n\")\n",
    "        f.write(f\"- **Total Analyses**: {len(analyses)}\\n\")\n",
    "        f.write(f\"- **LLMs Tested**: {', '.join(LLMS_TO_TEST.keys())}\\n\")\n",
    "        f.write(f\"- **Scenarios**: {len(set(a['scenario'] for a in analyses))}\\n\")\n",
    "        f.write(f\"- **Prompts per Scenario**: 4 (1 base + 3 variants)\\n\")\n",
    "        f.write(f\"- **Total Experiment Runs**: {len(analyses) // len(LLMS_TO_TEST)}\\n\")\n",
    "        \n",
    "        # LLM performance comparison\n",
    "        f.write(\"\\n### LLM Response Statistics\\n\\n\")\n",
    "        f.write(\"| LLM | Successful Analyses | Avg Response Length |\\n\")\n",
    "        f.write(\"|-----|---------------------|---------------------|\\n\")\n",
    "        \n",
    "        for llm_name in LLMS_TO_TEST.keys():\n",
    "            llm_analyses = [a for a in analyses if a['llm'] == llm_name]\n",
    "            successful = len([a for a in llm_analyses if not a['analysis'].startswith('ERROR')])\n",
    "            avg_len = sum(len(a['analysis']) for a in llm_analyses) / len(llm_analyses) if llm_analyses else 0\n",
    "            f.write(f\"| {llm_name} | {successful}/{len(llm_analyses)} | {int(avg_len)} chars |\\n\")\n",
    "    \n",
    "    print(f\"\\nüìÑ Comparison report saved: {report_path}\")\n",
    "\n",
    "# ===== RUN DIRECTLY IN NOTEBOOK =====\n",
    "# CHANGE THIS to your actual experiment path:\n",
    "experiment_root = \"flux_experiments/run_20251116_012345\"\n",
    "\n",
    "analyses = run_llm_comparison(experiment_root)\n",
    "\n",
    "print(f\"\\n‚úÖ Analysis complete! Check {experiment_root}/LLM_COMPARISON_REPORT.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44674e92-84db-4a2f-84b8-4a7f68594f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the end of the script, change this line:\n",
    "experiment_root = \"flux_experiments/run_20251116_003042\"  # Your actual path\n",
    "\n",
    "# Then just run the cell - no sys.argv needed!\n",
    "analyses = run_llm_comparison(experiment_root)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
